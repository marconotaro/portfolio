[
  {
    "objectID": "cnn_vs_fnn.html",
    "href": "cnn_vs_fnn.html",
    "title": "Assignment for the SIB course: Diving into deep learning - theory and applications with PyTorch",
    "section": "",
    "text": "My solutions to the SIB course Diving into deep learning - theory and applications with PyTorch - held on 07-08 November 2024.\n\nGoal\n\nImplementing a convolutional neural network (CNN) to predict protein subcellular localization relying on sequence information\nComparing the CNN with the feedforward neural network (FNN)\nExpected output: CNN should provide a performance improvement over FNN\n\n\n\nDataset information\n\nThe dataset is from https://academic.oup.com/bioinformatics/article/33/21/3387/3931857\nEach sequence is encoded as a matrix where each position is a row of size 20, for each possible amino acid\nThe values within the matrix represent the amino acid frequency at the given position\n\n\n\nLoading libraries\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn import metrics\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report\n\nimport torch\nfrom torch import nn\nimport pytorch_model_summary as pms \nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom pytorchtools import EarlyStopping\n\n# get cpu, gpu or mps device for training\ndevice = (\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cpu\"\n)\nprint(f\"Using {device} device\")\n\nUsing cpu device\n\n\n\n\nLoading the protein sequences and labels\n\n## training data\ntrain = np.load('data/reduced_train.npz')\nX_train = train['X_train']\ny_train = train['y_train']\nprint('train:', X_train.shape)\n\n## validation data\nvalidation = np.load('data/reduced_val.npz')\nX_valid = validation['X_val']\ny_valid = validation['y_val']\nprint('valid:', X_valid.shape)\n\ntrain: (2423, 400, 20)\nvalid: (635, 400, 20)\n\n\n\n\nDefining the subcellular localization\n\nclasses = ['Nucleus',\n           'Cytoplasm',\n           'Extracellular',\n           'Mitochondrion',\n           'Cell membrane',\n           'ER',\n           'Chloroplast',\n           'Golgi apparatus',\n           'Lysosome',\n           'Vacuole']\n\ndico_classes_subcell={i:v for i,v in enumerate(classes)}\nfor i in dico_classes_subcell.keys():\n    print('Target', i, dico_classes_subcell[i])\n\nTarget 0 Nucleus\nTarget 1 Cytoplasm\nTarget 2 Extracellular\nTarget 3 Mitochondrion\nTarget 4 Cell membrane\nTarget 5 ER\nTarget 6 Chloroplast\nTarget 7 Golgi apparatus\nTarget 8 Lysosome\nTarget 9 Vacuole\n\n\n\n\nBuilding the data loaders\n\nbatch_size = 128\n\n# transform to torch tensor\nX_train_tensor = torch.Tensor(X_train) \ny_train_tensor = torch.LongTensor(y_train)\n\nX_valid_tensor = torch.Tensor(X_valid) \ny_valid_tensor = torch.LongTensor(y_valid)\n\n# create the dataset\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor) \nvalid_dataset = TensorDataset(X_valid_tensor,y_valid_tensor) \n\n## create the dataloader\ntrain_dataloader = DataLoader(train_dataset, batch_size = batch_size) \nvalid_dataloader = DataLoader(valid_dataset, batch_size = batch_size)\n\n\n\nBuilding the convolutional neural network (CNN)\nThis CNN uses two convolutional layers with a 3x3 kernel and a ReLU activation, followed by max pooling to downsample the sequence length while preserving the feature dimension. The resulting features are flattened and fed into a fully connected layer, which maps the extracted features to the 10 subcellular localizations.\n\nclass ProteinLoc_CNN(nn.Module):\n    def __init__(self, seq_len=400, n_feat=20, n_class=10, out_channels=10):\n        super().__init__()\n        \n        ## - two 2D (data are 2D (400 x 20)) convolutional layers with:\n        ##   - a 3x3 kernel to capture local features\n        ##   - a 1x1 padding to preserve spatial dimension (output feature map dimension = input feature map dimension)\n        ## - a max pooling with a 5x1 padding to reduce the sequence length by a factor of 5 preserving feature dimension\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(3, 3), padding=(1, 1)),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(5, 1), stride=(5, 1)),\n\n            nn.Conv2d(in_channels=out_channels, out_channels=out_channels*2, kernel_size=(3, 3), padding=(1, 1)),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(5, 1), stride=(5, 1))\n        )\n        \n        ## flatten the layer to transforms the 2D feature maps from the convolutional layers into a 1D vector\n        self.flatten = nn.Flatten()\n        \n        ## fully connected layer\n        ## map the features from the convolutional layers to the subcellular localizations\n        self.dense_layers = nn.Sequential(\n            nn.Linear(out_channels * 2 * (seq_len // (5 * 5)) * n_feat, n_class)\n        )\n                        \n    def forward(self, x):\n        ## add a channel to reshape the data in the form (batch_size, 1, 400, 20) and\n        ## make them compatible with the Conv2d shape (batch_size, channels, height, width)\n        x = x.unsqueeze(1)\n        x = self.conv(x)  \n        x = self.flatten(x)  \n        x = self.dense_layers(x)\n        return x\n\n# initialize the model\nmodel = ProteinLoc_CNN(seq_len=400, n_feat=20, n_class=10, out_channels=40).to(device)\nprint(model)\n\n# check model\nx, _ = train_dataset[0]\nprint(pms.summary(model, x.reshape(1, 400, 20).to(device), show_input=False))\n\nProteinLoc_CNN(\n  (conv): Sequential(\n    (0): Conv2d(1, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=(5, 1), stride=(5, 1), padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(40, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): ReLU()\n    (5): MaxPool2d(kernel_size=(5, 1), stride=(5, 1), padding=0, dilation=1, ceil_mode=False)\n  )\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (dense_layers): Sequential(\n    (0): Linear(in_features=25600, out_features=10, bias=True)\n  )\n)\n------------------------------------------------------------------------\n      Layer (type)         Output Shape         Param #     Tr. Param #\n========================================================================\n          Conv2d-1     [1, 40, 400, 20]             400             400\n            ReLU-2     [1, 40, 400, 20]               0               0\n       MaxPool2d-3      [1, 40, 80, 20]               0               0\n          Conv2d-4      [1, 80, 80, 20]          28,880          28,880\n            ReLU-5      [1, 80, 80, 20]               0               0\n       MaxPool2d-6      [1, 80, 16, 20]               0               0\n         Flatten-7           [1, 25600]               0               0\n          Linear-8              [1, 10]         256,010         256,010\n========================================================================\nTotal params: 285,290\nTrainable params: 285,290\nNon-trainable params: 0\n------------------------------------------------------------------------\n\n\n\n\nBuilding the feedforward neural network (FNN)\nTaken from the 2nd notebook of the course\n\nclass ProteinLoc_FNN(torch.nn.Module):\n    def __init__(self , input_dim = 8000, \n                         hidden_dim = [80],\n                         output_dim = 10, \n                         dropout_fraction = 0.25):\n        super().__init__()\n        \n        ## we transform the input from 2D to 1D\n        self.flatten = nn.Flatten()\n        \n        elements = []\n        # each layer is made of a linear layer with a ReLu activation and a DropOut Layer\n        for i in range(len(hidden_dim)):\n            \n            elements.append( nn.Linear(input_dim, hidden_dim[i]) )\n            elements.append( nn.ReLU() )\n            elements.append( nn.Dropout(dropout_fraction) ) ## add regulation\n            \n            input_dim = hidden_dim[i] ## update the input dimension for the next layer\n        \n        elements.append( nn.Linear(input_dim, output_dim) )\n\n        self.layers = nn.Sequential( *elements )\n        \n    def forward(self, x):\n        x = self.flatten(x)\n        ## NB: here, the output of the last layer are logits\n        logits = self.layers(x)\n        return logits\n\n# initialize model\nmodel = ProteinLoc_FNN(input_dim=8000, hidden_dim=[80], output_dim=10, dropout_fraction=0.25).to(device)\nprint(model)\n\n## check model\nprint(pms.summary(model, torch.zeros(1,400,20).to(device), show_input=True))\n\nProteinLoc_FNN(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (layers): Sequential(\n    (0): Linear(in_features=8000, out_features=80, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.25, inplace=False)\n    (3): Linear(in_features=80, out_features=10, bias=True)\n  )\n)\n-----------------------------------------------------------------------\n      Layer (type)         Input Shape         Param #     Tr. Param #\n=======================================================================\n         Flatten-1        [1, 400, 20]               0               0\n          Linear-2           [1, 8000]         640,080         640,080\n            ReLU-3             [1, 80]               0               0\n         Dropout-4             [1, 80]               0               0\n          Linear-5             [1, 80]             810             810\n=======================================================================\nTotal params: 640,890\nTrainable params: 640,890\nNon-trainable params: 0\n-----------------------------------------------------------------------\n\n\n\n\nTraining/validation and plotting functions\n\ndef train(dataloader, model, loss_fn, optimizer, echo=True, echo_batch=False):\n    \n    size = len(dataloader.dataset) # how many batches do we have\n    model.train() # Sets the module in training mode.\n    \n       \n    for batch, (X, y) in enumerate(dataloader): # for each batch\n        X, y = X.to(device), y.to(device)       # send the data to the GPU or whatever device you use for training\n\n        # Compute prediction error\n        pred = model(X)              # prediction for the model -&gt; forward pass\n        loss = loss_fn(pred, y)      # loss function from these prediction\n                \n        # Backpropagation\n        loss.backward()              # backward propagation \n                                     # https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html\n                                     # https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\n        \n        optimizer.step()             \n        optimizer.zero_grad()        # reset the gradients\n                                     # https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n        \n        if echo_batch:\n            current =  (batch + 1) * len(X)\n            print(f\"Train loss: {loss.item():&gt;7f} [{current:&gt;5d}/{size:&gt;5d}]\")\n    \n    if echo:\n        current =  (batch + 1) * len(X)\n        print(f\"Train loss: {loss.item():&gt;7f}\")\n\n    # return the last batch loss\n    return loss.item()\n\ndef valid(dataloader, model, loss_fn, echo = True):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.eval() # Sets the module in evaluation mode\n        \n    valid_loss = 0\n    with torch.no_grad(): ## disables tracking of gradient: prevent accidental training + speeds up computation\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            valid_loss += loss_fn(pred, y).item()  ## accumulating the loss function over the batches\n              \n    valid_loss /= num_batches\n\n    if echo:\n        print(f\"\\tValid loss: {valid_loss:&gt;8f}\")\n   \n    return  valid_loss\n\n\n\nUtility functions\n\n## get predicted and target from the model\ndef get_model_predictions_and_y(model, dataloader):\n    target = np.array([], dtype= 'float32') ## []\n    predicted = np.array([], dtype= 'float32') ## []\n    with torch.no_grad():\n        for X,y in dataloader:\n            X = X.to(device)\n            pred = model(X)\n            target = np.concatenate([target, y.squeeze().numpy()]) ## extend -&gt; concatenate for list\n            predicted = np.concatenate([predicted, np.argmax(pred.to('cpu').detach().numpy() , axis=1) ] )\n\n    return predicted, target\n\n## utility function: compute additional metrics during training besides entropy loss \ndef get_additional_scores(predicted, target):   \n    return { 'balanced_accuracy': metrics.balanced_accuracy_score(target, predicted),\n             'accuracy': metrics.accuracy_score(target, predicted),\n             'f1': metrics.f1_score(target, predicted, average = 'macro') }\n\n\n\nPlotting functions\n\n## plot training metrics\ndef plot_model_training(train_scores, valid_scores):\n    fig, axes = plt.subplots(2,2,figsize = (14,8))    \n\n    for i,k in enumerate( ['loss', 'balanced_accuracy', 'accuracy', 'f1'] ) :\n        axes[i//2][i%2].plot(train_scores[k], label = 'train')\n        axes[i//2][i%2].plot(valid_scores[k], label = 'validation')\n        if k == 'loss':\n            axes[i//2][i%2].axvline(np.argmin(valid_scores[k]), linestyle='--', color='r',label='Early Stopping Checkpoint')\n        axes[i//2][i%2].legend()\n        axes[i//2][i%2].set_xlabel('epoch')\n        axes[i//2][i%2].set_ylabel(k)\n\n## plot confusion matrix\ndef plot_confusion_matrix(model, X_valid_tensor, y_valid):\n    ## we can also use get_model_predictions_and_y() instead \n    y_pred = model(X_valid_tensor.to(device))\n    y_pred = np.argmax(y_pred.detach().cpu().numpy(), axis=1)\n\n    df = pd.crosstab(y_valid, y_pred, rownames=['truth'], colnames=['prediction'])\n    df.columns = classes\n    df.index = classes\n\n    #trick to make the 0s dissapear\n    sns.heatmap(df, annot = df.astype(str).replace('0',''), fmt ='s', cmap = 'viridis')\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n## plotting accuracy\ndef plot_accuracy(accuracy, xlabel, ylabel='Accuracy', title='Accuracy comparison'):\n    plt.figure(figsize=(4,4))\n    acc_score = accuracy\n    x = np.arange(len(acc_score))\n    plt.bar(x, acc_score)\n    plt.title(title)\n    plt.ylabel(ylabel)\n    plt.xticks(x, xlabel, rotation=60)\n    for i, v in enumerate(acc_score):\n        plt.text(i, v-0.07, '%.3f'%v, color='white', fontweight='bold', ha='center')\n\n\n\nWrapper function\nTo train and evaluate CNN and FNN we use:\n\nCEloss as loss function\naccuracy, balanced_accuracy and F1_score as performance metrics\nAdam as optimizer. For both models we used the hyperparameters configuration adopted from the 2nd notebook\nearly stopping for regularization\n\n\n\ndef train_ProteinLoc(model = ProteinLoc_CNN().to(device), \n                     lr=10**-3, weight_decay=0, ## default Adam parameters setting\n                     epochs=100, patience=25):\n   \n    ## set the model\n    model = model\n\n    ## set the loss function counting class unbalancing\n    n_class=10\n    W = torch.Tensor(compute_class_weight(class_weight='balanced', \n                     classes = np.array(list(range(n_class))), ## map subcell locations to int\n                     y= y_train)).to(device)\n    CEloss = nn.CrossEntropyLoss(weight = W)\n    #print('weights_classes',W.cpu().numpy())\n\n    ## set the optimizer: https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n    optimizer = torch.optim.Adam(model.parameters(), \n                                 lr =  lr,\n                                 weight_decay = weight_decay)\n\n    ## early stopping: prevent overfitting and reduce training time\n    early_stopping = EarlyStopping(patience=patience, verbose=False)\n\n    ## keep the scores across epochs\n    train_scores = {'loss':[], 'balanced_accuracy':[], 'accuracy':[], 'f1':[]}\n    valid_scores = {'loss':[], 'balanced_accuracy':[], 'accuracy':[], 'f1':[]}\n    \n    ## train the model across epochs\n    for t in range(1,epochs+1):\n        echo = t%10==0\n        if echo:\n            print('Epoch',t )    \n        \n        ## training set\n        train_scores['loss'].append(train(train_dataloader, \n                                    model, \n                                    CEloss, \n                                    optimizer,                        \n                                    echo=echo, echo_batch=False))\n        pred_train, target_train = get_model_predictions_and_y(model, train_dataloader)\n        train_metric = get_additional_scores(pred_train, target_train)\n        \n        ## validation set\n        valid_scores['loss'].append(valid(valid_dataloader, \n                                    model, \n                                    CEloss,\n                                    echo=echo))\n        pred_valid, target_valid = get_model_predictions_and_y(model, valid_dataloader)\n        valid_metric = get_additional_scores(pred_valid, target_valid)\n        \n        ## add extra metric\n        for k in ['balanced_accuracy', 'accuracy', 'f1']:\n            train_scores[k].append(train_metric[k])\n            valid_scores[k].append(valid_metric[k])\n\n        early_stopping(valid_scores['loss'][-1], model) ## send last valid_score to early stop\n\n        if early_stopping.early_stop:\n            print(\"Early stopping\")\n            break\n\n    print(\"Done!\")\n    \n    return train_scores, valid_scores, model, CEloss, optimizer\n\n\n\nHyperparmeters\nHyperparmeters configuration used for training/testing all the models - for a fair comparison\n\nepochs = 500\nlr = 10**-4\nweight_decay = 10**-2\npatience = 100\n\n\n\nTraining the CNN\n\n%%time\n\ntrain_scores_cnn, valid_scores_cnn, model_cnn, CEloss_cnn, optimizer_cnn = \\\n    train_ProteinLoc(ProteinLoc_CNN(seq_len=400, n_feat=20, n_class=10, out_channels=40).to(device),\n                     lr=lr, weight_decay=weight_decay,\n                     epochs=epochs, patience=patience)\n\nEpoch 10\nTrain loss: 1.278552\n    Valid loss: 1.793025\nEpoch 20\nTrain loss: 0.594917\n    Valid loss: 1.319517\nEpoch 30\nTrain loss: 0.333948\n    Valid loss: 1.085360\nEpoch 40\nTrain loss: 0.238646\n    Valid loss: 0.987021\nEpoch 50\nTrain loss: 0.189067\n    Valid loss: 0.949180\nEpoch 60\nTrain loss: 0.155821\n    Valid loss: 0.940055\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEarlyStopping counter: 5 out of 100\nEarlyStopping counter: 6 out of 100\nEarlyStopping counter: 7 out of 100\nEarlyStopping counter: 8 out of 100\nEarlyStopping counter: 9 out of 100\nEpoch 70\nTrain loss: 0.132961\n    Valid loss: 0.945401\nEarlyStopping counter: 10 out of 100\nEarlyStopping counter: 11 out of 100\nEarlyStopping counter: 12 out of 100\nEarlyStopping counter: 13 out of 100\nEarlyStopping counter: 14 out of 100\nEarlyStopping counter: 15 out of 100\nEarlyStopping counter: 16 out of 100\nEarlyStopping counter: 17 out of 100\nEarlyStopping counter: 18 out of 100\nEarlyStopping counter: 19 out of 100\nEpoch 80\nTrain loss: 0.115674\n    Valid loss: 0.955135\nEarlyStopping counter: 20 out of 100\nEarlyStopping counter: 21 out of 100\nEarlyStopping counter: 22 out of 100\nEarlyStopping counter: 23 out of 100\nEarlyStopping counter: 24 out of 100\nEarlyStopping counter: 25 out of 100\nEarlyStopping counter: 26 out of 100\nEarlyStopping counter: 27 out of 100\nEarlyStopping counter: 28 out of 100\nEarlyStopping counter: 29 out of 100\nEpoch 90\nTrain loss: 0.101232\n    Valid loss: 0.964812\nEarlyStopping counter: 30 out of 100\nEarlyStopping counter: 31 out of 100\nEarlyStopping counter: 32 out of 100\nEarlyStopping counter: 33 out of 100\nEarlyStopping counter: 34 out of 100\nEarlyStopping counter: 35 out of 100\nEarlyStopping counter: 36 out of 100\nEarlyStopping counter: 37 out of 100\nEarlyStopping counter: 38 out of 100\nEarlyStopping counter: 39 out of 100\nEpoch 100\nTrain loss: 0.088912\n    Valid loss: 0.973630\nEarlyStopping counter: 40 out of 100\nEarlyStopping counter: 41 out of 100\nEarlyStopping counter: 42 out of 100\nEarlyStopping counter: 43 out of 100\nEarlyStopping counter: 44 out of 100\nEarlyStopping counter: 45 out of 100\nEarlyStopping counter: 46 out of 100\nEarlyStopping counter: 47 out of 100\nEarlyStopping counter: 48 out of 100\nEarlyStopping counter: 49 out of 100\nEpoch 110\nTrain loss: 0.078697\n    Valid loss: 0.981492\nEarlyStopping counter: 50 out of 100\nEarlyStopping counter: 51 out of 100\nEarlyStopping counter: 52 out of 100\nEarlyStopping counter: 53 out of 100\nEarlyStopping counter: 54 out of 100\nEarlyStopping counter: 55 out of 100\nEarlyStopping counter: 56 out of 100\nEarlyStopping counter: 57 out of 100\nEarlyStopping counter: 58 out of 100\nEarlyStopping counter: 59 out of 100\nEpoch 120\nTrain loss: 0.070570\n    Valid loss: 0.988403\nEarlyStopping counter: 60 out of 100\nEarlyStopping counter: 61 out of 100\nEarlyStopping counter: 62 out of 100\nEarlyStopping counter: 63 out of 100\nEarlyStopping counter: 64 out of 100\nEarlyStopping counter: 65 out of 100\nEarlyStopping counter: 66 out of 100\nEarlyStopping counter: 67 out of 100\nEarlyStopping counter: 68 out of 100\nEarlyStopping counter: 69 out of 100\nEpoch 130\nTrain loss: 0.064186\n    Valid loss: 0.994534\nEarlyStopping counter: 70 out of 100\nEarlyStopping counter: 71 out of 100\nEarlyStopping counter: 72 out of 100\nEarlyStopping counter: 73 out of 100\nEarlyStopping counter: 74 out of 100\nEarlyStopping counter: 75 out of 100\nEarlyStopping counter: 76 out of 100\nEarlyStopping counter: 77 out of 100\nEarlyStopping counter: 78 out of 100\nEarlyStopping counter: 79 out of 100\nEpoch 140\nTrain loss: 0.059257\n    Valid loss: 0.999891\nEarlyStopping counter: 80 out of 100\nEarlyStopping counter: 81 out of 100\nEarlyStopping counter: 82 out of 100\nEarlyStopping counter: 83 out of 100\nEarlyStopping counter: 84 out of 100\nEarlyStopping counter: 85 out of 100\nEarlyStopping counter: 86 out of 100\nEarlyStopping counter: 87 out of 100\nEarlyStopping counter: 88 out of 100\nEarlyStopping counter: 89 out of 100\nEpoch 150\nTrain loss: 0.055351\n    Valid loss: 1.004522\nEarlyStopping counter: 90 out of 100\nEarlyStopping counter: 91 out of 100\nEarlyStopping counter: 92 out of 100\nEarlyStopping counter: 93 out of 100\nEarlyStopping counter: 94 out of 100\nEarlyStopping counter: 95 out of 100\nEarlyStopping counter: 96 out of 100\nEarlyStopping counter: 97 out of 100\nEarlyStopping counter: 98 out of 100\nEarlyStopping counter: 99 out of 100\nEpoch 160\nTrain loss: 0.052179\n    Valid loss: 1.008472\nEarlyStopping counter: 100 out of 100\nEarly stopping\nDone!\nCPU times: user 5h 28min 40s, sys: 1h 42min 51s, total: 7h 11min 32s\nWall time: 1h 11min 58s\n\n\n\n\nEvaluating the CNN\n\nplot_model_training(train_scores_cnn, valid_scores_cnn)\n\n\n\n\n\n\n\n\n\n\nTraining the FNN\n\n%%time \n\ntrain_scores_fnn, valid_scores_fnn, model_fnn, CEloss_fnn, optimizer_fnn = \\\n    train_ProteinLoc(ProteinLoc_FNN(input_dim=8000, hidden_dim=[80], output_dim=10, \n                                    dropout_fraction=0.25).to(device),\n                     lr=lr, weight_decay=weight_decay,\n                     epochs=epochs, patience=patience)\n\nEpoch 10\nTrain loss: 1.721765\n    Valid loss: 2.017928\nEpoch 20\nTrain loss: 1.237484\n    Valid loss: 1.800885\nEpoch 30\nTrain loss: 0.955508\n    Valid loss: 1.631265\nEpoch 40\nTrain loss: 0.783767\n    Valid loss: 1.508611\nEpoch 50\nTrain loss: 0.655635\n    Valid loss: 1.418898\nEpoch 60\nTrain loss: 0.552101\n    Valid loss: 1.354256\nEpoch 70\nTrain loss: 0.512112\n    Valid loss: 1.313151\nEarlyStopping counter: 1 out of 100\nEpoch 80\nTrain loss: 0.434766\n    Valid loss: 1.279691\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEpoch 90\nTrain loss: 0.380775\n    Valid loss: 1.252143\nEpoch 100\nTrain loss: 0.391283\n    Valid loss: 1.230097\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEpoch 110\nTrain loss: 0.368573\n    Valid loss: 1.218942\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEpoch 120\nTrain loss: 0.324602\n    Valid loss: 1.203767\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEpoch 130\nTrain loss: 0.300448\n    Valid loss: 1.190869\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEpoch 140\nTrain loss: 0.299642\n    Valid loss: 1.181567\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEpoch 150\nTrain loss: 0.290868\n    Valid loss: 1.170999\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEpoch 160\nTrain loss: 0.264038\n    Valid loss: 1.166920\nEarlyStopping counter: 5 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEarlyStopping counter: 5 out of 100\nEarlyStopping counter: 6 out of 100\nEarlyStopping counter: 7 out of 100\nEarlyStopping counter: 8 out of 100\nEpoch 170\nTrain loss: 0.267423\n    Valid loss: 1.157207\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEarlyStopping counter: 5 out of 100\nEpoch 180\nTrain loss: 0.262831\n    Valid loss: 1.154265\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEpoch 190\nTrain loss: 0.254707\n    Valid loss: 1.158014\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEarlyStopping counter: 5 out of 100\nEpoch 200\nTrain loss: 0.242801\n    Valid loss: 1.141612\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEarlyStopping counter: 5 out of 100\nEarlyStopping counter: 6 out of 100\nEarlyStopping counter: 7 out of 100\nEpoch 210\nTrain loss: 0.236814\n    Valid loss: 1.145432\nEarlyStopping counter: 8 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEarlyStopping counter: 5 out of 100\nEarlyStopping counter: 6 out of 100\nEarlyStopping counter: 1 out of 100\nEpoch 220\nTrain loss: 0.236250\n    Valid loss: 1.137942\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 1 out of 100\nEpoch 230\nTrain loss: 0.208718\n    Valid loss: 1.141708\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEarlyStopping counter: 5 out of 100\nEarlyStopping counter: 6 out of 100\nEarlyStopping counter: 7 out of 100\nEpoch 240\nTrain loss: 0.217312\n    Valid loss: 1.137266\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEarlyStopping counter: 5 out of 100\nEarlyStopping counter: 6 out of 100\nEarlyStopping counter: 7 out of 100\nEarlyStopping counter: 8 out of 100\nEarlyStopping counter: 9 out of 100\nEarlyStopping counter: 10 out of 100\nEpoch 250\nTrain loss: 0.211357\n    Valid loss: 1.130197\nEarlyStopping counter: 11 out of 100\nEarlyStopping counter: 12 out of 100\nEarlyStopping counter: 13 out of 100\nEarlyStopping counter: 14 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEarlyStopping counter: 5 out of 100\nEpoch 260\nTrain loss: 0.218220\n    Valid loss: 1.130170\nEarlyStopping counter: 6 out of 100\nEarlyStopping counter: 7 out of 100\nEarlyStopping counter: 8 out of 100\nEarlyStopping counter: 9 out of 100\nEarlyStopping counter: 10 out of 100\nEarlyStopping counter: 11 out of 100\nEarlyStopping counter: 12 out of 100\nEarlyStopping counter: 13 out of 100\nEarlyStopping counter: 14 out of 100\nEarlyStopping counter: 15 out of 100\nEpoch 270\nTrain loss: 0.180335\n    Valid loss: 1.132610\nEarlyStopping counter: 16 out of 100\nEarlyStopping counter: 17 out of 100\nEarlyStopping counter: 18 out of 100\nEarlyStopping counter: 19 out of 100\nEarlyStopping counter: 20 out of 100\nEarlyStopping counter: 21 out of 100\nEarlyStopping counter: 22 out of 100\nEarlyStopping counter: 23 out of 100\nEarlyStopping counter: 24 out of 100\nEarlyStopping counter: 25 out of 100\nEpoch 280\nTrain loss: 0.197241\n    Valid loss: 1.128655\nEarlyStopping counter: 26 out of 100\nEarlyStopping counter: 27 out of 100\nEarlyStopping counter: 28 out of 100\nEarlyStopping counter: 29 out of 100\nEarlyStopping counter: 30 out of 100\nEarlyStopping counter: 31 out of 100\nEarlyStopping counter: 32 out of 100\nEarlyStopping counter: 33 out of 100\nEarlyStopping counter: 34 out of 100\nEarlyStopping counter: 35 out of 100\nEpoch 290\nTrain loss: 0.182162\n    Valid loss: 1.127565\nEarlyStopping counter: 36 out of 100\nEarlyStopping counter: 37 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEarlyStopping counter: 5 out of 100\nEarlyStopping counter: 6 out of 100\nEpoch 300\nTrain loss: 0.187505\n    Valid loss: 1.124462\nEarlyStopping counter: 7 out of 100\nEarlyStopping counter: 8 out of 100\nEarlyStopping counter: 9 out of 100\nEarlyStopping counter: 10 out of 100\nEarlyStopping counter: 11 out of 100\nEarlyStopping counter: 12 out of 100\nEarlyStopping counter: 13 out of 100\nEarlyStopping counter: 14 out of 100\nEarlyStopping counter: 15 out of 100\nEarlyStopping counter: 16 out of 100\nEpoch 310\nTrain loss: 0.206274\n    Valid loss: 1.127542\nEarlyStopping counter: 17 out of 100\nEarlyStopping counter: 18 out of 100\nEarlyStopping counter: 19 out of 100\nEarlyStopping counter: 20 out of 100\nEarlyStopping counter: 21 out of 100\nEarlyStopping counter: 22 out of 100\nEarlyStopping counter: 23 out of 100\nEarlyStopping counter: 24 out of 100\nEarlyStopping counter: 25 out of 100\nEarlyStopping counter: 26 out of 100\nEpoch 320\nTrain loss: 0.171165\n    Valid loss: 1.126099\nEarlyStopping counter: 27 out of 100\nEarlyStopping counter: 28 out of 100\nEarlyStopping counter: 29 out of 100\nEarlyStopping counter: 30 out of 100\nEarlyStopping counter: 31 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEpoch 330\nTrain loss: 0.182406\n    Valid loss: 1.110636\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEarlyStopping counter: 5 out of 100\nEarlyStopping counter: 6 out of 100\nEarlyStopping counter: 7 out of 100\nEarlyStopping counter: 8 out of 100\nEarlyStopping counter: 9 out of 100\nEpoch 340\nTrain loss: 0.182533\n    Valid loss: 1.117068\nEarlyStopping counter: 10 out of 100\nEarlyStopping counter: 11 out of 100\nEarlyStopping counter: 12 out of 100\nEarlyStopping counter: 13 out of 100\nEarlyStopping counter: 14 out of 100\nEarlyStopping counter: 15 out of 100\nEarlyStopping counter: 16 out of 100\nEarlyStopping counter: 17 out of 100\nEarlyStopping counter: 18 out of 100\nEarlyStopping counter: 19 out of 100\nEpoch 350\nTrain loss: 0.169552\n    Valid loss: 1.113264\nEarlyStopping counter: 20 out of 100\nEarlyStopping counter: 21 out of 100\nEarlyStopping counter: 22 out of 100\nEarlyStopping counter: 23 out of 100\nEarlyStopping counter: 24 out of 100\nEarlyStopping counter: 25 out of 100\nEarlyStopping counter: 26 out of 100\nEarlyStopping counter: 27 out of 100\nEarlyStopping counter: 28 out of 100\nEarlyStopping counter: 29 out of 100\nEpoch 360\nTrain loss: 0.176773\n    Valid loss: 1.122313\nEarlyStopping counter: 30 out of 100\nEarlyStopping counter: 31 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEarlyStopping counter: 5 out of 100\nEarlyStopping counter: 6 out of 100\nEarlyStopping counter: 7 out of 100\nEpoch 370\nTrain loss: 0.172929\n    Valid loss: 1.117166\nEarlyStopping counter: 8 out of 100\nEarlyStopping counter: 9 out of 100\nEarlyStopping counter: 10 out of 100\nEarlyStopping counter: 11 out of 100\nEarlyStopping counter: 12 out of 100\nEarlyStopping counter: 13 out of 100\nEarlyStopping counter: 14 out of 100\nEarlyStopping counter: 15 out of 100\nEarlyStopping counter: 16 out of 100\nEarlyStopping counter: 17 out of 100\nEpoch 380\nTrain loss: 0.172954\n    Valid loss: 1.114234\nEarlyStopping counter: 18 out of 100\nEarlyStopping counter: 19 out of 100\nEarlyStopping counter: 20 out of 100\nEarlyStopping counter: 21 out of 100\nEarlyStopping counter: 22 out of 100\nEarlyStopping counter: 23 out of 100\nEarlyStopping counter: 24 out of 100\nEarlyStopping counter: 25 out of 100\nEarlyStopping counter: 26 out of 100\nEarlyStopping counter: 27 out of 100\nEpoch 390\nTrain loss: 0.175194\n    Valid loss: 1.118202\nEarlyStopping counter: 28 out of 100\nEarlyStopping counter: 29 out of 100\nEarlyStopping counter: 30 out of 100\nEarlyStopping counter: 31 out of 100\nEarlyStopping counter: 32 out of 100\nEarlyStopping counter: 33 out of 100\nEarlyStopping counter: 34 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEpoch 400\nTrain loss: 0.163461\n    Valid loss: 1.115803\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEarlyStopping counter: 5 out of 100\nEarlyStopping counter: 6 out of 100\nEarlyStopping counter: 7 out of 100\nEarlyStopping counter: 8 out of 100\nEarlyStopping counter: 9 out of 100\nEarlyStopping counter: 10 out of 100\nEarlyStopping counter: 11 out of 100\nEarlyStopping counter: 12 out of 100\nEpoch 410\nTrain loss: 0.167024\n    Valid loss: 1.111715\nEarlyStopping counter: 13 out of 100\nEarlyStopping counter: 14 out of 100\nEarlyStopping counter: 15 out of 100\nEarlyStopping counter: 16 out of 100\nEarlyStopping counter: 17 out of 100\nEarlyStopping counter: 18 out of 100\nEarlyStopping counter: 19 out of 100\nEarlyStopping counter: 20 out of 100\nEarlyStopping counter: 21 out of 100\nEarlyStopping counter: 22 out of 100\nEpoch 420\nTrain loss: 0.172140\n    Valid loss: 1.122734\nEarlyStopping counter: 23 out of 100\nEarlyStopping counter: 24 out of 100\nEarlyStopping counter: 25 out of 100\nEarlyStopping counter: 26 out of 100\nEarlyStopping counter: 27 out of 100\nEarlyStopping counter: 28 out of 100\nEarlyStopping counter: 29 out of 100\nEarlyStopping counter: 30 out of 100\nEarlyStopping counter: 31 out of 100\nEarlyStopping counter: 32 out of 100\nEpoch 430\nTrain loss: 0.177140\n    Valid loss: 1.111379\nEarlyStopping counter: 33 out of 100\nEarlyStopping counter: 34 out of 100\nEarlyStopping counter: 35 out of 100\nEarlyStopping counter: 36 out of 100\nEarlyStopping counter: 37 out of 100\nEarlyStopping counter: 38 out of 100\nEarlyStopping counter: 39 out of 100\nEarlyStopping counter: 40 out of 100\nEarlyStopping counter: 41 out of 100\nEarlyStopping counter: 42 out of 100\nEpoch 440\nTrain loss: 0.170154\n    Valid loss: 1.116932\nEarlyStopping counter: 43 out of 100\nEarlyStopping counter: 44 out of 100\nEarlyStopping counter: 45 out of 100\nEarlyStopping counter: 46 out of 100\nEarlyStopping counter: 47 out of 100\nEarlyStopping counter: 48 out of 100\nEarlyStopping counter: 49 out of 100\nEarlyStopping counter: 50 out of 100\nEarlyStopping counter: 51 out of 100\nEarlyStopping counter: 52 out of 100\nEpoch 450\nTrain loss: 0.161532\n    Valid loss: 1.122467\nEarlyStopping counter: 53 out of 100\nEarlyStopping counter: 54 out of 100\nEarlyStopping counter: 55 out of 100\nEarlyStopping counter: 56 out of 100\nEarlyStopping counter: 57 out of 100\nEarlyStopping counter: 58 out of 100\nEarlyStopping counter: 59 out of 100\nEarlyStopping counter: 60 out of 100\nEarlyStopping counter: 61 out of 100\nEarlyStopping counter: 62 out of 100\nEpoch 460\nTrain loss: 0.175092\n    Valid loss: 1.114117\nEarlyStopping counter: 63 out of 100\nEarlyStopping counter: 64 out of 100\nEarlyStopping counter: 65 out of 100\nEarlyStopping counter: 66 out of 100\nEarlyStopping counter: 67 out of 100\nEarlyStopping counter: 68 out of 100\nEarlyStopping counter: 69 out of 100\nEarlyStopping counter: 70 out of 100\nEarlyStopping counter: 71 out of 100\nEarlyStopping counter: 72 out of 100\nEpoch 470\nTrain loss: 0.175786\n    Valid loss: 1.114911\nEarlyStopping counter: 73 out of 100\nEarlyStopping counter: 74 out of 100\nEarlyStopping counter: 75 out of 100\nEarlyStopping counter: 76 out of 100\nEarlyStopping counter: 77 out of 100\nEarlyStopping counter: 78 out of 100\nEarlyStopping counter: 79 out of 100\nEarlyStopping counter: 80 out of 100\nEarlyStopping counter: 81 out of 100\nEarlyStopping counter: 82 out of 100\nEpoch 480\nTrain loss: 0.150602\n    Valid loss: 1.113243\nEarlyStopping counter: 83 out of 100\nEarlyStopping counter: 84 out of 100\nEarlyStopping counter: 85 out of 100\nEarlyStopping counter: 86 out of 100\nEarlyStopping counter: 87 out of 100\nEarlyStopping counter: 88 out of 100\nEarlyStopping counter: 89 out of 100\nEarlyStopping counter: 90 out of 100\nEarlyStopping counter: 91 out of 100\nEarlyStopping counter: 92 out of 100\nEpoch 490\nTrain loss: 0.171302\n    Valid loss: 1.115211\nEarlyStopping counter: 93 out of 100\nEarlyStopping counter: 94 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEarlyStopping counter: 5 out of 100\nEarlyStopping counter: 6 out of 100\nEarlyStopping counter: 7 out of 100\nEpoch 500\nTrain loss: 0.159058\n    Valid loss: 1.115464\nEarlyStopping counter: 8 out of 100\nDone!\nCPU times: user 14min 53s, sys: 14.3 s, total: 15min 7s\nWall time: 2min 31s\n\n\n\n\nEvaluating the FNN\n\nplot_model_training(train_scores_fnn, valid_scores_fnn)\n\n\n\n\n\n\n\n\n\n\nPerformance comparison: CNN vs FNN\n\ny_pred_cnn, y_cnn = get_model_predictions_and_y(model_cnn, valid_dataloader)\ny_pred_fnn, y_fnn = get_model_predictions_and_y(model_fnn, valid_dataloader)\n\n\nCNN\n\nplot_confusion_matrix(model_cnn, X_valid_tensor, y_valid)\n\n\n\n\n\n\n\n\n\nprint(classification_report(y_valid, y_pred_cnn, target_names=classes))\n\n                 precision    recall  f1-score   support\n\n        Nucleus       0.87      0.85      0.86        95\n      Cytoplasm       0.88      0.74      0.80       151\n  Extracellular       0.92      0.82      0.87       131\n  Mitochondrion       0.67      0.84      0.74        64\n  Cell membrane       0.78      0.91      0.84        69\n             ER       0.60      0.69      0.64        13\n    Chloroplast       0.84      0.80      0.82        60\nGolgi apparatus       0.88      0.78      0.82        18\n       Lysosome       0.78      0.74      0.76        19\n        Vacuole       0.37      0.73      0.49        15\n\n       accuracy                           0.81       635\n      macro avg       0.76      0.79      0.76       635\n   weighted avg       0.83      0.81      0.81       635\n\n\n\n\n\nFNN\n\nplot_confusion_matrix(model_fnn, X_valid_tensor, y_valid)\n\n\n\n\n\n\n\n\n\nprint(classification_report(y_valid, y_pred_fnn, target_names=classes))\n\n                 precision    recall  f1-score   support\n\n        Nucleus       0.80      0.82      0.81        95\n      Cytoplasm       0.77      0.83      0.80       151\n  Extracellular       0.87      0.87      0.87       131\n  Mitochondrion       0.71      0.86      0.78        64\n  Cell membrane       0.74      0.81      0.77        69\n             ER       0.50      0.15      0.24        13\n    Chloroplast       0.86      0.73      0.79        60\nGolgi apparatus       0.68      0.72      0.70        18\n       Lysosome       0.82      0.47      0.60        19\n        Vacuole       0.80      0.27      0.40        15\n\n       accuracy                           0.79       635\n      macro avg       0.76      0.65      0.68       635\n   weighted avg       0.79      0.79      0.78       635\n\n\n\n\n\nAccuracy comparison\n\n## other way of computing accuracy\n# from sklearn.metrics import confusion_matrix\n# cm_fnn = confusion_matrix(y_valid, y_pred_fnn)\n# accuracy_fnn = np.trace(cm_fnn) / np.sum(cm_fnn)\n\naccuracy_cnn = get_additional_scores(y_pred_cnn, y_cnn)['accuracy']\naccuracy_fnn = get_additional_scores(y_pred_fnn, y_fnn)['accuracy']\n\n## plot accuracy \nplot_accuracy(accuracy=[accuracy_fnn, accuracy_cnn],\n              xlabel=['FNN','CNN'],\n              ylabel='Accuracy', title='')\n\n\n\n\n\n\n\n\n\n\nBalanced accuracy comparison\n\n## compute balanced accuracy over subcellular localizations (classes)\n\nbalanced_accuracy_cnn = get_additional_scores(y_pred_cnn, y_cnn)['balanced_accuracy']\nbalanced_accuracy_fnn = get_additional_scores(y_pred_fnn, y_fnn)['balanced_accuracy']\n\n## plot accuracy\n## note: p -&gt; padding and s -&gt; stride \nplot_accuracy(accuracy=[balanced_accuracy_fnn, balanced_accuracy_cnn],\n              xlabel=['FNN','CNN'],\n              ylabel='Balanced accuracy', title='')\n\n\n\n\n\n\n\n\n\n\n\nConclusions and considerations\n\nCNN slightly outperforms FNN, but at a much higher computational cost: CNN took 12min while FNN took 10sec in current configuration\nIncreasing the number of output channels (i.e. feature maps) boosts CNN performance, but at the cost of increased training time. For instance, a CNN with 80 output channels yielded an accuracy of around 0.82 after roughly 40 minutes of training (data not shown)."
  },
  {
    "objectID": "r1_eda.html",
    "href": "r1_eda.html",
    "title": "Exploratory data analysis: validation of technical replicates",
    "section": "",
    "text": "The GSE96058 patient cohort contains 3273 samples of which 136 have technical replicates. Some of these replicates were sequenced on a different sequencer (HiSeq2000 and NextSeq500). In this report, I analyze the gene expression profiles of these technical replicates to assess their similarity. This serves as a technical validation before averaging their gene expression values."
  },
  {
    "objectID": "r1_eda.html#aims",
    "href": "r1_eda.html#aims",
    "title": "Exploratory data analysis: validation of technical replicates",
    "section": "",
    "text": "The GSE96058 patient cohort contains 3273 samples of which 136 have technical replicates. Some of these replicates were sequenced on a different sequencer (HiSeq2000 and NextSeq500). In this report, I analyze the gene expression profiles of these technical replicates to assess their similarity. This serves as a technical validation before averaging their gene expression values."
  },
  {
    "objectID": "r1_eda.html#validation-of-technical-replicates",
    "href": "r1_eda.html#validation-of-technical-replicates",
    "title": "Exploratory data analysis: validation of technical replicates",
    "section": "Validation of technical replicates",
    "text": "Validation of technical replicates\n\n\n\n\n\n\nNote\n\n\n\nThe authors provided only transformed gene expression data, which I then used for downstream analysis.\n\n\nLoading data\n\ngexp &lt;- read_csv(\n  'data/GSE96058_gene_expression_3273_samples_and_136_replicates_transformed.csv.gz', \n  col_types = cols()\n) |&gt; rename(genes = `...1`)\n\nmeta &lt;- read_csv('data/metadata.csv', col_types = cols()) |&gt;\n  rename(sampleID = values, sampleName = ind)\n\n\nrep &lt;- gexp |&gt; select(contains('repl')) |&gt; colnames() |&gt; str_remove('repl')\n\nmxrep &lt;- gexp |&gt; \n  select(genes, all_of(rep) | contains('repl')) |&gt; \n  column_to_rownames(var = 'genes') \n\nmdrep &lt;- meta |&gt; \n  filter(sampleID %in% colnames(mxrep)) |&gt;\n  select(sampleID, instrument_model, age_at_diagnosis, \n         tumor_size, lymph_node_group, lymph_node_status, \n         her2_status, ki67_status, pgr_status, nhg, \n         endocrine_treated, chemo_treated) |&gt; \n  column_to_rownames(var = 'sampleID')\n\nPCA\nI apply a Principal Component Analysis (PCA) on the gene expression profiles of technical replicates (marked with the same color) to assess similarity in gene expression. As expected, technical replicates cluster together.\n\npc &lt;- PCAtools::pca(mxrep, metadata = mdrep, center = TRUE, scale = FALSE, removeVar = 0.1)\n\nstopifnot(rownames(mdrep) %in% rownames(pc$rotated))\n\nplot_pca &lt;- function(pc, pcx ='PC1', pcy = 'PC2'){\n  pc_data &lt;- cbind(pc$rotated[, c(pcx, pcy)], mdrep) |&gt;\n    rownames_to_column(var = 'sample')|&gt;\n    mutate(strip_rep = str_replace(sample, 'repl', ''))\n\n  pc_val &lt;- round(pc$variance[c(pcx, pcy)], 2) |&gt; unname()\n\n  ggplot(pc_data, aes(x = !!sym(pcx), y = !!sym(pcy), color = strip_rep, shape = instrument_model)) +\n    geom_point(size = 3) + \n    labs(title = \"\",\n         x = paste0(\"PC1 (\", pc_val[1], \"% variance)\"),\n         y = paste0(\"PC2 (\", pc_val[2], \"% variance)\"),\n         shape = \"Platform\",) +\n    geom_text_repel(data = pc_data, \n                    aes(label = sample), \n                    min.segment.length = 5, max.overlaps = Inf) +\n    theme_bw() +\n    theme(plot.title = element_text(hjust = 0.5)) +\n    guides(color = \"none\")\n}\n\nplot_pca(pc, pcx ='PC1', pcy = 'PC2')\n\n\n\n\n\n\n\nSample correlations\nFor a quantitative overview, I compute the Spearman’s rank correlation for each pair. As expected, technical replicates are strongly correlated (coefficient higher than 0.9) and their gene expression can be safely averaged for downstream analyses.\n\nmxrep_paired &lt;- mxrep[, str_sort(colnames(mxrep), numeric = TRUE)]\n\ntib &lt;- tibble(\n  comparison = character(),\n  correlation = numeric(),\n  p_value = numeric(),\n  informative = logical(),\n  significant = logical()\n)\n\nfor(i in seq(1, ncol(mxrep_paired)-1, by = 2)){\n  corr &lt;- cor.test(mxrep_paired[, i], \n                   mxrep_paired[, i+1], \n                   method = 'spearman',\n                   exact = FALSE\n                  )\n\n  tmp &lt;- tibble(\n    comparison = paste(colnames(mxrep_paired)[i:(i+1)], collapse=' vs '),\n    correlation = round(corr$estimate, 2),\n    p_value = corr$p.value,\n    informative =  round(corr$estimate, 2) &gt;= 0.90,\n    significant = corr$p.value &lt; 0.05\n  )\n\n  tib &lt;- rbind(tib, tmp)\n}\n\ndatatable &lt;- function(tib, row2display = 10) {\n  if(nrow(tib) &gt; 0){\n    DT::datatable(tib,\n      rownames   = FALSE,\n      options    = list(\n        dom = \"Bfrtip\",\n        scrollX = TRUE,\n        pageLength = row2display\n      )\n    )\n  }else{\n    print(\"No results\")\n  }\n}\ntib |&gt; datatable()\n\n\n\n\n\nAs an example, below I show the scatter plots of the first 12 pairs.\n\nscatter_plot &lt;- function(data, sample, replicate) {\n  ggplot(data, aes(x = !!sym(sample), y = !!sym(replicate))) +\n    geom_point(colour = \"#56B4E9\", alpha = 0.8, size = 1) +\n    geom_smooth(method = \"lm\", color = \"#D55E00\", formula = y ~ x, se = TRUE) +\n    labs(\n      title = paste(sample, \"vs\", replicate),\n      subtitle = paste(\"r =\", round(cor(data[[sample]], data[[replicate]], method = \"spearman\"), 3))\n    ) +\n    theme_bw() +\n    theme(\n      plot.title = element_text(size = 10),\n      plot.subtitle = element_text(size = 8)\n    )\n}\n\nget_pair &lt;- function(tib, npairs=10){\n  pairs &lt;- str_split(tib$comparison, \" vs \", simplify = TRUE)[1:npairs,]\n  sample &lt;- pairs[, 1]\n  replicate &lt;- pairs[, 2]\n  return(list(sample = sample, replicate = replicate))\n}\n\nsp &lt;- get_pair(tib, npairs = 12)\nplist &lt;- map2(sp$sample, sp$replicate, ~scatter_plot(mxrep, .x, .y))\nwrap_plots(plist, ncol = 4)"
  },
  {
    "objectID": "r1_eda.html#averaging-of-gene-expression-of-technical-replicates",
    "href": "r1_eda.html#averaging-of-gene-expression-of-technical-replicates",
    "title": "Exploratory data analysis: validation of technical replicates",
    "section": "Averaging of gene expression of technical replicates",
    "text": "Averaging of gene expression of technical replicates\n\nmxrep_averaged &lt;- c()\n\nfor(i in seq(1, ncol(mxrep_paired)-1, by = 2)){\n  mxrep_averaged &lt;- cbind(mxrep_averaged, rowMeans(mxrep_paired[,i:(i+1)]))\n  # cat(colnames(mxrep_paired)[i], colnames(mxrep_paired)[i+1], 'averaged\\n')\n}\n\nnorep &lt;- str_subset(colnames(mxrep_paired), 'repl', negate=TRUE)\ncolnames(mxrep_averaged) &lt;- norep\n\ngexp_rep &lt;- mxrep_averaged |&gt; \n  as.data.frame() |&gt; \n  rownames_to_column(var = 'genes') |&gt; \n  as_tibble()\n\nfout &lt;- 'data/gene_expression_profile.csv.gz'\nif(!file.exists(fout)){\n  gexp |&gt; \n    select(-all_of(colnames(gexp_rep)[-1])) |&gt; \n    left_join(gexp_rep, by = 'genes') |&gt; \n    select(-ends_with(\"repl\")) |&gt;\n    {\\(s)  ## lambda function ..\n      select(s, genes, all_of(str_sort(names(select(s, starts_with(\"F\"))), numeric = TRUE)))\n    }() |&gt;\n    ## %&gt;% select(., all_of(str_sort(colnames(select(., starts_with('F'))), numeric = TRUE)))\n    write_csv(fout) ## automatically compress by readr\n}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Personal Projects",
    "section": "",
    "text": "Identification of breast cancer subtypes via a graph-based approach from a public bulk RNA-Seq patient cohort.\n\nGetting data\nExploratory data analysis and preprocessing\nClustering analysis"
  },
  {
    "objectID": "index.html#genomics",
    "href": "index.html#genomics",
    "title": "Personal Projects",
    "section": "",
    "text": "Identification of breast cancer subtypes via a graph-based approach from a public bulk RNA-Seq patient cohort.\n\nGetting data\nExploratory data analysis and preprocessing\nClustering analysis"
  },
  {
    "objectID": "index.html#deep-learning",
    "href": "index.html#deep-learning",
    "title": "Personal Projects",
    "section": "Deep Learning",
    "text": "Deep Learning\nPredicting protein subcellular localization via a deep learning approach from aminoacid sequences.\n\nCNN vs FNN with PyTorch"
  },
  {
    "objectID": "index.html#ai-powered-tools",
    "href": "index.html#ai-powered-tools",
    "title": "Personal Projects",
    "section": "AI-powered tools",
    "text": "AI-powered tools\n\nDeveloping a RAG framework for bioinformatics applications"
  },
  {
    "objectID": "r0_getdata.html",
    "href": "r0_getdata.html",
    "title": "Downloading GEO data",
    "section": "",
    "text": "In this report, I download the 3409 breast cancer bulk RNA-seq samples and the corresponding clinical annotations, associated with the study: Clinical Value of RNA Sequencing-Based Classifiers for Prediction of the Five Conventional Breast Cancer Biomarkers: A Report From the Population-Based Multicenter Sweden Cancerome Analysis Network—Breast Initiative (GSE96058).\n\n\nif(!dir.exists(here::here('data')))\n  dir.create(here::here('data'))\n\n\n## get geo obj\ngeo &lt;- getGEO(GEO = \"GSE96058\", GSEMatrix = FALSE)\n\n## fetch metadata\nmeta &lt;- purrr::map(geo@gsms, ~.x@header$characteristics_ch1) |&gt;\n  stack() |&gt;\n  tidyr::separate(values, into = c(\"feature\", \"value\"), sep= \": \") |&gt;\n  tidyr::pivot_wider(names_from= feature, values_from = value) |&gt;\n  janitor::clean_names()\n\n## map samples\nsample &lt;- purrr::map(geo@gsms, ~.x@header$title) |&gt;\n  stack() |&gt;\n  as_tibble() |&gt;\n  mutate(ind = as.character(ind))\n\n## store metadata\nmeta &lt;- left_join(sample, meta, by = 'ind') |&gt;\n  write_csv(here::here(\"data/metadata.csv\"))\n\n\n\nsuccess &lt;- FALSE\nattempt &lt;- 1\nwhile (!success && attempt &lt;= 5) {\n  tryCatch({\n    getGEOSuppFiles(\"GSE96058\", makeDirectory = FALSE,\n                    baseDir = here::here('data'),\n                    fetch_files = TRUE, filter_regex = 'gene_expression')\n    success &lt;- TRUE\n    message(\"Download successful on attempt \", attempt)\n  }, error = function(e) {\n    message(\"Download failed on attempt \", attempt, \": \", e$message)\n    attempt &lt;&lt;- attempt + 1\n    Sys.sleep(5) # wait before retrying\n  })\n}\nif (!success)\n  stop(sprintf(\"Download failed after %s attempts\", attempt))"
  },
  {
    "objectID": "r0_getdata.html#aims",
    "href": "r0_getdata.html#aims",
    "title": "Downloading GEO data",
    "section": "",
    "text": "In this report, I download the 3409 breast cancer bulk RNA-seq samples and the corresponding clinical annotations, associated with the study: Clinical Value of RNA Sequencing-Based Classifiers for Prediction of the Five Conventional Breast Cancer Biomarkers: A Report From the Population-Based Multicenter Sweden Cancerome Analysis Network—Breast Initiative (GSE96058).\n\n\nif(!dir.exists(here::here('data')))\n  dir.create(here::here('data'))\n\n\n## get geo obj\ngeo &lt;- getGEO(GEO = \"GSE96058\", GSEMatrix = FALSE)\n\n## fetch metadata\nmeta &lt;- purrr::map(geo@gsms, ~.x@header$characteristics_ch1) |&gt;\n  stack() |&gt;\n  tidyr::separate(values, into = c(\"feature\", \"value\"), sep= \": \") |&gt;\n  tidyr::pivot_wider(names_from= feature, values_from = value) |&gt;\n  janitor::clean_names()\n\n## map samples\nsample &lt;- purrr::map(geo@gsms, ~.x@header$title) |&gt;\n  stack() |&gt;\n  as_tibble() |&gt;\n  mutate(ind = as.character(ind))\n\n## store metadata\nmeta &lt;- left_join(sample, meta, by = 'ind') |&gt;\n  write_csv(here::here(\"data/metadata.csv\"))\n\n\n\nsuccess &lt;- FALSE\nattempt &lt;- 1\nwhile (!success && attempt &lt;= 5) {\n  tryCatch({\n    getGEOSuppFiles(\"GSE96058\", makeDirectory = FALSE,\n                    baseDir = here::here('data'),\n                    fetch_files = TRUE, filter_regex = 'gene_expression')\n    success &lt;- TRUE\n    message(\"Download successful on attempt \", attempt)\n  }, error = function(e) {\n    message(\"Download failed on attempt \", attempt, \": \", e$message)\n    attempt &lt;&lt;- attempt + 1\n    Sys.sleep(5) # wait before retrying\n  })\n}\nif (!success)\n  stop(sprintf(\"Download failed after %s attempts\", attempt))"
  },
  {
    "objectID": "r2_clustering.html",
    "href": "r2_clustering.html",
    "title": "Identification of breast cancer subtypes",
    "section": "",
    "text": "The goal of this report is to identify breast cancer subtypes by using a graph-based approach. To this end, I built a K-nearest neighbor (KNN) graph, where each node is a patient connected to its nearest neighbors, in the high-dimensional space (i.e. I used the top 25 principal components and the top 3000 variable genes). Edges between patients are weighted based on the Jaccard similarity, the higher the weight the larger is their overlap in their local neighborhoods. I then applied the Louvain algorithm to identify patient communities, where patients in the same group are more strongly connected to each others compared to those in different groups (on the basis of gene expression profiles). Finally, I visualized the cluster distribution with t-SNE and UMAP and I labeled patients on the basis of status of 5 biomarkers (estrogen receptor (ER), progesterone receptor (PgR), human epidermal growth factor receptor 2 (HER2), Ki67, and Nottingham histologic grade (NHG)) to see if there are associations between patients communities and biomarker status."
  },
  {
    "objectID": "r2_clustering.html#aims",
    "href": "r2_clustering.html#aims",
    "title": "Identification of breast cancer subtypes",
    "section": "",
    "text": "The goal of this report is to identify breast cancer subtypes by using a graph-based approach. To this end, I built a K-nearest neighbor (KNN) graph, where each node is a patient connected to its nearest neighbors, in the high-dimensional space (i.e. I used the top 25 principal components and the top 3000 variable genes). Edges between patients are weighted based on the Jaccard similarity, the higher the weight the larger is their overlap in their local neighborhoods. I then applied the Louvain algorithm to identify patient communities, where patients in the same group are more strongly connected to each others compared to those in different groups (on the basis of gene expression profiles). Finally, I visualized the cluster distribution with t-SNE and UMAP and I labeled patients on the basis of status of 5 biomarkers (estrogen receptor (ER), progesterone receptor (PgR), human epidermal growth factor receptor 2 (HER2), Ki67, and Nottingham histologic grade (NHG)) to see if there are associations between patients communities and biomarker status."
  },
  {
    "objectID": "r2_clustering.html#methods",
    "href": "r2_clustering.html#methods",
    "title": "Identification of breast cancer subtypes",
    "section": "Methods",
    "text": "Methods\nDue to the presence of missing values (NA) in biomarker status (see Biomarkers Annotation), I explore these scenarios:\n\nClustering of all 3273 patients and annotation according to PAM50 subtypes (consensus histopathology labels​) provided in the metadata\nClustering of patients with complete annotations for all 5 biomarkers (1373 patients)\nClustering of patients separately for each biomarker (excluding NA)\n\nFor each clustering scenario, I considered the top 3000 genes that exhibit the highest patient-to-patient variation in the dataset (i.e, those genes that are highly expressed in some patients, and lowly expressed in others)."
  },
  {
    "objectID": "r2_clustering.html#results",
    "href": "r2_clustering.html#results",
    "title": "Identification of breast cancer subtypes",
    "section": "Results",
    "text": "Results\nOverall, the analysis suggests that patients stratify according to the 5 biomarkers, in particular according to ER and PgR status.\nLoading data\n\ngexp &lt;- read_csv(\n  'data/gene_expression_profile.csv.gz',\n  col_types = cols()\n)\n\nmeta &lt;- read_csv('data/metadata.csv', col_types = cols()) |&gt;\n  rename(sampleID = values, sampleName = ind) |&gt;\n  filter(sampleID %in% names(gexp)) |&gt;\n  mutate(\n    er_status = factor(er_status, levels = c(0, 1), labels = c(\"ER-\", \"ER+\")),\n    pgr_status = factor(pgr_status, levels = c(0, 1), labels = c(\"PgR-\", \"PgR+\")),\n    her2_status = factor(her2_status, levels = c(0, 1), labels = c(\"HER2-\", \"HER2+\")),\n    ki67_status = factor(ki67_status, levels = c(0, 1), labels = c(\"Ki67-\", \"Ki67+\")),\n    overall_survival_event = factor(overall_survival_event, levels = c(0, 1), labels = c(\"no survival\", \"survival\")),\n    endocrine_treated = factor(endocrine_treated, levels = c(0, 1), labels = c(\"no treated\", \"treated\")),\n    chemo_treated = factor(chemo_treated, levels = c(0, 1), labels = c(\"no treated\", \"treated\")),\n    lymph_node_group = factor(lymph_node_group),\n    lymph_node_status = factor(lymph_node_status),\n    pam50_subtype = factor(pam50_subtype),\n    nhg = factor(nhg)\n  )\n\n\nmd &lt;- meta |&gt; \n  column_to_rownames(var = 'sampleID')\n\nmx &lt;- gexp |&gt; \n  select(genes, rownames(md)) |&gt;\n  column_to_rownames(var = 'genes')\n\nBiomarkers Annotation\nPercentage of patients with a given annotation is reported for each biomarker.\n\nmeta_long &lt;- meta |&gt;\n  select(er_status, pgr_status, her2_status, ki67_status, nhg) |&gt;\n  pivot_longer(cols = everything(), names_to = \"biomarker\", values_to = \"status\")\n\ndf_counts &lt;- meta_long |&gt;\n  group_by(biomarker, status) |&gt;\n  summarise(n = n(), .groups = \"drop\") |&gt;\n  group_by(biomarker) |&gt;\n  mutate(perc = n / sum(n))\n\nggplot(df_counts, aes(x = biomarker, y = perc, fill = status)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(\n    aes(label = scales::percent(perc, accuracy = 1)),\n    position = position_stack(vjust = 0.5)\n  ) +\n  labs(x = \"\", y = \"\") +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  guides(fill = guide_legend(title = NULL)) +\n  theme_minimal() +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank()\n  )\n\n\n\n\n\n\n\nFunctions\nFunctions used to build the pipeline.\n\ntop_genes &lt;- function(mat, n_top = 500){\n  # use smallest between n_top and number of genes\n  n_top &lt;- min(n_top, nrow(mat))\n  # order by decreasing gene variance and slice\n  rv &lt;- matrixStats::rowVars(as.matrix(mat))\n  select_n &lt;- order(rv, decreasing = TRUE)[seq_len(n_top)]\n  mat &lt;- mat[select_n, ]\n  return(mat)\n}\n\nget_pc &lt;- function(mx, md){\n  pc &lt;- PCAtools::pca(mx, metadata = md, center = TRUE, scale = FALSE, removeVar = 0.1)\n  return(pc)\n}\n\nbuild_graph &lt;- function(pc, npc = 25, k = 20){\n  ## pc space\n  pcm &lt;- pc$rotated[, 1:npc]\n\n  # find k-nearest neighbors\n  knn_result &lt;- RANN::nn2(pcm, k = k)\n  knn_idx &lt;- knn_result$nn.idx\n  \n  # store sparse matrix triplets (i, j, value)\n  i_indices &lt;- c()\n  j_indices &lt;- c()\n  values &lt;- c()\n\n  npat &lt;- nrow(pcm)\n  # adj &lt;- matrix(0, npat, npat)\n\n  for (i in 1:npat){\n    for (j in knn_idx[i, ]){\n      if (i != j){ ## avoid self-loop\n        neighbors_i &lt;- knn_idx[i,]\n        neighbors_j &lt;- knn_idx[j,]\n        jaccard_sim &lt;- length(intersect(neighbors_i, neighbors_j)) / \n                       length(union(neighbors_i, neighbors_j)) ## union takes unique values\n\n        # Store indices and values for sparse matrix\n        i_indices &lt;- c(i_indices, i)\n        j_indices &lt;- c(j_indices, j)\n        values &lt;- c(values, jaccard_sim)\n        \n        # dense matrix \n        # adjt[i, j] &lt;- jaccard_sim\n      }\n    }\n  }\n\n  # Create sparse matrix using triplet format\n  adj &lt;- sparseMatrix(i = i_indices, j = j_indices, x = values, dims = c(npat, npat))\n\n  # build graph\n  g &lt;- graph_from_adjacency_matrix(adj,\n    mode = \"max\", ## preserve the strongest connections, same of adj &lt;- pmax(adj, t(adj))\n    weighted = TRUE\n  )\n  return(g)\n}\n\nfind_clusters &lt;- function(g, mx, resolution = 1){\n  # Louvain algorithm for community detection\n  louvain_communities &lt;- cluster_louvain(g, resolution = resolution)\n\n  # cluster assignment\n  clusters &lt;- membership(louvain_communities)\n\n  return(clusters)\n}\n\n## umap wrapper\numap &lt;- function(mx, ...){\n  defaults &lt;- list(\n    n_components = 2,\n    n_neighbors = 20,     ## as perplexity in t-SNE\n    min_dist = 0.1,       ## how tightly points cluster together\n    metric = \"euclidean\",\n    spread = 1,           ## global structure preservation\n    n_threads = 10\n  )\n  user_args &lt;- modifyList(defaults, list(...))\n  purrr::exec(uwot::umap, X = t(mx), !!!user_args)\n}\n\n## tsne wrapper\ntsne &lt;- function(mx, ...){\n  defaults &lt;- list(\n    dims = 2,\n    perplexity = 20,\n    num_threads = 10\n  )\n  user_args &lt;- modifyList(defaults, list(...))\n  purrr::exec(Rtsne::Rtsne, X = t(mx), !!!user_args)\n}\n\njoin_results &lt;- function(tib, pc, clusters){\n  if(is.list(tib)){ ## tsne returns a list ..\n    restib &lt;- tibble(\n      sample = rownames(pc$rotated),\n      cluster = as.factor(clusters),\n      tsne1 = tib$Y[, 1],\n      tsne2 = tib$Y[, 2]) |&gt;\n      left_join(meta, by = c('sample' = 'sampleID'))\n  }else{ ## .. umap a dataframe\n    restib &lt;- tibble(\n      sample = rownames(pc$rotated),\n      cluster = as.factor(clusters),\n      umap1 = tib[, 1],\n      umap2 = tib[, 2]) |&gt;\n      left_join(meta, by = c('sample' = 'sampleID'))\n  }\n  return(restib)\n}\n\nplot_clusters &lt;- function(tib, cluster = 'cluster', animate = FALSE){\n  dim1 &lt;- colnames(tib)[3]\n  dim2 &lt;- colnames(tib)[4]\n  \n  p &lt;- ggplot(tib, aes(x = !!sym(dim1), y = !!sym(dim2), color = !!sym(cluster))) +\n    geom_point(alpha = 0.7, size = 2) +\n    labs(title = str_replace(cluster, '_', ' '), x = dim1, y = dim2) +\n    theme_minimal()\n\n  if(!animate & cluster == 'cluster'){\n    # Calculate cluster centroids for label positioning\n    cluster_centers &lt;- tib |&gt;\n      group_by(!!sym(cluster)) |&gt;\n      summarise(\n        x_center = mean(!!sym(dim1), na.rm = TRUE),\n        y_center = mean(!!sym(dim2), na.rm = TRUE),\n        .groups = 'drop'\n      )\n    \n    p &lt;- p +\n      geom_text_repel(\n        data = cluster_centers,\n        aes(x = x_center, y = y_center, label = !!sym(cluster)),\n        color = \"black\",\n        size = 8,\n        fontface = \"bold\",\n        vjust = 0.5,\n        hjust = 0.5\n      )\n  }\n  return(p)\n}\n\nformatter &lt;- function(tib, cols_to_format = \"all\", digits = 3){\n  selector &lt;- if(length(cols_to_format) == 1 && cols_to_format == \"all\"){\n    where(is.numeric)\n  } else {\n    all_of(cols_to_format)\n  }\n\n  tib |&gt; \n    mutate(across({{ selector }},\n           ~format(., scientific = TRUE, digits = digits)))\n}\n\ndatatable &lt;- function(tib, row2display = 10) {\n  if(nrow(tib) &gt; 0){\n    DT::datatable(tib,\n      rownames   = FALSE,\n      extensions = \"Buttons\",\n      options    = list(\n        dom = \"Bfrtip\",\n        scrollX = TRUE,\n        pageLength = row2display,\n        buttons = list(\n          list(\n            extend  = \"collection\",\n            buttons = c(\"csv\", \"excel\"),\n            text    = \"Download\"\n          )\n        )\n      )\n    )\n  }else{\n    print(\"No results\")\n  }\n}\n\nA. Clustering all the patients\n\nmxtop &lt;- top_genes(mx, n_top = 3000)\npc &lt;- get_pc(mxtop, md)\ng &lt;- build_graph(pc, npc = 25, k = 20)\ncls &lt;- find_clusters(g, mxtop, resolution = 1)\n\nt-SNE\n\ntsneres &lt;- tsne(mxtop, perplexity = 20)\ntsnetib &lt;- join_results(tsneres, pc, cls)\n\nvars &lt;- c('cluster', 'pam50_subtype')\nplist &lt;- map(vars, ~plot_clusters(tsnetib, .x))\nwrap_plots(plist, ncol = 2)\n\n\n\n\n\n\n\nUMAP\n\numapres &lt;- umap(mxtop, n_neighbors = 20, spread = 1)\numaptib &lt;- join_results(umapres, pc, cls)\n\nvars &lt;- c('cluster', 'pam50_subtype')\nplist &lt;- map(vars, ~plot_clusters(umaptib, .x))\nwrap_plots(plist, ncol = 2)\n\n\n\n\n\n\n\nPatient cluster assignment\nThe table below shows the obtained cluster assignment for each patient.\n\ntsnetib |&gt; \n  left_join(umaptib, by = 'sample', suffix=c('_tsne', '_umap')) |&gt;\n  select(sample, sampleName_tsne, cluster_tsne, \n  tsne1, tsne2, umap1, umap2, \n  tumor_size_tsne, lymph_node_group_tsne, \n  lymph_node_status_tsne, er_status_tsne, \n  pgr_status_tsne, her2_status_tsne,           \n  ki67_status_tsne, nhg_tsne, \n  overall_survival_days_tsne, overall_survival_event_tsne,\n  endocrine_treated_tsne, chemo_treated_tsne) |&gt; \n  rename_with(~ str_remove(.x, \"_tsne$\")) |&gt;\n  formatter(cols_to_format = c(\"tsne1\", \"tsne2\", \"umap1\", \"umap2\")) |&gt;\n  datatable()\n\n\n\n\n\nB. Clustering of patients with complete annotations for all 5 biomarkers\n\n## reduce dataset to complete cases \nmd_complete &lt;- meta |&gt;\n  filter(!is.na(er_status)   & !is.na(pgr_status) & \n         !is.na(her2_status) & !is.na(ki67_status) &\n         !is.na(nhg)) |&gt;\n  column_to_rownames(var = 'sampleID')       \n\nmx_complete &lt;- gexp |&gt; \n    select(genes, rownames(md_complete)) |&gt;\n    column_to_rownames(var = 'genes')\n\n\n## run pipeline\nmxtop_complete &lt;- top_genes(mx_complete, n_top = 3000)\npc_complete &lt;- get_pc(mxtop_complete, md_complete)\ng_complete &lt;- build_graph(pc_complete, npc = 25, k = 20)\ncls_complete &lt;- find_clusters(g_complete, mxtop_complete, resolution = 1)\n\nt-SNE\n\nvars &lt;- c('cluster', 'er_status', 'pgr_status', 'her2_status', 'ki67_status', 'nhg')\n\n## tsne\ntsneres_complete &lt;- tsne(mxtop_complete, perplexity = 20)\ntsnetib_complete &lt;- join_results(tsneres_complete, pc_complete, cls_complete)\n\nplist &lt;- map(vars, ~plot_clusters(tsnetib_complete, .x))\nwrap_plots(plist, ncol = 2)\n\n\n\n\n\n\n\nUMAP\n\numapres_complete &lt;- umap(mxtop_complete, n_neighbors = 20, spread = 1)\numaptib_complete &lt;- join_results(umapres_complete, pc_complete, cls_complete)\n\nplist &lt;- map(vars, ~plot_clusters(umaptib_complete, .x))\nwrap_plots(plist, ncol = 2)\n\n\n\n\n\n\n\nC. Clustering of patients separately for each biomarker\n\nbiomarkers &lt;- c('er_status', 'pgr_status', 'her2_status', 'ki67_status', 'nhg')\n\ntsnetib_bm &lt;- list()\numaptib_bm &lt;- list()\n\nptsne &lt;- list()\npumap &lt;- list()\n\nfor(biomarker in biomarkers){\n  ## remove not annotated patients\n  md_bm &lt;- meta |&gt;\n    filter(!is.na(!!sym(biomarker))) |&gt;\n    column_to_rownames(var = 'sampleID')\n\n  mx_bm &lt;- gexp |&gt; \n    select(genes, rownames(md_bm)) |&gt;\n    column_to_rownames(var = 'genes')\n\n  ## clustering\n  mxtop_bm &lt;- top_genes(mx_bm, n_top = 3000)\n  pc_bm &lt;- get_pc(mxtop_bm, md_bm)\n  g_bm &lt;- build_graph(pc_bm, npc = 25, k = 20)\n  cls_bm &lt;- find_clusters(g_bm, mxtop_bm, resolution = 1)\n\n  ## save results\n  tsneres_bm &lt;- tsne(mxtop_bm, perplexity = 20)\n  tsnetib_bm[[biomarker]] &lt;- join_results(tsneres_bm, pc_bm, cls_bm)\n\n  umapres_bm &lt;- umap(mxtop_bm,  n_neighbors = 20)\n  umaptib_bm[[biomarker]] &lt;- join_results(umapres_bm, pc_bm, cls_bm)\n\n  vars &lt;- c('cluster', biomarker)\n  ptsne[[biomarker]] &lt;- map(vars, ~plot_clusters(tsnetib_bm[[biomarker]], .x))\n  pumap[[biomarker]] &lt;- map(vars, ~plot_clusters(umaptib_bm[[biomarker]], .x))\n}\n\nt-SNE\n\npwrap &lt;- map(ptsne, ~.x[[1]] + .x[[2]])\nwrap_plots(pwrap, ncol = 1)\n\n\n\n\n\n\n\nUMAP\n\npwrap &lt;- map(pumap, ~.x[[1]] + .x[[2]])\nwrap_plots(pwrap, ncol = 1)\n\n\n\n\n\n\n\nClustering at different resolutions\nClusters detected at each resolutions are marked in different colors. Patients in the community colored in red tend to cluster together at different resolutions, suggesting that patients within this community are strongly connected (similar expression profiles).\n\n# loop for resolutions\nresolutions &lt;- c(0.1, 0.3, 0.6, 1, 1.5)\n\ntsne_tune &lt;- map_dfr(resolutions, function(resolution){\n  cls &lt;- find_clusters(g, mxtop, resolution = resolution) \n  tsne &lt;- tsne(mxtop, dims = 2, perplexity = 20)\n  tibble(\n    sample = colnames(mxtop),\n    cluster = as.factor(cls),\n    tsne1 = tsne$Y[, 1],\n    tsne2 = tsne$Y[, 2],\n    resolution = resolution,\n    ncl = length(unique(cls)) ## number of clusters at a given resolution\n  ) |&gt;\n  left_join(meta, by = c('sample' = 'sampleID'))\n})\n\n# {unique(tsne_tune$ncl[tsne_tune$resolution == closest_state])}\nncl_lookup &lt;- tsne_tune |&gt; distinct(resolution, ncl) |&gt; deframe()\n\nplot &lt;- plot_clusters(tsne_tune, cluster = 'cluster', animate = TRUE) +\n  # theme(legend.position=\"none\") +\n  labs(subtitle = \"Resolution: {closest_state} | Clusters: {ncl_lookup[as.character(closest_state)]}\") +\n  transition_states(resolution, transition_length = 5, state_length = 3) +\n  ease_aes(\"linear\")\n\nanimate(\n  plot,\n  width = 8,\n  height = 6,\n  res = 100,\n  nframes = 300,\n  fps = 30,\n  device = \"ragg_png\",\n  renderer = gifski_renderer()\n)\n\n\n\n\n\n\n\nWhen specifically coloring patients based on ER status, we observe that the expression profiles of ER- patients cluster well. Furthermore, it is worth noting that the status of some patients within this cluster is unknown (gray patients). These might be considered as ER- patients since they cluster strongly with patients annotated as ER-.\n\nplot &lt;- plot_clusters(tsne_tune, cluster = 'er_status', animate = TRUE) +\n  labs(subtitle = \"Resolution: {closest_state} | Clusters: {ncl_lookup[as.character(closest_state)]}\") +\n  transition_states(resolution, transition_length = 5, state_length = 3) +\n  ease_aes(\"linear\")\n\nanimate(\n  plot,\n  width = 8,\n  height = 6,\n  res = 100,\n  nframes = 300,\n  fps = 30,\n  device = \"ragg_png\",\n  renderer = gifski_renderer()\n)"
  },
  {
    "objectID": "r2_clustering.html#conclusions",
    "href": "r2_clustering.html#conclusions",
    "title": "Identification of breast cancer subtypes",
    "section": "Conclusions",
    "text": "Conclusions\nI identified breast cancer subtypes based on gene expression data via a graph-based approach. Two main distinct clusters of patients are detected. The smaller cluster is enriched for ER- patients and likely represent the most aggressive expression profile (basal PAM50 subtype, e.g. G3, PgR-, ER-, HER-)."
  },
  {
    "objectID": "r2_clustering.html#next-step",
    "href": "r2_clustering.html#next-step",
    "title": "Identification of breast cancer subtypes",
    "section": "Next step",
    "text": "Next step\nCluster assignment can be used as features in a supervised learning approach to predict biomarker status (such as random forest, support vector machine, label propagation). Furthermore, we can use LLMs to further validate biological insights using AI-powered tools tailored for bioinformatics resources such as ExpasyGPT."
  }
]