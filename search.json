[
  {
    "objectID": "weather-records.html",
    "href": "weather-records.html",
    "title": "Visualize Michigan’s temperature records",
    "section": "",
    "text": "Plot line graphs of the min and max temperatures for the years 2005 through 2014 highlighting the daily 2015 temperatures that exceeded those values."
  },
  {
    "objectID": "weather-records.html#goal",
    "href": "weather-records.html#goal",
    "title": "Visualize Michigan’s temperature records",
    "section": "",
    "text": "Plot line graphs of the min and max temperatures for the years 2005 through 2014 highlighting the daily 2015 temperatures that exceeded those values."
  },
  {
    "objectID": "weather-records.html#dataset",
    "href": "weather-records.html#dataset",
    "title": "Visualize Michigan’s temperature records",
    "section": "Dataset",
    "text": "Dataset\nThe data comes from a subset of The National Centers for Environmental Information (NCEI) Global Historical Climatology Network daily (GHCNd) (GHCN-Daily), which includes daily climate records from thousands of land surface stations across the globe.\nHere I use data from the Ann Arbor Michigan location.\nEach row in this datafile corresponds to a single observation from a weather station, and has the following variables:\n\nid : station identification code\ndate : date in YYYY-MM-DD format (e.g. 2012-01-24 = January 24, 2012)\nelement : indicator of element type\n\nTMAX : Maximum temperature (tenths of degrees C)\nTMIN : Minimum temperature (tenths of degrees C)\n\nvalue : data value for element (tenths of degrees C)\n\n\n#  I'll be using the folium package to render the data into a map in Jupyter.\n\nimport folium\nimport pandas as pd\n\n# get the location information for this dataset\ndf = pd.read_csv('data/BinSize_d400.csv')\nstation_locations_by_hash = df[df['hash'] == 'fb441e62df2d58994928907a91895ec62c2c42e6cd075c2700843b89']\n\n# get longitude and lattitude to plot\nlons = station_locations_by_hash['LONGITUDE'].tolist()\nlats = station_locations_by_hash['LATITUDE'].tolist()\n\n# plot on a beautiful folium map\nmy_map = folium.Map(location = [lats[0], lons[0]], height = 500,  zoom_start = 9)\nfor lat, lon in zip(lats, lons):\n    folium.Marker([lat, lon]).add_to(my_map)\n\n# render map in Jupyter\ndisplay(my_map)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nStep 1\nLoad the dataset and transform the data into Celsius (refer to documentation) then extract all of the rows which have minimum or maximum temperatures.\n\nimport pandas as pd\ndf = pd.read_csv('data/temperatures.csv')\ndf.head()\n\n\n\n\n\n\n\n\nID\nDate\nElement\nData_Value\n\n\n\n\n0\nUSW00094889\n2014-11-12\nTMAX\n22\n\n\n1\nUSC00208972\n2009-04-29\nTMIN\n56\n\n\n2\nUSC00200032\n2008-05-26\nTMAX\n278\n\n\n3\nUSC00205563\n2005-11-11\nTMAX\n139\n\n\n4\nUSC00200230\n2014-02-27\nTMAX\n-106\n\n\n\n\n\n\n\n\n# In this code cell, transform the Data_Value column\ndf['Data_Value_Tenths'] = df['Data_Value'] / 10\n\n## drop leap day\ndf['Date'] = pd.to_datetime(df['Date'])\ndf = df[~((df['Date'].dt.month == 2) & (df['Date'].dt.day == 29))]\n\ndf_max = df[df['Element'] == 'TMAX']\ndf_min = df[df['Element'] == 'TMIN']\n\nprint(df.shape, df_min.shape, df_max.shape)\n\n(165002, 5) (81982, 5) (83020, 5)\n\n\n\n\nStep 2\nIn order to visualize the data we would plot the min and max data for each day of the year between the years 2005 and 2014 across all weather stations. But we also need to find out when the min or max temperature in 2015 falls below the min or rises above the max for the previous decade.\n\n# create a DataFrame of maximum temperature by date\nmax_temp_date = df_max.groupby('Date')['Data_Value_Tenths'].max().reset_index()\n\n# create a DataFrame of minimum temperatures by date\nmin_temp_date = df_min.groupby('Date')['Data_Value_Tenths'].min().reset_index()\n\nprint(max_temp_date.shape, min_temp_date.shape)\n\n(4015, 2) (4015, 2)\n\n\n\n\nStep 3\nNow that we have grouped the daily max and min temperatures for each day of the years 2005 through 2015, we can separate out the data for 2015.\n\nimport numpy as np\n\n# calculate the minimum and maximum values for the day of the year for 2005 through 2014\nmax_temp_0514 = max_temp_date[max_temp_date['Date'].dt.year &lt; 2015]\nmax_temp_0514 = max_temp_0514.groupby(max_temp_0514['Date'].dt.strftime('%m-%d')).agg({'Data_Value_Tenths': 'max'})\nmax_temp_0514.rename(columns={'Data_Value_Tenths': 'Max_Temp_0514'}, inplace=True)\n\nmin_temp_0514 = min_temp_date[min_temp_date['Date'].dt.year &lt; 2015]\nmin_temp_0514 = min_temp_0514.groupby(min_temp_0514['Date'].dt.strftime('%m-%d')).agg({'Data_Value_Tenths': 'min'})\nmin_temp_0514.rename(columns={'Data_Value_Tenths': 'Min_Temp_0514'}, inplace=True)\n\n# calculate the minimum and maximum values for the years 2015\nmax_temp_2015 = max_temp_date[max_temp_date['Date'].dt.year == 2015]\nmax_temp_2015 = max_temp_2015.groupby(max_temp_2015['Date'].dt.strftime('%m-%d')).agg({'Data_Value_Tenths': 'max'})\nmax_temp_2015.rename(columns={'Data_Value_Tenths': 'Max_Temp_2015'}, inplace=True)\n\nmin_temp_2015 = min_temp_date[min_temp_date['Date'].dt.year == 2015]\nmin_temp_2015 = min_temp_2015.groupby(min_temp_2015['Date'].dt.strftime('%m-%d')).agg({'Data_Value_Tenths': 'min'})\nmin_temp_2015.rename(columns={'Data_Value_Tenths': 'Min_Temp_2015'}, inplace=True)\n\n## join df and find broken records\ndf_temp = pd.merge(max_temp_0514, min_temp_0514, on='Date').merge(max_temp_2015, on='Date').merge(min_temp_2015, on='Date')\ndf_temp['Broke_Record_Max'] = df_temp['Max_Temp_2015'] &gt; df_temp['Max_Temp_0514']\ndf_temp['Broke_Record_Min'] = df_temp['Min_Temp_2015'] &lt; df_temp['Min_Temp_0514']\n\nprint(max_temp_0514.shape, min_temp_0514.shape, max_temp_2015.shape, min_temp_2015.shape, df_temp.shape)\n\n(365, 1) (365, 1) (365, 1) (365, 1) (365, 6)\n\n\n\n\nStep 4\nNow it’s time to plot!\n\nimport matplotlib.pyplot as plt\nfrom calendar import month_abbr\n\n# put your plotting code here!\nfig, ax = plt.subplots(figsize=(12, 6))\n\nplt.plot(df_temp['Max_Temp_0514'].values, label='Max Temp (2005-2014)', linewidth=1, alpha = 0.7, c='firebrick')\nplt.plot(df_temp['Min_Temp_0514'].values, label='Min Temp (2005-2014)', linewidth=1, alpha = 0.7, c='royalblue')\n\nplt.fill_between(range(df_temp.shape[0]), df_temp['Max_Temp_0514'], df_temp['Min_Temp_0514'], facecolor='gray', alpha=0.3)\n\nplt.scatter(np.where(df_temp['Broke_Record_Max'] == True), \n            df_temp[df_temp['Broke_Record_Max'] == True]['Max_Temp_2015'].values,\n            s=15, color='red', label='High Temp Broken (2015)', marker='^')\nplt.scatter(np.where(df_temp['Broke_Record_Min'] == True), \n            df_temp[df_temp['Broke_Record_Min'] == True]['Min_Temp_2015'].values,\n            s=15, color='indigo', label='Low Temp Broken (2015)', marker='v')\n\nplt.legend(loc = 'upper right', fontsize=8)\n\nplt.xticks(np.linspace(0, 365, num = 12), list(month_abbr)[1:], size=10)\nplt.yticks(size=10)\nplt.xlabel('Months', size = 12)\nplt.ylabel('Temperature (Celsius)', size = 12)\nplt.title('Temperature Patterns', size = 14)\n\nax.spines[['right', 'top']].set_visible(False)\n# ax.spines[['bottom', 'left']].set_alpha(0.3)\n# ax.tick_params(axis='x', color='gray')\n# ax.tick_params(axis='y', color='gray')\n\n# plt.savefig('temperature_plot.png')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAll in one\n\nimport pandas as pd\nimport numpy as np\nfrom calendar import month_abbr\n\n## load data\ndf = pd.read_csv('data/temperatures.csv')\n\n## isolate year,month,day\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['Year'] = df['Date'].dt.year\ndf['Month'] = df['Date'].dt.strftime('%b') ## month short form\ndf['Day'] = df['Date'].dt.day\n\n## sort month alphabethically\nmonth_order = list(month_abbr)[1:]\ndf['Month'] = pd.Categorical(df['Month'], categories=month_order, ordered=True)\n\n## drop leap day\ndf = df[~((df['Month'] == 'Feb') & (df['Day'] == 29))]\n\n## transform temp\ndf['Data_Value'] = df['Data_Value'] / 10\n\n## group temps\nmax_temp_0514 = df[(df['Element'] == 'TMAX') & (df['Year'] &lt; 2015)].groupby(['Month','Day'], observed=True).agg({'Data_Value': 'max'})\nmin_temp_0514 = df[(df['Element'] == 'TMIN') & (df['Year'] &lt; 2015)].groupby(['Month','Day'], observed=True).agg({'Data_Value': 'min'})\nmax_temp_2015 = df[(df['Element'] == 'TMAX') & (df['Year'] == 2015)].groupby(['Month','Day'], observed=True).agg({'Data_Value': 'max'})\nmin_temp_2015 = df[(df['Element'] == 'TMIN') & (df['Year'] == 2015)].groupby(['Month','Day'], observed=True).agg({'Data_Value': 'min'})\n\n## rename\nmax_temp_0514.rename(columns={'Data_Value': 'Max_Temp_0514'}, inplace=True)\nmin_temp_0514.rename(columns={'Data_Value': 'Min_Temp_0514'}, inplace=True)\nmax_temp_2015.rename(columns={'Data_Value': 'Max_Temp_2015'}, inplace=True)\nmin_temp_2015.rename(columns={'Data_Value': 'Min_Temp_2015'}, inplace=True)\n\n## join df and find broken records\ndf_temp = pd.merge(max_temp_0514, min_temp_0514, on=['Month', 'Day'])\\\n            .merge(max_temp_2015, on=['Month', 'Day'])\\\n            .merge(min_temp_2015, on=['Month', 'Day'])\ndf_temp['Broke_Record_Max'] = df_temp['Max_Temp_2015'] &gt; df_temp['Max_Temp_0514']\ndf_temp['Broke_Record_Min'] = df_temp['Min_Temp_2015'] &lt; df_temp['Min_Temp_0514']\n\n## drop 29/30/31 in Feb because of merge\ndf_temp.dropna(inplace=True)\n\n# plotting \nfig, ax = plt.subplots(figsize=(12, 6))\n\nplt.plot(df_temp['Max_Temp_0514'].values, label='Max Temp (2005-2014)', linewidth=1, alpha = 0.7, c='firebrick')\nplt.plot(df_temp['Min_Temp_0514'].values, label='Min Temp (2005-2014)', linewidth=1, alpha = 0.7, c='royalblue')\n\nplt.fill_between(range(df_temp.shape[0]), df_temp['Max_Temp_0514'], df_temp['Min_Temp_0514'], facecolor='gray', alpha=0.3)\n\nplt.scatter(np.where(df_temp['Broke_Record_Max'] == True), \n            df_temp[df_temp['Broke_Record_Max'] == True]['Max_Temp_2015'].values,\n            s=15, color='red', label='High Temp Broken (2015)', marker='^')\nplt.scatter(np.where(df_temp['Broke_Record_Min'] == True), \n            df_temp[df_temp['Broke_Record_Min'] == True]['Min_Temp_2015'].values,\n            s=15, color='indigo', label='Low Temp Broken (2015)', marker='v')\n\nplt.legend(loc = 'best', fontsize=10)\n\nplt.xticks(np.linspace(0, 365, num = 12), list(month_abbr)[1:], size=10)\nplt.yticks(size=10)\nplt.xlabel('Months', size = 12)\nplt.ylabel('Temperature (Celsius)', size = 12)\nplt.title('Temperature Patterns', size = 14)\n\nax.spines[['right', 'top']].set_visible(False)\n# ax.spines[['bottom', 'left']].set_alpha(0.3)\n# ax.tick_params(axis='x', color='gray')\n# ax.tick_params(axis='y', color='gray')\n\nplt.show()"
  },
  {
    "objectID": "ml-graph.html",
    "href": "ml-graph.html",
    "title": "Predicting salary and new connections from network data",
    "section": "",
    "text": "Note\n\n\n\nNote: data from the Coursera course Applied Social Network Analysis in Python\nimport networkx as nx\nimport pandas as pd\nimport numpy as np\nimport pickle\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nimport random"
  },
  {
    "objectID": "ml-graph.html#company-emails",
    "href": "ml-graph.html#company-emails",
    "title": "Predicting salary and new connections from network data",
    "section": "Company Emails",
    "text": "Company Emails\nHere I analzye a company’s email network where a node corresponds to a person and an edge indicates that at least one email has been sent between two people. The network also contains the node attributes Department and ManagmentSalary. Department indicates the department in the company which the person belongs to, and ManagmentSalary indicates whether that person is receiving a managment position salary.\n\nG = pickle.load(open('data/email_prediction.txt', 'rb'))\n\nprint(f\"Graph with {len(nx.nodes(G))} nodes and {len(nx.edges(G))} edges\")\n\nGraph with 1005 nodes and 16706 edges"
  },
  {
    "objectID": "ml-graph.html#salary-prediction",
    "href": "ml-graph.html#salary-prediction",
    "title": "Predicting salary and new connections from network data",
    "section": "Salary Prediction",
    "text": "Salary Prediction\nHere I aim at predicting if people whithout a ManagementSalary will receive a managment position salary. To this end, I train a classifier on people that have ManagementSalary and then I predict the probability of receiving a managment salary for those people where ManagementSalary is missing. Models are evaluated with different performance metrics, Area Under the ROC Curve (AUCROC), Area under the Precision Recall Curve (AUPRC) and Balanced Accuracy.\n\nlist(G.nodes(data=True))[:5] # print the first 5 nodes\n\n[(0, {'Department': 1, 'ManagementSalary': 0.0}),\n (1, {'Department': 1, 'ManagementSalary': nan}),\n (581, {'Department': 3, 'ManagementSalary': 0.0}),\n (6, {'Department': 25, 'ManagementSalary': 1.0}),\n (65, {'Department': 4, 'ManagementSalary': nan})]\n\n\n\ndef build_salary_dataset(G):\n    df = pd.DataFrame()\n    df['management_salary'] = nx.get_node_attributes(G, 'ManagementSalary')\n    df['department'] = nx.get_node_attributes(G, 'Department')\n    df['clustering'] = nx.clustering(G)\n    df['betweenness'] = nx.betweenness_centrality(G)\n    df['closenness'] = nx.closeness_centrality(G)\n    df['pagerank'] = nx.pagerank(G)\n    df['hubs'], df['auth'] = nx.hits(G)\n    df.sort_index(inplace=True)\n    return df\n\ndef get_feature_and_label(df, label='management_salary'):\n    X = df.drop(columns=[label]) ## df.iloc[:,1:]\n    y = df[label]                ## df.iloc[:,0]\n    return X,y\n\nclass GridDict(dict):\n    def __init__(self, *args, **kwargs):\n        super(GridDict, self).__init__(*args, **kwargs)\n        self.best_params_ = self\n\ndef get_best_parameters(X, y, tune=True):\n    # from sklearn.model_selection import train_test_split\n    ## with random_state=0 same as splitting below\n    # X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    X_train, y_train = X.loc[~y.isna(),:], y[~y.isna()]\n    X_test, y_test = X.loc[y.isna(),:], y[y.isna()]\n\n    # Create the pipeline: in this way the scaling is included in each cv, making the model selection more robust\n    pipeline = Pipeline([('scaler', StandardScaler()), # Step 1: Scale the data \n                         ('rfc', RandomForestClassifier(random_state=0)) # Step 2: Train the model \n                        ])\n    ## grid search\n    param_grid = {\n        'rfc__n_estimators': [50, 100, 200],\n        'rfc__max_depth': [10, 20, 30],\n        'rfc__min_samples_split': [2, 5, 10],\n        'rfc__min_samples_leaf': [1, 2, 4],\n        'rfc__max_features': ['sqrt', 'log2']\n    }\n\n    if tune:\n        rfc = RandomForestClassifier(random_state=0)\n        grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n        grid_search.fit(X_train, y_train)\n        return grid_search\n    else:\n        ## random subsampling grid search to avoid grader's TimeoutError\n        ## to select the first elem: [value[0]]\n        param_grid = {key: random.choice(value) for key, value in param_grid.items()}\n        return GridDict(param_grid)\n\n\n## build dataset\ndf = build_salary_dataset(G)\nX, y = get_feature_and_label(df)\n\n# from sklearn.model_selection import train_test_split\n## with random_state=0 same as splitting below\n# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nX_train, y_train = X.loc[~y.isna(),:], y[~y.isna()]\nX_test, y_test = X.loc[y.isna(),:], y[y.isna()]\n\n\ndef salary_predictions(X_train, y_train, X_test, y_test):\n    ### scale dataset\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    ## gest best parameter\n    best_param = get_best_parameters(X, y, tune=True)\n    \n    ## train model\n    clf = RandomForestClassifier(max_depth=best_param.best_params_['rfc__max_depth'], \n                                 max_features=best_param.best_params_['rfc__max_features'], \n                                 min_samples_leaf=best_param.best_params_['rfc__min_samples_leaf'], \n                                 min_samples_split=best_param.best_params_['rfc__min_samples_split'], \n                                 n_estimators=best_param.best_params_['rfc__n_estimators'], \n                                 n_jobs=-1, random_state=0)\n\n    clf.fit(X_train_scaled, y_train)\n    \n    ## return proba\n    y_pred = clf.predict_proba(X_test_scaled)\n    probs = y_pred[:,1]\n    return pd.Series(probs, index=X_test.index)\n\npred = salary_predictions(X_train, y_train, X_test, y_test)\npred\n\n1       0.082298\n2       0.900215\n5       0.990329\n8       0.062392\n14      0.095849\n          ...   \n992     0.000000\n994     0.001667\n996     0.000000\n1000    0.010440\n1001    0.026925\nLength: 252, dtype: float64\n\n\n\nPerformance evaluation\n\nmask = ~y.isna()\nX_known, y_known = X[mask], y[mask]\n\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('rfc', RandomForestClassifier(random_state=0))\n])\n\nmetrics = ['roc_auc', 'average_precision', 'balanced_accuracy']\nperfname = ['AUROC', 'AUPRC', 'Balanced Accuracy']\n\nfor perf, name in zip(metrics, perfname):\n  scores = cross_val_score(pipeline, X_known, y_known, cv=5, scoring=perf)\n  print(f\"Averaged {name}: {scores.mean():.3f}\")\n\nAveraged AUROC: 0.954\nAveraged AUPRC: 0.851\nAveraged Balanced Accuracy: 0.771"
  },
  {
    "objectID": "ml-graph.html#new-connections-prediction",
    "href": "ml-graph.html#new-connections-prediction",
    "title": "Predicting salary and new connections from network data",
    "section": "New Connections Prediction",
    "text": "New Connections Prediction\nHere I aim at predicting future connections between employees of the network. The future connections information is loaded into the variable future_connections. The index is a tuple indicating a pair of nodes that currently do not have a connection, and the Future Connection column indicates if an edge between those two nodes will exist in the future, where a value of 1.0 indicates a future connection.\n\nfuture_connections = pd.read_csv('data/future_connections.csv', index_col=0, converters={0: eval})\nfuture_connections\n\n\n\n\n\n\n\n\nFuture Connection\n\n\n\n\n(6, 840)\n0.0\n\n\n(4, 197)\n0.0\n\n\n(620, 979)\n0.0\n\n\n(519, 872)\n0.0\n\n\n(382, 423)\n0.0\n\n\n...\n...\n\n\n(165, 923)\nNaN\n\n\n(673, 755)\nNaN\n\n\n(939, 940)\nNaN\n\n\n(555, 905)\nNaN\n\n\n(75, 101)\nNaN\n\n\n\n\n488446 rows × 1 columns\n\n\n\nHere I aim at predicting if people will have a future connections. To this end:\n\nI creat a matrix of features for the edges found in future_connections using Networkx\n\nI train a sklearn classifier on those edges in future_connections that have Future Connection data\n\nI predict a probability of the edge being a future connection for those edges in future_connections where Future Connection is missing.\nI evaluate the model with different metrics - AUROC, AUPRC and Balaced Accuracy\n\n\n## build data\ndef build_connection_dataset(future_connections, G):\n    df = future_connections.copy() ## create a deep copy of future_connections\n    df.rename(columns={'Future Connection': 'future_conn'}, inplace=True)\n    map_edges = lambda ed: dict({(x,y):z for x,y,z in ed})\n    df['common_neigh'] = map_edges([(e[0], e[1], len(list(nx.common_neighbors(G, e[0], e[1])))) for e in nx.non_edges(G)])\n    df['jaccard_coef'] = map_edges(nx.jaccard_coefficient(G))\n    df['alloc_index']  = map_edges(nx.resource_allocation_index(G))\n    df['adamic_adar']  = map_edges(nx.adamic_adar_index(G))\n    df['pref_attach']  = map_edges(nx.preferential_attachment(G))\n    df.sort_index(inplace=True)\n    return df\n\ndef get_feature_and_label(df, label='future_conn'):\n    X = df.drop(columns=[label]) ## df.iloc[:,1:]\n    y = df[label]                ## df.iloc[:,0]\n    return X,y\n\ndf = build_connection_dataset(future_connections, G)\nX, y = get_feature_and_label(df, label='future_conn')\n\n# from sklearn.model_selection import train_test_split\n## with random_state=0 same as splitting below\n# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nX_train, y_train = X.loc[~y.isna(),:], y[~y.isna()]\nX_test, y_test = X.loc[y.isna(),:], y[y.isna()]\n\n\nclass GridDict(dict):\n    def __init__(self, *args, **kwargs):\n        super(GridDict, self).__init__(*args, **kwargs)\n        self.best_params_ = self\n\ndef get_best_parameters(X_train, y_train, tune=True):\n    # Create the pipeline: in this way the scaling is included in each cv, making the model selection more robust\n    pipeline = Pipeline([('scaler', StandardScaler()), # Step 1: Scale the data \n                         ('rfc', RandomForestClassifier(random_state=0)) # Step 2: Train the model \n                        ])\n    ## grid search\n    param_grid = {\n        'rfc__n_estimators': [50, 100, 200],\n        'rfc__max_depth': [10, 20, 30],\n        'rfc__min_samples_split': [2, 5, 10],\n        'rfc__min_samples_leaf': [1, 2, 4],\n        'rfc__max_features': ['sqrt', 'log2']\n    }\n    \n    if tune:\n        rfc = RandomForestClassifier(random_state=0)\n        grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n        grid_search.fit(X_train, y_train)\n        return grid_search\n    else:\n        ## random subsampling grid search to avoid grader's TimeoutError\n        ## to select the first elem: [value[0]]\n        param_grid = {key: random.choice(value) for key, value in param_grid.items()}\n        return GridDict(param_grid)\n    \ndef new_connections_predictions(X_train, y_train, X_test, y_test, tune=False):\n    ### scale dataset\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\n    ## get best parameters\n    best_param = get_best_parameters(X, y, tune=tune)\n\n    ## train model\n    clf = RandomForestClassifier(max_depth=best_param.best_params_['rfc__max_depth'], \n                                    max_features=best_param.best_params_['rfc__max_features'], \n                                    min_samples_leaf=best_param.best_params_['rfc__min_samples_leaf'], \n                                    min_samples_split=best_param.best_params_['rfc__min_samples_split'], \n                                    n_estimators=best_param.best_params_['rfc__n_estimators'], \n                                    n_jobs=-1, random_state=0)\n\n    clf.fit(X_train_scaled, y_train)\n\n    ## return proba\n    y_pred = clf.predict_proba(X_test_scaled)\n    probs = y_pred[:,1]\n    return pd.Series(probs, index=X_test.index)\n\npred = new_connections_predictions(X_train, y_train, X_test, y_test, tune=False)\npred\n\n(0, 9)          0.015451\n(0, 19)         0.052432\n(0, 20)         0.332290\n(0, 35)         0.006085\n(0, 38)         0.009872\n                  ...   \n(998, 999)      0.014456\n(1000, 1002)    0.011865\n(1000, 1003)    0.011865\n(1000, 1004)    0.011865\n(1001, 1002)    0.012798\nLength: 122112, dtype: float64\n\n\n\nPerformance evaluation\n\nmask = ~y.isna()\nX_known, y_known = X[mask], y[mask]\n\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('rfc', RandomForestClassifier(random_state=0))\n])\n\nmetrics = ['roc_auc', 'average_precision', 'balanced_accuracy']\nperfname = ['AUROC', 'AUPRC', 'Balanced Accuracy']\n\nfor perf, name in zip(metrics, perfname):\n  scores = cross_val_score(pipeline, X_known, y_known, cv=5, scoring=perf)\n  print(f\"Averaged {name}: {scores.mean():.3f}\")\n\nAveraged AUROC: 0.888\nAveraged AUPRC: 0.736\nAveraged Balanced Accuracy: 0.786"
  },
  {
    "objectID": "cnn_vs_fnn.html",
    "href": "cnn_vs_fnn.html",
    "title": "Predicting protein subcellular localizations based on sequence information",
    "section": "",
    "text": "Note\n\n\n\nData from the SIB course Diving into deep learning - theory and applications with PyTorch."
  },
  {
    "objectID": "cnn_vs_fnn.html#goal",
    "href": "cnn_vs_fnn.html#goal",
    "title": "Predicting protein subcellular localizations based on sequence information",
    "section": "Goal",
    "text": "Goal\n\nImplementing a convolutional neural network (CNN) to predict protein subcellular localization relying on sequence information\nComparing the CNN with the feedforward neural network (FNN)\nExpected output: CNN should provide a performance improvement over FNN"
  },
  {
    "objectID": "cnn_vs_fnn.html#dataset-information",
    "href": "cnn_vs_fnn.html#dataset-information",
    "title": "Predicting protein subcellular localizations based on sequence information",
    "section": "Dataset information",
    "text": "Dataset information\n\nThe dataset is from https://academic.oup.com/bioinformatics/article/33/21/3387/3931857\nEach sequence is encoded as a matrix where each position is a row of size 20, for each possible amino acid\nThe values within the matrix represent the amino acid frequency at the given position"
  },
  {
    "objectID": "cnn_vs_fnn.html#loading-libraries",
    "href": "cnn_vs_fnn.html#loading-libraries",
    "title": "Predicting protein subcellular localizations based on sequence information",
    "section": "Loading libraries",
    "text": "Loading libraries\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn import metrics\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report\n\nimport torch\nfrom torch import nn\nimport pytorch_model_summary as pms \nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom pytorchtools import EarlyStopping\n\n# get cpu, gpu or mps device for training\ndevice = (\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cpu\"\n)\nprint(f\"Using {device} device\")\n\nUsing cpu device"
  },
  {
    "objectID": "cnn_vs_fnn.html#loading-the-protein-sequences-and-labels",
    "href": "cnn_vs_fnn.html#loading-the-protein-sequences-and-labels",
    "title": "Predicting protein subcellular localizations based on sequence information",
    "section": "Loading the protein sequences and labels",
    "text": "Loading the protein sequences and labels\n\n## training data\ntrain = np.load('data/reduced_train.npz')\nX_train = train['X_train']\ny_train = train['y_train']\nprint('train:', X_train.shape)\n\n## validation data\nvalidation = np.load('data/reduced_val.npz')\nX_valid = validation['X_val']\ny_valid = validation['y_val']\nprint('valid:', X_valid.shape)\n\ntrain: (2423, 400, 20)\nvalid: (635, 400, 20)"
  },
  {
    "objectID": "cnn_vs_fnn.html#defining-the-subcellular-localization",
    "href": "cnn_vs_fnn.html#defining-the-subcellular-localization",
    "title": "Predicting protein subcellular localizations based on sequence information",
    "section": "Defining the subcellular localization",
    "text": "Defining the subcellular localization\n\nclasses = ['Nucleus',\n           'Cytoplasm',\n           'Extracellular',\n           'Mitochondrion',\n           'Cell membrane',\n           'ER',\n           'Chloroplast',\n           'Golgi apparatus',\n           'Lysosome',\n           'Vacuole']\n\ndico_classes_subcell={i:v for i,v in enumerate(classes)}\nfor i in dico_classes_subcell.keys():\n    print('Target', i, dico_classes_subcell[i])\n\nTarget 0 Nucleus\nTarget 1 Cytoplasm\nTarget 2 Extracellular\nTarget 3 Mitochondrion\nTarget 4 Cell membrane\nTarget 5 ER\nTarget 6 Chloroplast\nTarget 7 Golgi apparatus\nTarget 8 Lysosome\nTarget 9 Vacuole"
  },
  {
    "objectID": "cnn_vs_fnn.html#building-the-data-loaders",
    "href": "cnn_vs_fnn.html#building-the-data-loaders",
    "title": "Predicting protein subcellular localizations based on sequence information",
    "section": "Building the data loaders",
    "text": "Building the data loaders\n\nbatch_size = 128\n\n# transform to torch tensor\nX_train_tensor = torch.Tensor(X_train) \ny_train_tensor = torch.LongTensor(y_train)\n\nX_valid_tensor = torch.Tensor(X_valid) \ny_valid_tensor = torch.LongTensor(y_valid)\n\n# create the dataset\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor) \nvalid_dataset = TensorDataset(X_valid_tensor,y_valid_tensor) \n\n## create the dataloader\ntrain_dataloader = DataLoader(train_dataset, batch_size = batch_size) \nvalid_dataloader = DataLoader(valid_dataset, batch_size = batch_size)\n\n\nBuilding the convolutional neural network (CNN)\nThis CNN uses two convolutional layers with a 3x3 kernel and a ReLU activation, followed by max pooling to downsample the sequence length while preserving the feature dimension. The resulting features are flattened and fed into a fully connected layer, which maps the extracted features to the 10 subcellular localizations.\n\nclass ProteinLoc_CNN(nn.Module):\n    def __init__(self, seq_len=400, n_feat=20, n_class=10, out_channels=10):\n        super().__init__()\n        \n        ## - two 2D (data are 2D (400 x 20)) convolutional layers with:\n        ##   - a 3x3 kernel to capture local features\n        ##   - a 1x1 padding to preserve spatial dimension (output feature map dimension = input feature map dimension)\n        ## - a max pooling with a 5x1 padding to reduce the sequence length by a factor of 5 preserving feature dimension\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(3, 3), padding=(1, 1)),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(5, 1), stride=(5, 1)),\n\n            nn.Conv2d(in_channels=out_channels, out_channels=out_channels*2, kernel_size=(3, 3), padding=(1, 1)),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(5, 1), stride=(5, 1))\n        )\n        \n        ## flatten the layer to transforms the 2D feature maps from the convolutional layers into a 1D vector\n        self.flatten = nn.Flatten()\n        \n        ## fully connected layer\n        ## map the features from the convolutional layers to the subcellular localizations\n        self.dense_layers = nn.Sequential(\n            nn.Linear(out_channels * 2 * (seq_len // (5 * 5)) * n_feat, n_class)\n        )\n                        \n    def forward(self, x):\n        ## add a channel to reshape the data in the form (batch_size, 1, 400, 20) and\n        ## make them compatible with the Conv2d shape (batch_size, channels, height, width)\n        x = x.unsqueeze(1)\n        x = self.conv(x)  \n        x = self.flatten(x)  \n        x = self.dense_layers(x)\n        return x\n\n# initialize the model\nmodel = ProteinLoc_CNN(seq_len=400, n_feat=20, n_class=10, out_channels=40).to(device)\nprint(model)\n\n# check model\nx, _ = train_dataset[0]\nprint(pms.summary(model, x.reshape(1, 400, 20).to(device), show_input=False))\n\nProteinLoc_CNN(\n  (conv): Sequential(\n    (0): Conv2d(1, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=(5, 1), stride=(5, 1), padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(40, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): ReLU()\n    (5): MaxPool2d(kernel_size=(5, 1), stride=(5, 1), padding=0, dilation=1, ceil_mode=False)\n  )\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (dense_layers): Sequential(\n    (0): Linear(in_features=25600, out_features=10, bias=True)\n  )\n)\n------------------------------------------------------------------------\n      Layer (type)         Output Shape         Param #     Tr. Param #\n========================================================================\n          Conv2d-1     [1, 40, 400, 20]             400             400\n            ReLU-2     [1, 40, 400, 20]               0               0\n       MaxPool2d-3      [1, 40, 80, 20]               0               0\n          Conv2d-4      [1, 80, 80, 20]          28,880          28,880\n            ReLU-5      [1, 80, 80, 20]               0               0\n       MaxPool2d-6      [1, 80, 16, 20]               0               0\n         Flatten-7           [1, 25600]               0               0\n          Linear-8              [1, 10]         256,010         256,010\n========================================================================\nTotal params: 285,290\nTrainable params: 285,290\nNon-trainable params: 0\n------------------------------------------------------------------------\n\n\n\n\nBuilding the feedforward neural network (FNN)\nTaken from the 2nd notebook of the course\n\nclass ProteinLoc_FNN(torch.nn.Module):\n    def __init__(self , input_dim = 8000, \n                         hidden_dim = [80],\n                         output_dim = 10, \n                         dropout_fraction = 0.25):\n        super().__init__()\n        \n        ## we transform the input from 2D to 1D\n        self.flatten = nn.Flatten()\n        \n        elements = []\n        # each layer is made of a linear layer with a ReLu activation and a DropOut Layer\n        for i in range(len(hidden_dim)):\n            \n            elements.append( nn.Linear(input_dim, hidden_dim[i]) )\n            elements.append( nn.ReLU() )\n            elements.append( nn.Dropout(dropout_fraction) ) ## add regulation\n            \n            input_dim = hidden_dim[i] ## update the input dimension for the next layer\n        \n        elements.append( nn.Linear(input_dim, output_dim) )\n\n        self.layers = nn.Sequential( *elements )\n        \n    def forward(self, x):\n        x = self.flatten(x)\n        ## NB: here, the output of the last layer are logits\n        logits = self.layers(x)\n        return logits\n\n# initialize model\nmodel = ProteinLoc_FNN(input_dim=8000, hidden_dim=[80], output_dim=10, dropout_fraction=0.25).to(device)\nprint(model)\n\n## check model\nprint(pms.summary(model, torch.zeros(1,400,20).to(device), show_input=True))\n\nProteinLoc_FNN(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (layers): Sequential(\n    (0): Linear(in_features=8000, out_features=80, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.25, inplace=False)\n    (3): Linear(in_features=80, out_features=10, bias=True)\n  )\n)\n-----------------------------------------------------------------------\n      Layer (type)         Input Shape         Param #     Tr. Param #\n=======================================================================\n         Flatten-1        [1, 400, 20]               0               0\n          Linear-2           [1, 8000]         640,080         640,080\n            ReLU-3             [1, 80]               0               0\n         Dropout-4             [1, 80]               0               0\n          Linear-5             [1, 80]             810             810\n=======================================================================\nTotal params: 640,890\nTrainable params: 640,890\nNon-trainable params: 0\n-----------------------------------------------------------------------"
  },
  {
    "objectID": "cnn_vs_fnn.html#trainingvalidation-and-plotting-functions",
    "href": "cnn_vs_fnn.html#trainingvalidation-and-plotting-functions",
    "title": "Predicting protein subcellular localizations based on sequence information",
    "section": "Training/validation and plotting functions",
    "text": "Training/validation and plotting functions\n\ndef train(dataloader, model, loss_fn, optimizer, echo=True, echo_batch=False):\n    \n    size = len(dataloader.dataset) # how many batches do we have\n    model.train() # Sets the module in training mode.\n    \n       \n    for batch, (X, y) in enumerate(dataloader): # for each batch\n        X, y = X.to(device), y.to(device)       # send the data to the GPU or whatever device you use for training\n\n        # Compute prediction error\n        pred = model(X)              # prediction for the model -&gt; forward pass\n        loss = loss_fn(pred, y)      # loss function from these prediction\n                \n        # Backpropagation\n        loss.backward()              # backward propagation \n                                     # https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html\n                                     # https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\n        \n        optimizer.step()             \n        optimizer.zero_grad()        # reset the gradients\n                                     # https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n        \n        if echo_batch:\n            current =  (batch + 1) * len(X)\n            print(f\"Train loss: {loss.item():&gt;7f} [{current:&gt;5d}/{size:&gt;5d}]\")\n    \n    if echo:\n        current =  (batch + 1) * len(X)\n        print(f\"Train loss: {loss.item():&gt;7f}\")\n\n    # return the last batch loss\n    return loss.item()\n\ndef valid(dataloader, model, loss_fn, echo = True):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.eval() # Sets the module in evaluation mode\n        \n    valid_loss = 0\n    with torch.no_grad(): ## disables tracking of gradient: prevent accidental training + speeds up computation\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            valid_loss += loss_fn(pred, y).item()  ## accumulating the loss function over the batches\n              \n    valid_loss /= num_batches\n\n    if echo:\n        print(f\"\\tValid loss: {valid_loss:&gt;8f}\")\n   \n    return  valid_loss\n\n\nUtility functions\n\n## get predicted and target from the model\ndef get_model_predictions_and_y(model, dataloader):\n    target = np.array([], dtype= 'float32') ## []\n    predicted = np.array([], dtype= 'float32') ## []\n    with torch.no_grad():\n        for X,y in dataloader:\n            X = X.to(device)\n            pred = model(X)\n            target = np.concatenate([target, y.squeeze().numpy()]) ## extend -&gt; concatenate for list\n            predicted = np.concatenate([predicted, np.argmax(pred.to('cpu').detach().numpy() , axis=1) ] )\n\n    return predicted, target\n\n## utility function: compute additional metrics during training besides entropy loss \ndef get_additional_scores(predicted, target):   \n    return { 'balanced_accuracy': metrics.balanced_accuracy_score(target, predicted),\n             'accuracy': metrics.accuracy_score(target, predicted),\n             'f1': metrics.f1_score(target, predicted, average = 'macro') }\n\n\n\nPlotting functions\n\n## plot training metrics\ndef plot_model_training(train_scores, valid_scores):\n    fig, axes = plt.subplots(2,2,figsize = (14,8))    \n\n    for i,k in enumerate( ['loss', 'balanced_accuracy', 'accuracy', 'f1'] ) :\n        axes[i//2][i%2].plot(train_scores[k], label = 'train')\n        axes[i//2][i%2].plot(valid_scores[k], label = 'validation')\n        if k == 'loss':\n            axes[i//2][i%2].axvline(np.argmin(valid_scores[k]), linestyle='--', color='r',label='Early Stopping Checkpoint')\n        axes[i//2][i%2].legend()\n        axes[i//2][i%2].set_xlabel('epoch')\n        axes[i//2][i%2].set_ylabel(k)\n\n## plot confusion matrix\ndef plot_confusion_matrix(model, X_valid_tensor, y_valid):\n    ## we can also use get_model_predictions_and_y() instead \n    y_pred = model(X_valid_tensor.to(device))\n    y_pred = np.argmax(y_pred.detach().cpu().numpy(), axis=1)\n\n    df = pd.crosstab(y_valid, y_pred, rownames=['truth'], colnames=['prediction'])\n    df.columns = classes\n    df.index = classes\n\n    #trick to make the 0s dissapear\n    sns.heatmap(df, annot = df.astype(str).replace('0',''), fmt ='s', cmap = 'viridis')\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n## plotting accuracy\ndef plot_accuracy(accuracy, xlabel, ylabel='Accuracy', title='Accuracy comparison'):\n    plt.figure(figsize=(4,4))\n    acc_score = accuracy\n    x = np.arange(len(acc_score))\n    plt.bar(x, acc_score)\n    plt.title(title)\n    plt.ylabel(ylabel)\n    plt.xticks(x, xlabel, rotation=60)\n    for i, v in enumerate(acc_score):\n        plt.text(i, v-0.07, '%.3f'%v, color='white', fontweight='bold', ha='center')\n\n\n\nWrapper function\nTo train and evaluate CNN and FNN we use:\n\nCEloss as loss function\naccuracy, balanced_accuracy and F1_score as performance metrics\nAdam as optimizer. For both models we used the hyperparameters configuration adopted from the 2nd notebook\nearly stopping for regularization\n\n\n\ndef train_ProteinLoc(model = ProteinLoc_CNN().to(device), \n                     lr=10**-3, weight_decay=0, ## default Adam parameters setting\n                     epochs=100, patience=25):\n   \n    ## set the model\n    model = model\n\n    ## set the loss function counting class unbalancing\n    n_class=10\n    W = torch.Tensor(compute_class_weight(class_weight='balanced', \n                     classes = np.array(list(range(n_class))), ## map subcell locations to int\n                     y= y_train)).to(device)\n    CEloss = nn.CrossEntropyLoss(weight = W)\n    #print('weights_classes',W.cpu().numpy())\n\n    ## set the optimizer: https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n    optimizer = torch.optim.Adam(model.parameters(), \n                                 lr =  lr,\n                                 weight_decay = weight_decay)\n\n    ## early stopping: prevent overfitting and reduce training time\n    early_stopping = EarlyStopping(patience=patience, verbose=False)\n\n    ## keep the scores across epochs\n    train_scores = {'loss':[], 'balanced_accuracy':[], 'accuracy':[], 'f1':[]}\n    valid_scores = {'loss':[], 'balanced_accuracy':[], 'accuracy':[], 'f1':[]}\n    \n    ## train the model across epochs\n    for t in range(1,epochs+1):\n        echo = t%10==0\n        if echo:\n            print('Epoch',t )    \n        \n        ## training set\n        train_scores['loss'].append(train(train_dataloader, \n                                    model, \n                                    CEloss, \n                                    optimizer,                        \n                                    echo=echo, echo_batch=False))\n        pred_train, target_train = get_model_predictions_and_y(model, train_dataloader)\n        train_metric = get_additional_scores(pred_train, target_train)\n        \n        ## validation set\n        valid_scores['loss'].append(valid(valid_dataloader, \n                                    model, \n                                    CEloss,\n                                    echo=echo))\n        pred_valid, target_valid = get_model_predictions_and_y(model, valid_dataloader)\n        valid_metric = get_additional_scores(pred_valid, target_valid)\n        \n        ## add extra metric\n        for k in ['balanced_accuracy', 'accuracy', 'f1']:\n            train_scores[k].append(train_metric[k])\n            valid_scores[k].append(valid_metric[k])\n\n        early_stopping(valid_scores['loss'][-1], model) ## send last valid_score to early stop\n\n        if early_stopping.early_stop:\n            print(\"Early stopping\")\n            break\n\n    print(\"Done!\")\n    \n    return train_scores, valid_scores, model, CEloss, optimizer\n\n\n\nHyperparmeters\nHyperparmeters configuration used for training/testing all the models - for a fair comparison\n\nepochs = 500\nlr = 10**-4\nweight_decay = 10**-2\npatience = 100"
  },
  {
    "objectID": "cnn_vs_fnn.html#training-the-cnn",
    "href": "cnn_vs_fnn.html#training-the-cnn",
    "title": "Predicting protein subcellular localizations based on sequence information",
    "section": "Training the CNN",
    "text": "Training the CNN\n\n%%time\n\ntrain_scores_cnn, valid_scores_cnn, model_cnn, CEloss_cnn, optimizer_cnn = \\\n    train_ProteinLoc(ProteinLoc_CNN(seq_len=400, n_feat=20, n_class=10, out_channels=40).to(device),\n                     lr=lr, weight_decay=weight_decay,\n                     epochs=epochs, patience=patience)\n\nEpoch 10\nTrain loss: 1.278552\n    Valid loss: 1.793025\nEpoch 20\nTrain loss: 0.594917\n    Valid loss: 1.319517\nEpoch 30\nTrain loss: 0.333948\n    Valid loss: 1.085360\nEpoch 40\nTrain loss: 0.238646\n    Valid loss: 0.987021\nEpoch 50\nTrain loss: 0.189067\n    Valid loss: 0.949180\nEpoch 60\nTrain loss: 0.155821\n    Valid loss: 0.940055\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEarlyStopping counter: 5 out of 100\nEarlyStopping counter: 6 out of 100\nEarlyStopping counter: 7 out of 100\nEarlyStopping counter: 8 out of 100\nEarlyStopping counter: 9 out of 100\nEpoch 70\nTrain loss: 0.132961\n    Valid loss: 0.945401\nEarlyStopping counter: 10 out of 100\nEarlyStopping counter: 11 out of 100\nEarlyStopping counter: 12 out of 100\nEarlyStopping counter: 13 out of 100\nEarlyStopping counter: 14 out of 100\nEarlyStopping counter: 15 out of 100\nEarlyStopping counter: 16 out of 100\nEarlyStopping counter: 17 out of 100\nEarlyStopping counter: 18 out of 100\nEarlyStopping counter: 19 out of 100\nEpoch 80\nTrain loss: 0.115674\n    Valid loss: 0.955135\nEarlyStopping counter: 20 out of 100\nEarlyStopping counter: 21 out of 100\nEarlyStopping counter: 22 out of 100\nEarlyStopping counter: 23 out of 100\nEarlyStopping counter: 24 out of 100\nEarlyStopping counter: 25 out of 100\nEarlyStopping counter: 26 out of 100\nEarlyStopping counter: 27 out of 100\nEarlyStopping counter: 28 out of 100\nEarlyStopping counter: 29 out of 100\nEpoch 90\nTrain loss: 0.101232\n    Valid loss: 0.964812\nEarlyStopping counter: 30 out of 100\nEarlyStopping counter: 31 out of 100\nEarlyStopping counter: 32 out of 100\nEarlyStopping counter: 33 out of 100\nEarlyStopping counter: 34 out of 100\nEarlyStopping counter: 35 out of 100\nEarlyStopping counter: 36 out of 100\nEarlyStopping counter: 37 out of 100\nEarlyStopping counter: 38 out of 100\nEarlyStopping counter: 39 out of 100\nEpoch 100\nTrain loss: 0.088912\n    Valid loss: 0.973630\nEarlyStopping counter: 40 out of 100\nEarlyStopping counter: 41 out of 100\nEarlyStopping counter: 42 out of 100\nEarlyStopping counter: 43 out of 100\nEarlyStopping counter: 44 out of 100\nEarlyStopping counter: 45 out of 100\nEarlyStopping counter: 46 out of 100\nEarlyStopping counter: 47 out of 100\nEarlyStopping counter: 48 out of 100\nEarlyStopping counter: 49 out of 100\nEpoch 110\nTrain loss: 0.078697\n    Valid loss: 0.981492\nEarlyStopping counter: 50 out of 100\nEarlyStopping counter: 51 out of 100\nEarlyStopping counter: 52 out of 100\nEarlyStopping counter: 53 out of 100\nEarlyStopping counter: 54 out of 100\nEarlyStopping counter: 55 out of 100\nEarlyStopping counter: 56 out of 100\nEarlyStopping counter: 57 out of 100\nEarlyStopping counter: 58 out of 100\nEarlyStopping counter: 59 out of 100\nEpoch 120\nTrain loss: 0.070570\n    Valid loss: 0.988403\nEarlyStopping counter: 60 out of 100\nEarlyStopping counter: 61 out of 100\nEarlyStopping counter: 62 out of 100\nEarlyStopping counter: 63 out of 100\nEarlyStopping counter: 64 out of 100\nEarlyStopping counter: 65 out of 100\nEarlyStopping counter: 66 out of 100\nEarlyStopping counter: 67 out of 100\nEarlyStopping counter: 68 out of 100\nEarlyStopping counter: 69 out of 100\nEpoch 130\nTrain loss: 0.064186\n    Valid loss: 0.994534\nEarlyStopping counter: 70 out of 100\nEarlyStopping counter: 71 out of 100\nEarlyStopping counter: 72 out of 100\nEarlyStopping counter: 73 out of 100\nEarlyStopping counter: 74 out of 100\nEarlyStopping counter: 75 out of 100\nEarlyStopping counter: 76 out of 100\nEarlyStopping counter: 77 out of 100\nEarlyStopping counter: 78 out of 100\nEarlyStopping counter: 79 out of 100\nEpoch 140\nTrain loss: 0.059257\n    Valid loss: 0.999891\nEarlyStopping counter: 80 out of 100\nEarlyStopping counter: 81 out of 100\nEarlyStopping counter: 82 out of 100\nEarlyStopping counter: 83 out of 100\nEarlyStopping counter: 84 out of 100\nEarlyStopping counter: 85 out of 100\nEarlyStopping counter: 86 out of 100\nEarlyStopping counter: 87 out of 100\nEarlyStopping counter: 88 out of 100\nEarlyStopping counter: 89 out of 100\nEpoch 150\nTrain loss: 0.055351\n    Valid loss: 1.004522\nEarlyStopping counter: 90 out of 100\nEarlyStopping counter: 91 out of 100\nEarlyStopping counter: 92 out of 100\nEarlyStopping counter: 93 out of 100\nEarlyStopping counter: 94 out of 100\nEarlyStopping counter: 95 out of 100\nEarlyStopping counter: 96 out of 100\nEarlyStopping counter: 97 out of 100\nEarlyStopping counter: 98 out of 100\nEarlyStopping counter: 99 out of 100\nEpoch 160\nTrain loss: 0.052179\n    Valid loss: 1.008472\nEarlyStopping counter: 100 out of 100\nEarly stopping\nDone!\nCPU times: user 5h 28min 40s, sys: 1h 42min 51s, total: 7h 11min 32s\nWall time: 1h 11min 58s\n\n\n\nEvaluating the CNN\n\nplot_model_training(train_scores_cnn, valid_scores_cnn)"
  },
  {
    "objectID": "cnn_vs_fnn.html#training-the-fnn",
    "href": "cnn_vs_fnn.html#training-the-fnn",
    "title": "Predicting protein subcellular localizations based on sequence information",
    "section": "Training the FNN",
    "text": "Training the FNN\n\n%%time \n\ntrain_scores_fnn, valid_scores_fnn, model_fnn, CEloss_fnn, optimizer_fnn = \\\n    train_ProteinLoc(ProteinLoc_FNN(input_dim=8000, hidden_dim=[80], output_dim=10, \n                                    dropout_fraction=0.25).to(device),\n                     lr=lr, weight_decay=weight_decay,\n                     epochs=epochs, patience=patience)\n\nEpoch 10\nTrain loss: 1.721765\n    Valid loss: 2.017928\nEpoch 20\nTrain loss: 1.237484\n    Valid loss: 1.800885\nEpoch 30\nTrain loss: 0.955508\n    Valid loss: 1.631265\nEpoch 40\nTrain loss: 0.783767\n    Valid loss: 1.508611\nEpoch 50\nTrain loss: 0.655635\n    Valid loss: 1.418898\nEpoch 60\nTrain loss: 0.552101\n    Valid loss: 1.354256\nEpoch 70\nTrain loss: 0.512112\n    Valid loss: 1.313151\nEarlyStopping counter: 1 out of 100\nEpoch 80\nTrain loss: 0.434766\n    Valid loss: 1.279691\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEpoch 90\nTrain loss: 0.380775\n    Valid loss: 1.252143\nEpoch 100\nTrain loss: 0.391283\n    Valid loss: 1.230097\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEpoch 110\nTrain loss: 0.368573\n    Valid loss: 1.218942\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEpoch 120\nTrain loss: 0.324602\n    Valid loss: 1.203767\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEpoch 130\nTrain loss: 0.300448\n    Valid loss: 1.190869\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEpoch 140\nTrain loss: 0.299642\n    Valid loss: 1.181567\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEpoch 150\nTrain loss: 0.290868\n    Valid loss: 1.170999\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEpoch 160\nTrain loss: 0.264038\n    Valid loss: 1.166920\nEarlyStopping counter: 5 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEarlyStopping counter: 5 out of 100\nEarlyStopping counter: 6 out of 100\nEarlyStopping counter: 7 out of 100\nEarlyStopping counter: 8 out of 100\nEpoch 170\nTrain loss: 0.267423\n    Valid loss: 1.157207\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEarlyStopping counter: 5 out of 100\nEpoch 180\nTrain loss: 0.262831\n    Valid loss: 1.154265\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEpoch 190\nTrain loss: 0.254707\n    Valid loss: 1.158014\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEarlyStopping counter: 5 out of 100\nEpoch 200\nTrain loss: 0.242801\n    Valid loss: 1.141612\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEarlyStopping counter: 5 out of 100\nEarlyStopping counter: 6 out of 100\nEarlyStopping counter: 7 out of 100\nEpoch 210\nTrain loss: 0.236814\n    Valid loss: 1.145432\nEarlyStopping counter: 8 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEarlyStopping counter: 5 out of 100\nEarlyStopping counter: 6 out of 100\nEarlyStopping counter: 1 out of 100\nEpoch 220\nTrain loss: 0.236250\n    Valid loss: 1.137942\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 1 out of 100\nEpoch 230\nTrain loss: 0.208718\n    Valid loss: 1.141708\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEarlyStopping counter: 5 out of 100\nEarlyStopping counter: 6 out of 100\nEarlyStopping counter: 7 out of 100\nEpoch 240\nTrain loss: 0.217312\n    Valid loss: 1.137266\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEarlyStopping counter: 5 out of 100\nEarlyStopping counter: 6 out of 100\nEarlyStopping counter: 7 out of 100\nEarlyStopping counter: 8 out of 100\nEarlyStopping counter: 9 out of 100\nEarlyStopping counter: 10 out of 100\nEpoch 250\nTrain loss: 0.211357\n    Valid loss: 1.130197\nEarlyStopping counter: 11 out of 100\nEarlyStopping counter: 12 out of 100\nEarlyStopping counter: 13 out of 100\nEarlyStopping counter: 14 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEarlyStopping counter: 5 out of 100\nEpoch 260\nTrain loss: 0.218220\n    Valid loss: 1.130170\nEarlyStopping counter: 6 out of 100\nEarlyStopping counter: 7 out of 100\nEarlyStopping counter: 8 out of 100\nEarlyStopping counter: 9 out of 100\nEarlyStopping counter: 10 out of 100\nEarlyStopping counter: 11 out of 100\nEarlyStopping counter: 12 out of 100\nEarlyStopping counter: 13 out of 100\nEarlyStopping counter: 14 out of 100\nEarlyStopping counter: 15 out of 100\nEpoch 270\nTrain loss: 0.180335\n    Valid loss: 1.132610\nEarlyStopping counter: 16 out of 100\nEarlyStopping counter: 17 out of 100\nEarlyStopping counter: 18 out of 100\nEarlyStopping counter: 19 out of 100\nEarlyStopping counter: 20 out of 100\nEarlyStopping counter: 21 out of 100\nEarlyStopping counter: 22 out of 100\nEarlyStopping counter: 23 out of 100\nEarlyStopping counter: 24 out of 100\nEarlyStopping counter: 25 out of 100\nEpoch 280\nTrain loss: 0.197241\n    Valid loss: 1.128655\nEarlyStopping counter: 26 out of 100\nEarlyStopping counter: 27 out of 100\nEarlyStopping counter: 28 out of 100\nEarlyStopping counter: 29 out of 100\nEarlyStopping counter: 30 out of 100\nEarlyStopping counter: 31 out of 100\nEarlyStopping counter: 32 out of 100\nEarlyStopping counter: 33 out of 100\nEarlyStopping counter: 34 out of 100\nEarlyStopping counter: 35 out of 100\nEpoch 290\nTrain loss: 0.182162\n    Valid loss: 1.127565\nEarlyStopping counter: 36 out of 100\nEarlyStopping counter: 37 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEarlyStopping counter: 5 out of 100\nEarlyStopping counter: 6 out of 100\nEpoch 300\nTrain loss: 0.187505\n    Valid loss: 1.124462\nEarlyStopping counter: 7 out of 100\nEarlyStopping counter: 8 out of 100\nEarlyStopping counter: 9 out of 100\nEarlyStopping counter: 10 out of 100\nEarlyStopping counter: 11 out of 100\nEarlyStopping counter: 12 out of 100\nEarlyStopping counter: 13 out of 100\nEarlyStopping counter: 14 out of 100\nEarlyStopping counter: 15 out of 100\nEarlyStopping counter: 16 out of 100\nEpoch 310\nTrain loss: 0.206274\n    Valid loss: 1.127542\nEarlyStopping counter: 17 out of 100\nEarlyStopping counter: 18 out of 100\nEarlyStopping counter: 19 out of 100\nEarlyStopping counter: 20 out of 100\nEarlyStopping counter: 21 out of 100\nEarlyStopping counter: 22 out of 100\nEarlyStopping counter: 23 out of 100\nEarlyStopping counter: 24 out of 100\nEarlyStopping counter: 25 out of 100\nEarlyStopping counter: 26 out of 100\nEpoch 320\nTrain loss: 0.171165\n    Valid loss: 1.126099\nEarlyStopping counter: 27 out of 100\nEarlyStopping counter: 28 out of 100\nEarlyStopping counter: 29 out of 100\nEarlyStopping counter: 30 out of 100\nEarlyStopping counter: 31 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEpoch 330\nTrain loss: 0.182406\n    Valid loss: 1.110636\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEarlyStopping counter: 5 out of 100\nEarlyStopping counter: 6 out of 100\nEarlyStopping counter: 7 out of 100\nEarlyStopping counter: 8 out of 100\nEarlyStopping counter: 9 out of 100\nEpoch 340\nTrain loss: 0.182533\n    Valid loss: 1.117068\nEarlyStopping counter: 10 out of 100\nEarlyStopping counter: 11 out of 100\nEarlyStopping counter: 12 out of 100\nEarlyStopping counter: 13 out of 100\nEarlyStopping counter: 14 out of 100\nEarlyStopping counter: 15 out of 100\nEarlyStopping counter: 16 out of 100\nEarlyStopping counter: 17 out of 100\nEarlyStopping counter: 18 out of 100\nEarlyStopping counter: 19 out of 100\nEpoch 350\nTrain loss: 0.169552\n    Valid loss: 1.113264\nEarlyStopping counter: 20 out of 100\nEarlyStopping counter: 21 out of 100\nEarlyStopping counter: 22 out of 100\nEarlyStopping counter: 23 out of 100\nEarlyStopping counter: 24 out of 100\nEarlyStopping counter: 25 out of 100\nEarlyStopping counter: 26 out of 100\nEarlyStopping counter: 27 out of 100\nEarlyStopping counter: 28 out of 100\nEarlyStopping counter: 29 out of 100\nEpoch 360\nTrain loss: 0.176773\n    Valid loss: 1.122313\nEarlyStopping counter: 30 out of 100\nEarlyStopping counter: 31 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEarlyStopping counter: 5 out of 100\nEarlyStopping counter: 6 out of 100\nEarlyStopping counter: 7 out of 100\nEpoch 370\nTrain loss: 0.172929\n    Valid loss: 1.117166\nEarlyStopping counter: 8 out of 100\nEarlyStopping counter: 9 out of 100\nEarlyStopping counter: 10 out of 100\nEarlyStopping counter: 11 out of 100\nEarlyStopping counter: 12 out of 100\nEarlyStopping counter: 13 out of 100\nEarlyStopping counter: 14 out of 100\nEarlyStopping counter: 15 out of 100\nEarlyStopping counter: 16 out of 100\nEarlyStopping counter: 17 out of 100\nEpoch 380\nTrain loss: 0.172954\n    Valid loss: 1.114234\nEarlyStopping counter: 18 out of 100\nEarlyStopping counter: 19 out of 100\nEarlyStopping counter: 20 out of 100\nEarlyStopping counter: 21 out of 100\nEarlyStopping counter: 22 out of 100\nEarlyStopping counter: 23 out of 100\nEarlyStopping counter: 24 out of 100\nEarlyStopping counter: 25 out of 100\nEarlyStopping counter: 26 out of 100\nEarlyStopping counter: 27 out of 100\nEpoch 390\nTrain loss: 0.175194\n    Valid loss: 1.118202\nEarlyStopping counter: 28 out of 100\nEarlyStopping counter: 29 out of 100\nEarlyStopping counter: 30 out of 100\nEarlyStopping counter: 31 out of 100\nEarlyStopping counter: 32 out of 100\nEarlyStopping counter: 33 out of 100\nEarlyStopping counter: 34 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEpoch 400\nTrain loss: 0.163461\n    Valid loss: 1.115803\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEarlyStopping counter: 5 out of 100\nEarlyStopping counter: 6 out of 100\nEarlyStopping counter: 7 out of 100\nEarlyStopping counter: 8 out of 100\nEarlyStopping counter: 9 out of 100\nEarlyStopping counter: 10 out of 100\nEarlyStopping counter: 11 out of 100\nEarlyStopping counter: 12 out of 100\nEpoch 410\nTrain loss: 0.167024\n    Valid loss: 1.111715\nEarlyStopping counter: 13 out of 100\nEarlyStopping counter: 14 out of 100\nEarlyStopping counter: 15 out of 100\nEarlyStopping counter: 16 out of 100\nEarlyStopping counter: 17 out of 100\nEarlyStopping counter: 18 out of 100\nEarlyStopping counter: 19 out of 100\nEarlyStopping counter: 20 out of 100\nEarlyStopping counter: 21 out of 100\nEarlyStopping counter: 22 out of 100\nEpoch 420\nTrain loss: 0.172140\n    Valid loss: 1.122734\nEarlyStopping counter: 23 out of 100\nEarlyStopping counter: 24 out of 100\nEarlyStopping counter: 25 out of 100\nEarlyStopping counter: 26 out of 100\nEarlyStopping counter: 27 out of 100\nEarlyStopping counter: 28 out of 100\nEarlyStopping counter: 29 out of 100\nEarlyStopping counter: 30 out of 100\nEarlyStopping counter: 31 out of 100\nEarlyStopping counter: 32 out of 100\nEpoch 430\nTrain loss: 0.177140\n    Valid loss: 1.111379\nEarlyStopping counter: 33 out of 100\nEarlyStopping counter: 34 out of 100\nEarlyStopping counter: 35 out of 100\nEarlyStopping counter: 36 out of 100\nEarlyStopping counter: 37 out of 100\nEarlyStopping counter: 38 out of 100\nEarlyStopping counter: 39 out of 100\nEarlyStopping counter: 40 out of 100\nEarlyStopping counter: 41 out of 100\nEarlyStopping counter: 42 out of 100\nEpoch 440\nTrain loss: 0.170154\n    Valid loss: 1.116932\nEarlyStopping counter: 43 out of 100\nEarlyStopping counter: 44 out of 100\nEarlyStopping counter: 45 out of 100\nEarlyStopping counter: 46 out of 100\nEarlyStopping counter: 47 out of 100\nEarlyStopping counter: 48 out of 100\nEarlyStopping counter: 49 out of 100\nEarlyStopping counter: 50 out of 100\nEarlyStopping counter: 51 out of 100\nEarlyStopping counter: 52 out of 100\nEpoch 450\nTrain loss: 0.161532\n    Valid loss: 1.122467\nEarlyStopping counter: 53 out of 100\nEarlyStopping counter: 54 out of 100\nEarlyStopping counter: 55 out of 100\nEarlyStopping counter: 56 out of 100\nEarlyStopping counter: 57 out of 100\nEarlyStopping counter: 58 out of 100\nEarlyStopping counter: 59 out of 100\nEarlyStopping counter: 60 out of 100\nEarlyStopping counter: 61 out of 100\nEarlyStopping counter: 62 out of 100\nEpoch 460\nTrain loss: 0.175092\n    Valid loss: 1.114117\nEarlyStopping counter: 63 out of 100\nEarlyStopping counter: 64 out of 100\nEarlyStopping counter: 65 out of 100\nEarlyStopping counter: 66 out of 100\nEarlyStopping counter: 67 out of 100\nEarlyStopping counter: 68 out of 100\nEarlyStopping counter: 69 out of 100\nEarlyStopping counter: 70 out of 100\nEarlyStopping counter: 71 out of 100\nEarlyStopping counter: 72 out of 100\nEpoch 470\nTrain loss: 0.175786\n    Valid loss: 1.114911\nEarlyStopping counter: 73 out of 100\nEarlyStopping counter: 74 out of 100\nEarlyStopping counter: 75 out of 100\nEarlyStopping counter: 76 out of 100\nEarlyStopping counter: 77 out of 100\nEarlyStopping counter: 78 out of 100\nEarlyStopping counter: 79 out of 100\nEarlyStopping counter: 80 out of 100\nEarlyStopping counter: 81 out of 100\nEarlyStopping counter: 82 out of 100\nEpoch 480\nTrain loss: 0.150602\n    Valid loss: 1.113243\nEarlyStopping counter: 83 out of 100\nEarlyStopping counter: 84 out of 100\nEarlyStopping counter: 85 out of 100\nEarlyStopping counter: 86 out of 100\nEarlyStopping counter: 87 out of 100\nEarlyStopping counter: 88 out of 100\nEarlyStopping counter: 89 out of 100\nEarlyStopping counter: 90 out of 100\nEarlyStopping counter: 91 out of 100\nEarlyStopping counter: 92 out of 100\nEpoch 490\nTrain loss: 0.171302\n    Valid loss: 1.115211\nEarlyStopping counter: 93 out of 100\nEarlyStopping counter: 94 out of 100\nEarlyStopping counter: 1 out of 100\nEarlyStopping counter: 2 out of 100\nEarlyStopping counter: 3 out of 100\nEarlyStopping counter: 4 out of 100\nEarlyStopping counter: 5 out of 100\nEarlyStopping counter: 6 out of 100\nEarlyStopping counter: 7 out of 100\nEpoch 500\nTrain loss: 0.159058\n    Valid loss: 1.115464\nEarlyStopping counter: 8 out of 100\nDone!\nCPU times: user 14min 53s, sys: 14.3 s, total: 15min 7s\nWall time: 2min 31s\n\n\n\nEvaluating the FNN\n\nplot_model_training(train_scores_fnn, valid_scores_fnn)"
  },
  {
    "objectID": "cnn_vs_fnn.html#performance-comparison-cnn-vs-fnn",
    "href": "cnn_vs_fnn.html#performance-comparison-cnn-vs-fnn",
    "title": "Predicting protein subcellular localizations based on sequence information",
    "section": "Performance comparison: CNN vs FNN",
    "text": "Performance comparison: CNN vs FNN\n\ny_pred_cnn, y_cnn = get_model_predictions_and_y(model_cnn, valid_dataloader)\ny_pred_fnn, y_fnn = get_model_predictions_and_y(model_fnn, valid_dataloader)\n\n\nCNN\n\nplot_confusion_matrix(model_cnn, X_valid_tensor, y_valid)\n\n\n\n\n\n\n\n\n\nprint(classification_report(y_valid, y_pred_cnn, target_names=classes))\n\n                 precision    recall  f1-score   support\n\n        Nucleus       0.87      0.85      0.86        95\n      Cytoplasm       0.88      0.74      0.80       151\n  Extracellular       0.92      0.82      0.87       131\n  Mitochondrion       0.67      0.84      0.74        64\n  Cell membrane       0.78      0.91      0.84        69\n             ER       0.60      0.69      0.64        13\n    Chloroplast       0.84      0.80      0.82        60\nGolgi apparatus       0.88      0.78      0.82        18\n       Lysosome       0.78      0.74      0.76        19\n        Vacuole       0.37      0.73      0.49        15\n\n       accuracy                           0.81       635\n      macro avg       0.76      0.79      0.76       635\n   weighted avg       0.83      0.81      0.81       635\n\n\n\n\n\nFNN\n\nplot_confusion_matrix(model_fnn, X_valid_tensor, y_valid)\n\n\n\n\n\n\n\n\n\nprint(classification_report(y_valid, y_pred_fnn, target_names=classes))\n\n                 precision    recall  f1-score   support\n\n        Nucleus       0.80      0.82      0.81        95\n      Cytoplasm       0.77      0.83      0.80       151\n  Extracellular       0.87      0.87      0.87       131\n  Mitochondrion       0.71      0.86      0.78        64\n  Cell membrane       0.74      0.81      0.77        69\n             ER       0.50      0.15      0.24        13\n    Chloroplast       0.86      0.73      0.79        60\nGolgi apparatus       0.68      0.72      0.70        18\n       Lysosome       0.82      0.47      0.60        19\n        Vacuole       0.80      0.27      0.40        15\n\n       accuracy                           0.79       635\n      macro avg       0.76      0.65      0.68       635\n   weighted avg       0.79      0.79      0.78       635"
  },
  {
    "objectID": "cnn_vs_fnn.html#accuracy-comparison",
    "href": "cnn_vs_fnn.html#accuracy-comparison",
    "title": "Predicting protein subcellular localizations based on sequence information",
    "section": "Accuracy comparison",
    "text": "Accuracy comparison\n\n## other way of computing accuracy\n# from sklearn.metrics import confusion_matrix\n# cm_fnn = confusion_matrix(y_valid, y_pred_fnn)\n# accuracy_fnn = np.trace(cm_fnn) / np.sum(cm_fnn)\n\naccuracy_cnn = get_additional_scores(y_pred_cnn, y_cnn)['accuracy']\naccuracy_fnn = get_additional_scores(y_pred_fnn, y_fnn)['accuracy']\n\n## plot accuracy \nplot_accuracy(accuracy=[accuracy_fnn, accuracy_cnn],\n              xlabel=['FNN','CNN'],\n              ylabel='Accuracy', title='')\n\n\n\n\n\n\n\n\n\nBalanced accuracy comparison\n\n## compute balanced accuracy over subcellular localizations (classes)\n\nbalanced_accuracy_cnn = get_additional_scores(y_pred_cnn, y_cnn)['balanced_accuracy']\nbalanced_accuracy_fnn = get_additional_scores(y_pred_fnn, y_fnn)['balanced_accuracy']\n\n## plot accuracy\n## note: p -&gt; padding and s -&gt; stride \nplot_accuracy(accuracy=[balanced_accuracy_fnn, balanced_accuracy_cnn],\n              xlabel=['FNN','CNN'],\n              ylabel='Balanced accuracy', title='')"
  },
  {
    "objectID": "cnn_vs_fnn.html#conclusions-and-considerations",
    "href": "cnn_vs_fnn.html#conclusions-and-considerations",
    "title": "Predicting protein subcellular localizations based on sequence information",
    "section": "Conclusions and considerations",
    "text": "Conclusions and considerations\n\nCNN slightly outperforms FNN, but at a much higher computational cost: CNN took 12min while FNN took 10sec in current configuration\nIncreasing the number of output channels (i.e. feature maps) boosts CNN performance, but at the cost of increased training time. For instance, a CNN with 80 output channels yielded an accuracy of around 0.82 after roughly 40 minutes of training (data not shown)."
  },
  {
    "objectID": "r1_eda.html",
    "href": "r1_eda.html",
    "title": "Exploratory data analysis: validation of technical replicates",
    "section": "",
    "text": "The GSE96058 patient cohort contains 3273 samples of which 136 have technical replicates. Some of these replicates were sequenced on a different sequencer (HiSeq2000 and NextSeq500). In this report, I analyze the gene expression profiles of these technical replicates to assess their similarity. This serves as a technical validation before averaging their gene expression values."
  },
  {
    "objectID": "r1_eda.html#aims",
    "href": "r1_eda.html#aims",
    "title": "Exploratory data analysis: validation of technical replicates",
    "section": "",
    "text": "The GSE96058 patient cohort contains 3273 samples of which 136 have technical replicates. Some of these replicates were sequenced on a different sequencer (HiSeq2000 and NextSeq500). In this report, I analyze the gene expression profiles of these technical replicates to assess their similarity. This serves as a technical validation before averaging their gene expression values."
  },
  {
    "objectID": "r1_eda.html#validation-of-technical-replicates",
    "href": "r1_eda.html#validation-of-technical-replicates",
    "title": "Exploratory data analysis: validation of technical replicates",
    "section": "Validation of technical replicates",
    "text": "Validation of technical replicates\n\n\n\n\n\n\nNote\n\n\n\nThe authors provided only transformed gene expression data, which I then used for downstream analysis.\n\n\nLoading data\n\ngexp &lt;- read_csv(\n  'data/GSE96058_gene_expression_3273_samples_and_136_replicates_transformed.csv.gz', \n  col_types = cols()\n) |&gt; rename(genes = `...1`)\n\nmeta &lt;- read_csv('data/metadata.csv', col_types = cols()) |&gt;\n  rename(sampleID = values, sampleName = ind)\n\n\nrep &lt;- gexp |&gt; select(contains('repl')) |&gt; colnames() |&gt; str_remove('repl')\n\nmxrep &lt;- gexp |&gt; \n  select(genes, all_of(rep) | contains('repl')) |&gt; \n  column_to_rownames(var = 'genes') \n\nmdrep &lt;- meta |&gt; \n  filter(sampleID %in% colnames(mxrep)) |&gt;\n  select(sampleID, instrument_model, age_at_diagnosis, \n         tumor_size, lymph_node_group, lymph_node_status, \n         her2_status, ki67_status, pgr_status, nhg, \n         endocrine_treated, chemo_treated) |&gt; \n  column_to_rownames(var = 'sampleID')\n\nPCA\nI apply a Principal Component Analysis (PCA) on the gene expression profiles of technical replicates (marked with the same color) to assess similarity in gene expression. As expected, technical replicates cluster together.\n\npc &lt;- PCAtools::pca(mxrep, metadata = mdrep, center = TRUE, scale = FALSE, removeVar = 0.1)\n\nstopifnot(rownames(mdrep) %in% rownames(pc$rotated))\n\nplot_pca &lt;- function(pc, pcx ='PC1', pcy = 'PC2'){\n  pc_data &lt;- cbind(pc$rotated[, c(pcx, pcy)], mdrep) |&gt;\n    rownames_to_column(var = 'sample')|&gt;\n    mutate(strip_rep = str_replace(sample, 'repl', ''))\n\n  pc_val &lt;- round(pc$variance[c(pcx, pcy)], 2) |&gt; unname()\n\n  ggplot(pc_data, aes(x = !!sym(pcx), y = !!sym(pcy), color = strip_rep, shape = instrument_model)) +\n    geom_point(size = 3) + \n    labs(title = \"\",\n         x = paste0(\"PC1 (\", pc_val[1], \"% variance)\"),\n         y = paste0(\"PC2 (\", pc_val[2], \"% variance)\"),\n         shape = \"Platform\",) +\n    geom_text_repel(data = pc_data, \n                    aes(label = sample), \n                    min.segment.length = 5, max.overlaps = Inf) +\n    theme_bw() +\n    theme(plot.title = element_text(hjust = 0.5)) +\n    guides(color = \"none\")\n}\n\nplot_pca(pc, pcx ='PC1', pcy = 'PC2')\n\n\n\n\n\n\n\nSample correlations\nFor a quantitative overview, I compute the Spearman’s rank correlation for each pair. As expected, technical replicates are strongly correlated (coefficient higher than 0.9) and their gene expression can be safely averaged for downstream analyses.\n\nmxrep_paired &lt;- mxrep[, str_sort(colnames(mxrep), numeric = TRUE)]\n\ntib &lt;- tibble(\n  comparison = character(),\n  correlation = numeric(),\n  p_value = numeric(),\n  informative = logical(),\n  significant = logical()\n)\n\nfor(i in seq(1, ncol(mxrep_paired)-1, by = 2)){\n  corr &lt;- cor.test(mxrep_paired[, i], \n                   mxrep_paired[, i+1], \n                   method = 'spearman',\n                   exact = FALSE\n                  )\n\n  tmp &lt;- tibble(\n    comparison = paste(colnames(mxrep_paired)[i:(i+1)], collapse=' vs '),\n    correlation = round(corr$estimate, 2),\n    p_value = corr$p.value,\n    informative =  round(corr$estimate, 2) &gt;= 0.90,\n    significant = corr$p.value &lt; 0.05\n  )\n\n  tib &lt;- rbind(tib, tmp)\n}\n\ndatatable &lt;- function(tib, row2display = 10) {\n  if(nrow(tib) &gt; 0){\n    DT::datatable(tib,\n      rownames   = FALSE,\n      options    = list(\n        dom = \"Bfrtip\",\n        scrollX = TRUE,\n        pageLength = row2display\n      )\n    )\n  }else{\n    print(\"No results\")\n  }\n}\ntib |&gt; datatable()\n\n\n\n\n\nAs an example, below I show the scatter plots of the first 12 pairs.\n\nscatter_plot &lt;- function(data, sample, replicate) {\n  ggplot(data, aes(x = !!sym(sample), y = !!sym(replicate))) +\n    geom_point(colour = \"#56B4E9\", alpha = 0.8, size = 1) +\n    geom_smooth(method = \"lm\", color = \"#D55E00\", formula = y ~ x, se = TRUE) +\n    labs(\n      title = paste(sample, \"vs\", replicate),\n      subtitle = paste(\"r =\", round(cor(data[[sample]], data[[replicate]], method = \"spearman\"), 3))\n    ) +\n    theme_bw() +\n    theme(\n      plot.title = element_text(size = 10),\n      plot.subtitle = element_text(size = 8)\n    )\n}\n\nget_pair &lt;- function(tib, npairs=10){\n  pairs &lt;- str_split(tib$comparison, \" vs \", simplify = TRUE)[1:npairs,]\n  sample &lt;- pairs[, 1]\n  replicate &lt;- pairs[, 2]\n  return(list(sample = sample, replicate = replicate))\n}\n\nsp &lt;- get_pair(tib, npairs = 12)\nplist &lt;- map2(sp$sample, sp$replicate, ~scatter_plot(mxrep, .x, .y))\nwrap_plots(plist, ncol = 4)"
  },
  {
    "objectID": "r1_eda.html#averaging-of-gene-expression-of-technical-replicates",
    "href": "r1_eda.html#averaging-of-gene-expression-of-technical-replicates",
    "title": "Exploratory data analysis: validation of technical replicates",
    "section": "Averaging of gene expression of technical replicates",
    "text": "Averaging of gene expression of technical replicates\n\nmxrep_averaged &lt;- c()\n\nfor(i in seq(1, ncol(mxrep_paired)-1, by = 2)){\n  mxrep_averaged &lt;- cbind(mxrep_averaged, rowMeans(mxrep_paired[,i:(i+1)]))\n  # cat(colnames(mxrep_paired)[i], colnames(mxrep_paired)[i+1], 'averaged\\n')\n}\n\nnorep &lt;- str_subset(colnames(mxrep_paired), 'repl', negate=TRUE)\ncolnames(mxrep_averaged) &lt;- norep\n\ngexp_rep &lt;- mxrep_averaged |&gt; \n  as.data.frame() |&gt; \n  rownames_to_column(var = 'genes') |&gt; \n  as_tibble()\n\nfout &lt;- 'data/gene_expression_profile.csv.gz'\nif(!file.exists(fout)){\n  gexp |&gt; \n    select(-all_of(colnames(gexp_rep)[-1])) |&gt; \n    left_join(gexp_rep, by = 'genes') |&gt; \n    select(-ends_with(\"repl\")) |&gt;\n    {\\(s)  ## lambda function ..\n      select(s, genes, all_of(str_sort(names(select(s, starts_with(\"F\"))), numeric = TRUE)))\n    }() |&gt;\n    ## %&gt;% select(., all_of(str_sort(colnames(select(., starts_with('F'))), numeric = TRUE)))\n    write_csv(fout) ## automatically compress by readr\n}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Personal Projects",
    "section": "",
    "text": "Identification of breast cancer subtypes via a graph-based approach"
  },
  {
    "objectID": "index.html#genomics",
    "href": "index.html#genomics",
    "title": "Personal Projects",
    "section": "",
    "text": "Identification of breast cancer subtypes via a graph-based approach"
  },
  {
    "objectID": "index.html#machine-learning",
    "href": "index.html#machine-learning",
    "title": "Personal Projects",
    "section": "Machine learning",
    "text": "Machine learning\n\nPredicting viewer engagement with educational videos\nPredicting salary and new connections from network data"
  },
  {
    "objectID": "index.html#deep-learning",
    "href": "index.html#deep-learning",
    "title": "Personal Projects",
    "section": "Deep Learning",
    "text": "Deep Learning\n\nPredicting protein subcellular localization from aminoacid sequences with PyTorch"
  },
  {
    "objectID": "index.html#ai-powered-tools",
    "href": "index.html#ai-powered-tools",
    "title": "Personal Projects",
    "section": "AI-powered tools",
    "text": "AI-powered tools\n\nDeveloping a RAG framework for bioinformatics applications"
  },
  {
    "objectID": "index.html#dataviz",
    "href": "index.html#dataviz",
    "title": "Personal Projects",
    "section": "DataViz",
    "text": "DataViz\n\nVisualize Michigan’s temperature records\nVisualize Detroit’s sports teams win percentages"
  },
  {
    "objectID": "index.html#software-libraries",
    "href": "index.html#software-libraries",
    "title": "Personal Projects",
    "section": "Software Libraries",
    "text": "Software Libraries\n\nA meta-learning tool for predicting novel protein functions and novel associations between human genes and abnormal phenotype from bio-ontology data\nA Perl module to parse biomedical ontology and gene association data"
  },
  {
    "objectID": "cancer-subtypes.html",
    "href": "cancer-subtypes.html",
    "title": "Identification of breast cancer subtypes from a bulk RNA-Seq data via a graph-based approach",
    "section": "",
    "text": "Getting data\nExploratory data analysis and preprocessing\nClustering analysis"
  },
  {
    "objectID": "r0_getdata.html",
    "href": "r0_getdata.html",
    "title": "Downloading GEO data",
    "section": "",
    "text": "In this report, I download the 3409 breast cancer bulk RNA-seq samples and the corresponding clinical annotations, associated with the study: Clinical Value of RNA Sequencing-Based Classifiers for Prediction of the Five Conventional Breast Cancer Biomarkers: A Report From the Population-Based Multicenter Sweden Cancerome Analysis Network—Breast Initiative (GSE96058).\n\n\nif(!dir.exists(here::here('data')))\n  dir.create(here::here('data'))\n\n\n## get geo obj\ngeo &lt;- getGEO(GEO = \"GSE96058\", GSEMatrix = FALSE)\n\n## fetch metadata\nmeta &lt;- purrr::map(geo@gsms, ~.x@header$characteristics_ch1) |&gt;\n  stack() |&gt;\n  tidyr::separate(values, into = c(\"feature\", \"value\"), sep= \": \") |&gt;\n  tidyr::pivot_wider(names_from= feature, values_from = value) |&gt;\n  janitor::clean_names()\n\n## map samples\nsample &lt;- purrr::map(geo@gsms, ~.x@header$title) |&gt;\n  stack() |&gt;\n  as_tibble() |&gt;\n  mutate(ind = as.character(ind))\n\n## store metadata\nmeta &lt;- left_join(sample, meta, by = 'ind') |&gt;\n  write_csv(here::here(\"data/metadata.csv\"))\n\n\n\nsuccess &lt;- FALSE\nattempt &lt;- 1\nwhile (!success && attempt &lt;= 5) {\n  tryCatch({\n    getGEOSuppFiles(\"GSE96058\", makeDirectory = FALSE,\n                    baseDir = here::here('data'),\n                    fetch_files = TRUE, filter_regex = 'gene_expression')\n    success &lt;- TRUE\n    message(\"Download successful on attempt \", attempt)\n  }, error = function(e) {\n    message(\"Download failed on attempt \", attempt, \": \", e$message)\n    attempt &lt;&lt;- attempt + 1\n    Sys.sleep(5) # wait before retrying\n  })\n}\nif (!success)\n  stop(sprintf(\"Download failed after %s attempts\", attempt))"
  },
  {
    "objectID": "r0_getdata.html#aims",
    "href": "r0_getdata.html#aims",
    "title": "Downloading GEO data",
    "section": "",
    "text": "In this report, I download the 3409 breast cancer bulk RNA-seq samples and the corresponding clinical annotations, associated with the study: Clinical Value of RNA Sequencing-Based Classifiers for Prediction of the Five Conventional Breast Cancer Biomarkers: A Report From the Population-Based Multicenter Sweden Cancerome Analysis Network—Breast Initiative (GSE96058).\n\n\nif(!dir.exists(here::here('data')))\n  dir.create(here::here('data'))\n\n\n## get geo obj\ngeo &lt;- getGEO(GEO = \"GSE96058\", GSEMatrix = FALSE)\n\n## fetch metadata\nmeta &lt;- purrr::map(geo@gsms, ~.x@header$characteristics_ch1) |&gt;\n  stack() |&gt;\n  tidyr::separate(values, into = c(\"feature\", \"value\"), sep= \": \") |&gt;\n  tidyr::pivot_wider(names_from= feature, values_from = value) |&gt;\n  janitor::clean_names()\n\n## map samples\nsample &lt;- purrr::map(geo@gsms, ~.x@header$title) |&gt;\n  stack() |&gt;\n  as_tibble() |&gt;\n  mutate(ind = as.character(ind))\n\n## store metadata\nmeta &lt;- left_join(sample, meta, by = 'ind') |&gt;\n  write_csv(here::here(\"data/metadata.csv\"))\n\n\n\nsuccess &lt;- FALSE\nattempt &lt;- 1\nwhile (!success && attempt &lt;= 5) {\n  tryCatch({\n    getGEOSuppFiles(\"GSE96058\", makeDirectory = FALSE,\n                    baseDir = here::here('data'),\n                    fetch_files = TRUE, filter_regex = 'gene_expression')\n    success &lt;- TRUE\n    message(\"Download successful on attempt \", attempt)\n  }, error = function(e) {\n    message(\"Download failed on attempt \", attempt, \": \", e$message)\n    attempt &lt;&lt;- attempt + 1\n    Sys.sleep(5) # wait before retrying\n  })\n}\nif (!success)\n  stop(sprintf(\"Download failed after %s attempts\", attempt))"
  },
  {
    "objectID": "r2_clustering.html",
    "href": "r2_clustering.html",
    "title": "Identification of breast cancer subtypes",
    "section": "",
    "text": "The goal of this report is to identify breast cancer subtypes by using a graph-based approach. To this end, I built a K-nearest neighbor (KNN) graph, where each node is a patient connected to its nearest neighbors, in the high-dimensional space (i.e. I used the top 25 principal components and the top 3000 variable genes). Edges between patients are weighted based on the Jaccard similarity, the higher the weight the larger is their overlap in their local neighborhoods. I then applied the Louvain algorithm to identify patient communities, where patients in the same group are more strongly connected to each others compared to those in different groups (on the basis of gene expression profiles). Finally, I visualized the cluster distribution with t-SNE and UMAP and I labeled patients on the basis of status of 5 biomarkers (estrogen receptor (ER), progesterone receptor (PgR), human epidermal growth factor receptor 2 (HER2), Ki67, and Nottingham histologic grade (NHG)) to see if there are associations between patients communities and biomarker status."
  },
  {
    "objectID": "r2_clustering.html#aims",
    "href": "r2_clustering.html#aims",
    "title": "Identification of breast cancer subtypes",
    "section": "",
    "text": "The goal of this report is to identify breast cancer subtypes by using a graph-based approach. To this end, I built a K-nearest neighbor (KNN) graph, where each node is a patient connected to its nearest neighbors, in the high-dimensional space (i.e. I used the top 25 principal components and the top 3000 variable genes). Edges between patients are weighted based on the Jaccard similarity, the higher the weight the larger is their overlap in their local neighborhoods. I then applied the Louvain algorithm to identify patient communities, where patients in the same group are more strongly connected to each others compared to those in different groups (on the basis of gene expression profiles). Finally, I visualized the cluster distribution with t-SNE and UMAP and I labeled patients on the basis of status of 5 biomarkers (estrogen receptor (ER), progesterone receptor (PgR), human epidermal growth factor receptor 2 (HER2), Ki67, and Nottingham histologic grade (NHG)) to see if there are associations between patients communities and biomarker status."
  },
  {
    "objectID": "r2_clustering.html#methods",
    "href": "r2_clustering.html#methods",
    "title": "Identification of breast cancer subtypes",
    "section": "Methods",
    "text": "Methods\nDue to the presence of missing values (NA) in biomarker status (see Biomarkers Annotation), I explore these scenarios:\n\nClustering of all 3273 patients and annotation according to PAM50 subtypes (consensus histopathology labels​) provided in the metadata\nClustering of patients with complete annotations for all 5 biomarkers (1373 patients)\nClustering of patients separately for each biomarker (excluding NA)\n\nFor each clustering scenario, I considered the top 3000 genes that exhibit the highest patient-to-patient variation in the dataset (i.e, those genes that are highly expressed in some patients, and lowly expressed in others)."
  },
  {
    "objectID": "r2_clustering.html#results",
    "href": "r2_clustering.html#results",
    "title": "Identification of breast cancer subtypes",
    "section": "Results",
    "text": "Results\nOverall, the analysis suggests that patients stratify according to the 5 biomarkers, in particular according to ER and PgR status.\nLoading data\n\ngexp &lt;- read_csv(\n  'data/gene_expression_profile.csv.gz',\n  col_types = cols()\n)\n\nmeta &lt;- read_csv('data/metadata.csv', col_types = cols()) |&gt;\n  rename(sampleID = values, sampleName = ind) |&gt;\n  filter(sampleID %in% names(gexp)) |&gt;\n  mutate(\n    er_status = factor(er_status, levels = c(0, 1), labels = c(\"ER-\", \"ER+\")),\n    pgr_status = factor(pgr_status, levels = c(0, 1), labels = c(\"PgR-\", \"PgR+\")),\n    her2_status = factor(her2_status, levels = c(0, 1), labels = c(\"HER2-\", \"HER2+\")),\n    ki67_status = factor(ki67_status, levels = c(0, 1), labels = c(\"Ki67-\", \"Ki67+\")),\n    overall_survival_event = factor(overall_survival_event, levels = c(0, 1), labels = c(\"no survival\", \"survival\")),\n    endocrine_treated = factor(endocrine_treated, levels = c(0, 1), labels = c(\"no treated\", \"treated\")),\n    chemo_treated = factor(chemo_treated, levels = c(0, 1), labels = c(\"no treated\", \"treated\")),\n    lymph_node_group = factor(lymph_node_group),\n    lymph_node_status = factor(lymph_node_status),\n    pam50_subtype = factor(pam50_subtype),\n    nhg = factor(nhg)\n  )\n\n\nmd &lt;- meta |&gt; \n  column_to_rownames(var = 'sampleID')\n\nmx &lt;- gexp |&gt; \n  select(genes, rownames(md)) |&gt;\n  column_to_rownames(var = 'genes')\n\nBiomarkers Annotation\nPercentage of patients with a given annotation is reported for each biomarker.\n\nmeta_long &lt;- meta |&gt;\n  select(er_status, pgr_status, her2_status, ki67_status, nhg) |&gt;\n  pivot_longer(cols = everything(), names_to = \"biomarker\", values_to = \"status\")\n\ndf_counts &lt;- meta_long |&gt;\n  group_by(biomarker, status) |&gt;\n  summarise(n = n(), .groups = \"drop\") |&gt;\n  group_by(biomarker) |&gt;\n  mutate(perc = n / sum(n))\n\nggplot(df_counts, aes(x = biomarker, y = perc, fill = status)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(\n    aes(label = scales::percent(perc, accuracy = 1)),\n    position = position_stack(vjust = 0.5)\n  ) +\n  labs(x = \"\", y = \"\") +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  guides(fill = guide_legend(title = NULL)) +\n  theme_minimal() +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank()\n  )\n\n\n\n\n\n\n\nFunctions\nFunctions used to build the pipeline.\n\ntop_genes &lt;- function(mat, n_top = 500){\n  # use smallest between n_top and number of genes\n  n_top &lt;- min(n_top, nrow(mat))\n  # order by decreasing gene variance and slice\n  rv &lt;- matrixStats::rowVars(as.matrix(mat))\n  select_n &lt;- order(rv, decreasing = TRUE)[seq_len(n_top)]\n  mat &lt;- mat[select_n, ]\n  return(mat)\n}\n\nget_pc &lt;- function(mx, md){\n  pc &lt;- PCAtools::pca(mx, metadata = md, center = TRUE, scale = FALSE, removeVar = 0.1)\n  return(pc)\n}\n\nbuild_graph &lt;- function(pc, npc = 25, k = 20){\n  ## pc space\n  pcm &lt;- pc$rotated[, 1:npc]\n\n  # find k-nearest neighbors\n  knn_result &lt;- RANN::nn2(pcm, k = k)\n  knn_idx &lt;- knn_result$nn.idx\n  \n  # store sparse matrix triplets (i, j, value)\n  i_indices &lt;- c()\n  j_indices &lt;- c()\n  values &lt;- c()\n\n  npat &lt;- nrow(pcm)\n  # adj &lt;- matrix(0, npat, npat)\n\n  for (i in 1:npat){\n    for (j in knn_idx[i, ]){\n      if (i != j){ ## avoid self-loop\n        neighbors_i &lt;- knn_idx[i,]\n        neighbors_j &lt;- knn_idx[j,]\n        jaccard_sim &lt;- length(intersect(neighbors_i, neighbors_j)) / \n                       length(union(neighbors_i, neighbors_j)) ## union takes unique values\n\n        # Store indices and values for sparse matrix\n        i_indices &lt;- c(i_indices, i)\n        j_indices &lt;- c(j_indices, j)\n        values &lt;- c(values, jaccard_sim)\n        \n        # dense matrix \n        # adjt[i, j] &lt;- jaccard_sim\n      }\n    }\n  }\n\n  # Create sparse matrix using triplet format\n  adj &lt;- sparseMatrix(i = i_indices, j = j_indices, x = values, dims = c(npat, npat))\n\n  # build graph\n  g &lt;- graph_from_adjacency_matrix(adj,\n    mode = \"max\", ## preserve the strongest connections, same of adj &lt;- pmax(adj, t(adj))\n    weighted = TRUE\n  )\n  return(g)\n}\n\nfind_clusters &lt;- function(g, mx, resolution = 1){\n  # Louvain algorithm for community detection\n  louvain_communities &lt;- cluster_louvain(g, resolution = resolution)\n\n  # cluster assignment\n  clusters &lt;- membership(louvain_communities)\n\n  return(clusters)\n}\n\n## umap wrapper\numap &lt;- function(mx, ...){\n  defaults &lt;- list(\n    n_components = 2,\n    n_neighbors = 20,     ## as perplexity in t-SNE\n    min_dist = 0.1,       ## how tightly points cluster together\n    metric = \"euclidean\",\n    spread = 1,           ## global structure preservation\n    n_threads = 10\n  )\n  user_args &lt;- modifyList(defaults, list(...))\n  purrr::exec(uwot::umap, X = t(mx), !!!user_args)\n}\n\n## tsne wrapper\ntsne &lt;- function(mx, ...){\n  defaults &lt;- list(\n    dims = 2,\n    perplexity = 20,\n    num_threads = 10\n  )\n  user_args &lt;- modifyList(defaults, list(...))\n  purrr::exec(Rtsne::Rtsne, X = t(mx), !!!user_args)\n}\n\njoin_results &lt;- function(tib, pc, clusters){\n  if(is.list(tib)){ ## tsne returns a list ..\n    restib &lt;- tibble(\n      sample = rownames(pc$rotated),\n      cluster = as.factor(clusters),\n      tsne1 = tib$Y[, 1],\n      tsne2 = tib$Y[, 2]) |&gt;\n      left_join(meta, by = c('sample' = 'sampleID'))\n  }else{ ## .. umap a dataframe\n    restib &lt;- tibble(\n      sample = rownames(pc$rotated),\n      cluster = as.factor(clusters),\n      umap1 = tib[, 1],\n      umap2 = tib[, 2]) |&gt;\n      left_join(meta, by = c('sample' = 'sampleID'))\n  }\n  return(restib)\n}\n\nplot_clusters &lt;- function(tib, cluster = 'cluster', animate = FALSE){\n  dim1 &lt;- colnames(tib)[3]\n  dim2 &lt;- colnames(tib)[4]\n  \n  p &lt;- ggplot(tib, aes(x = !!sym(dim1), y = !!sym(dim2), color = !!sym(cluster))) +\n    geom_point(alpha = 0.7, size = 2) +\n    labs(title = str_replace(cluster, '_', ' '), x = dim1, y = dim2) +\n    theme_minimal()\n\n  if(!animate & cluster == 'cluster'){\n    # Calculate cluster centroids for label positioning\n    cluster_centers &lt;- tib |&gt;\n      group_by(!!sym(cluster)) |&gt;\n      summarise(\n        x_center = mean(!!sym(dim1), na.rm = TRUE),\n        y_center = mean(!!sym(dim2), na.rm = TRUE),\n        .groups = 'drop'\n      )\n    \n    p &lt;- p +\n      geom_text_repel(\n        data = cluster_centers,\n        aes(x = x_center, y = y_center, label = !!sym(cluster)),\n        color = \"black\",\n        size = 8,\n        fontface = \"bold\",\n        vjust = 0.5,\n        hjust = 0.5\n      )\n  }\n  return(p)\n}\n\nformatter &lt;- function(tib, cols_to_format = \"all\", digits = 3){\n  selector &lt;- if(length(cols_to_format) == 1 && cols_to_format == \"all\"){\n    where(is.numeric)\n  } else {\n    all_of(cols_to_format)\n  }\n\n  tib |&gt; \n    mutate(across({{ selector }},\n           ~format(., scientific = TRUE, digits = digits)))\n}\n\ndatatable &lt;- function(tib, row2display = 10) {\n  if(nrow(tib) &gt; 0){\n    DT::datatable(tib,\n      rownames   = FALSE,\n      extensions = \"Buttons\",\n      options    = list(\n        dom = \"Bfrtip\",\n        scrollX = TRUE,\n        pageLength = row2display,\n        buttons = list(\n          list(\n            extend  = \"collection\",\n            buttons = c(\"csv\", \"excel\"),\n            text    = \"Download\"\n          )\n        )\n      ),\n      class = \"display\",\n      style = \"bootstrap\"\n    )\n  }else{\n    print(\"No results\")\n  }\n}\n\nA. Clustering all the patients\n\nmxtop &lt;- top_genes(mx, n_top = 3000)\npc &lt;- get_pc(mxtop, md)\ng &lt;- build_graph(pc, npc = 25, k = 20)\ncls &lt;- find_clusters(g, mxtop, resolution = 1)\n\nt-SNE\n\ntsneres &lt;- tsne(mxtop, perplexity = 20)\ntsnetib &lt;- join_results(tsneres, pc, cls)\n\nvars &lt;- c('cluster', 'pam50_subtype')\nplist &lt;- map(vars, ~plot_clusters(tsnetib, .x))\nwrap_plots(plist, ncol = 2)\n\n\n\n\n\n\n\nUMAP\n\numapres &lt;- umap(mxtop, n_neighbors = 20, spread = 1)\numaptib &lt;- join_results(umapres, pc, cls)\n\nvars &lt;- c('cluster', 'pam50_subtype')\nplist &lt;- map(vars, ~plot_clusters(umaptib, .x))\nwrap_plots(plist, ncol = 2)\n\n\n\n\n\n\n\nPatient cluster assignment\nThe table below shows the obtained cluster assignment for each patient.\n\ntsnetib |&gt; \n  left_join(umaptib, by = 'sample', suffix=c('_tsne', '_umap')) |&gt;\n  select(sample, sampleName_tsne, cluster_tsne, \n  tsne1, tsne2, umap1, umap2, \n  tumor_size_tsne, lymph_node_group_tsne, \n  lymph_node_status_tsne, er_status_tsne, \n  pgr_status_tsne, her2_status_tsne,           \n  ki67_status_tsne, nhg_tsne, \n  overall_survival_days_tsne, overall_survival_event_tsne,\n  endocrine_treated_tsne, chemo_treated_tsne) |&gt; \n  rename_with(~ str_remove(.x, \"_tsne$\")) |&gt;\n  formatter(cols_to_format = c(\"tsne1\", \"tsne2\", \"umap1\", \"umap2\")) |&gt;\n  datatable()\n\n\n\n\n\nB. Clustering of patients with complete annotations for all 5 biomarkers\n\n## reduce dataset to complete cases \nmd_complete &lt;- meta |&gt;\n  filter(!is.na(er_status)   & !is.na(pgr_status) & \n         !is.na(her2_status) & !is.na(ki67_status) &\n         !is.na(nhg)) |&gt;\n  column_to_rownames(var = 'sampleID')       \n\nmx_complete &lt;- gexp |&gt; \n    select(genes, rownames(md_complete)) |&gt;\n    column_to_rownames(var = 'genes')\n\n\n## run pipeline\nmxtop_complete &lt;- top_genes(mx_complete, n_top = 3000)\npc_complete &lt;- get_pc(mxtop_complete, md_complete)\ng_complete &lt;- build_graph(pc_complete, npc = 25, k = 20)\ncls_complete &lt;- find_clusters(g_complete, mxtop_complete, resolution = 1)\n\nt-SNE\n\nvars &lt;- c('cluster', 'er_status', 'pgr_status', 'her2_status', 'ki67_status', 'nhg')\n\n## tsne\ntsneres_complete &lt;- tsne(mxtop_complete, perplexity = 20)\ntsnetib_complete &lt;- join_results(tsneres_complete, pc_complete, cls_complete)\n\nplist &lt;- map(vars, ~plot_clusters(tsnetib_complete, .x))\nwrap_plots(plist, ncol = 2)\n\n\n\n\n\n\n\nUMAP\n\numapres_complete &lt;- umap(mxtop_complete, n_neighbors = 20, spread = 1)\numaptib_complete &lt;- join_results(umapres_complete, pc_complete, cls_complete)\n\nplist &lt;- map(vars, ~plot_clusters(umaptib_complete, .x))\nwrap_plots(plist, ncol = 2)\n\n\n\n\n\n\n\nC. Clustering of patients separately for each biomarker\n\nbiomarkers &lt;- c('er_status', 'pgr_status', 'her2_status', 'ki67_status', 'nhg')\n\ntsnetib_bm &lt;- list()\numaptib_bm &lt;- list()\n\nptsne &lt;- list()\npumap &lt;- list()\n\nfor(biomarker in biomarkers){\n  ## remove not annotated patients\n  md_bm &lt;- meta |&gt;\n    filter(!is.na(!!sym(biomarker))) |&gt;\n    column_to_rownames(var = 'sampleID')\n\n  mx_bm &lt;- gexp |&gt; \n    select(genes, rownames(md_bm)) |&gt;\n    column_to_rownames(var = 'genes')\n\n  ## clustering\n  mxtop_bm &lt;- top_genes(mx_bm, n_top = 3000)\n  pc_bm &lt;- get_pc(mxtop_bm, md_bm)\n  g_bm &lt;- build_graph(pc_bm, npc = 25, k = 20)\n  cls_bm &lt;- find_clusters(g_bm, mxtop_bm, resolution = 1)\n\n  ## save results\n  tsneres_bm &lt;- tsne(mxtop_bm, perplexity = 20)\n  tsnetib_bm[[biomarker]] &lt;- join_results(tsneres_bm, pc_bm, cls_bm)\n\n  umapres_bm &lt;- umap(mxtop_bm,  n_neighbors = 20)\n  umaptib_bm[[biomarker]] &lt;- join_results(umapres_bm, pc_bm, cls_bm)\n\n  vars &lt;- c('cluster', biomarker)\n  ptsne[[biomarker]] &lt;- map(vars, ~plot_clusters(tsnetib_bm[[biomarker]], .x))\n  pumap[[biomarker]] &lt;- map(vars, ~plot_clusters(umaptib_bm[[biomarker]], .x))\n}\n\nt-SNE\n\npwrap &lt;- map(ptsne, ~.x[[1]] + .x[[2]])\nwrap_plots(pwrap, ncol = 1)\n\n\n\n\n\n\n\nUMAP\n\npwrap &lt;- map(pumap, ~.x[[1]] + .x[[2]])\nwrap_plots(pwrap, ncol = 1)\n\n\n\n\n\n\n\nClustering at different resolutions\nClusters detected at each resolutions are marked in different colors. Patients in the community colored in red tend to cluster together at different resolutions, suggesting that patients within this community are strongly connected (similar expression profiles).\n\n# loop for resolutions\nresolutions &lt;- c(0.1, 0.3, 0.6, 1, 1.5)\n\ntsne_tune &lt;- map_dfr(resolutions, function(resolution){\n  cls &lt;- find_clusters(g, mxtop, resolution = resolution) \n  tsne &lt;- tsne(mxtop, dims = 2, perplexity = 20)\n  tibble(\n    sample = colnames(mxtop),\n    cluster = as.factor(cls),\n    tsne1 = tsne$Y[, 1],\n    tsne2 = tsne$Y[, 2],\n    resolution = resolution,\n    ncl = length(unique(cls)) ## number of clusters at a given resolution\n  ) |&gt;\n  left_join(meta, by = c('sample' = 'sampleID'))\n})\n\n# {unique(tsne_tune$ncl[tsne_tune$resolution == closest_state])}\nncl_lookup &lt;- tsne_tune |&gt; distinct(resolution, ncl) |&gt; deframe()\n\nplot &lt;- plot_clusters(tsne_tune, cluster = 'cluster', animate = TRUE) +\n  # theme(legend.position=\"none\") +\n  labs(subtitle = \"Resolution: {closest_state} | Clusters: {ncl_lookup[as.character(closest_state)]}\") +\n  transition_states(resolution, transition_length = 5, state_length = 3) +\n  ease_aes(\"linear\")\n\nanimate(\n  plot,\n  width = 8,\n  height = 6,\n  res = 100,\n  nframes = 300,\n  fps = 30,\n  device = \"ragg_png\",\n  renderer = gifski_renderer()\n)\n\n\n\n\n\n\n\nWhen specifically coloring patients based on ER status, we observe that the expression profiles of ER- patients cluster well. Furthermore, it is worth noting that the status of some patients within this cluster is unknown (gray patients). These might be considered as ER- patients since they cluster strongly with patients annotated as ER-.\n\nplot &lt;- plot_clusters(tsne_tune, cluster = 'er_status', animate = TRUE) +\n  labs(subtitle = \"Resolution: {closest_state} | Clusters: {ncl_lookup[as.character(closest_state)]}\") +\n  transition_states(resolution, transition_length = 5, state_length = 3) +\n  ease_aes(\"linear\")\n\nanimate(\n  plot,\n  width = 8,\n  height = 6,\n  res = 100,\n  nframes = 300,\n  fps = 30,\n  device = \"ragg_png\",\n  renderer = gifski_renderer()\n)"
  },
  {
    "objectID": "r2_clustering.html#conclusions",
    "href": "r2_clustering.html#conclusions",
    "title": "Identification of breast cancer subtypes",
    "section": "Conclusions",
    "text": "Conclusions\nI identified breast cancer subtypes based on gene expression data via a graph-based approach. Two main distinct clusters of patients are detected. The smaller cluster is enriched for ER- patients and likely represent the most aggressive expression profile (basal PAM50 subtype, e.g. G3, PgR-, ER-, HER-)."
  },
  {
    "objectID": "r2_clustering.html#next-step",
    "href": "r2_clustering.html#next-step",
    "title": "Identification of breast cancer subtypes",
    "section": "Next step",
    "text": "Next step\nCluster assignment can be used as features in a supervised learning approach to predict biomarker status (such as random forest, support vector machine, label propagation). Furthermore, we can use LLMs to further validate biological insights using AI-powered tools tailored for bioinformatics resources such as ExpasyGPT."
  },
  {
    "objectID": "detroit-wiki.html",
    "href": "detroit-wiki.html",
    "title": "Visualize Detroit’s sports teams win percentages",
    "section": "",
    "text": "How have the win percentages for Detroit’s major sports teams (Pistons, Red Wings, Tigers, and Lions) changed over the past 80 years using a 10-year sliding window?\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom packaging.version import Version\n\n\n\n\ndatasets={\n    'pistons':  'https://en.wikipedia.org/wiki/List_of_Detroit_Pistons_seasons',\n    'redwings': 'https://en.wikipedia.org/wiki/List_of_Detroit_Red_Wings_seasons',\n    'tiger':    'https://en.wikipedia.org/wiki/List_of_Detroit_Tigers_seasons',\n    'lions':    'https://en.wikipedia.org/wiki/List_of_Detroit_Lions_seasons'\n}\n\n\n\n\n\ndef get_window_and_winratio(df, window=10, team='Pistons'):\n    df['Window'] = (df['Year'] // window) * window\n    if team == 'RedWings' or team == 'Lions':\n        df = df.groupby('Window').agg({'Wins': 'sum', 'Losses': 'sum', 'Ties': 'sum'}).reset_index()\n        df['WinRatio'] = (df['Wins'] + 0.5 * df['Ties']) / (df['Wins'] + df['Losses'] + df['Ties'])\n    else:\n        df = df.groupby('Window').agg({'Wins': 'sum', 'Losses': 'sum'}).reset_index()\n        df['WinRatio'] = df['Wins'] / (df['Wins'] + df['Losses'])\n    df.columns = [col + '_' + team if col != 'Window' else col for col in df.columns]\n    return df\n\n\n\n\n\n# pd.set_option('display.max_rows', None)\n# pd.reset_option('display.max_rows')\n\nwiki_pistons = pd.read_html(datasets['pistons'])[1][1:]\n\npistons = pd.DataFrame()\npistons['Year'] = wiki_pistons['Team'].str[:4]\npistons[['Wins','Losses']] = wiki_pistons[['Wins','Losses']]\npistons = pistons[~(pistons['Year'].str.contains('Fort|Detr'))]\npistons[['Year', 'Wins', 'Losses']] = pistons[['Year', 'Wins', 'Losses']].astype(int)\npistons\n\n\n\n\n\n\n\n\nYear\nWins\nLosses\n\n\n\n\n1\n1941\n15\n9\n\n\n2\n1942\n17\n6\n\n\n3\n1943\n18\n4\n\n\n4\n1944\n25\n5\n\n\n5\n1945\n26\n8\n\n\n...\n...\n...\n...\n\n\n82\n2020\n20\n52\n\n\n83\n2021\n23\n59\n\n\n84\n2022\n17\n65\n\n\n85\n2023\n14\n68\n\n\n86\n2024\n44\n38\n\n\n\n\n84 rows × 3 columns\n\n\n\n\n\n\n\n# pd.set_option('display.max_rows', None)\n# pd.reset_option('display.max_rows')\n\nwiki_redwings = pd.read_html(datasets['redwings'])[2][1:]\n\nredwings = pd.DataFrame()\nredwings['Year'] = wiki_redwings['NHL season']['NHL season'].str[:4]\nredwings[['GP', 'Wins','Losses', 'Ties', 'OT']] = wiki_redwings['Regular season[3][6][7][8]'][['GP','W','L','T','OT']]\nredwings = redwings[~((redwings['Year'].str.contains('^Detr|^Tota')) |\n                      (redwings['Wins'].str.contains('^—')) |\n                      (redwings['Losses'].str.contains('^—')) |\n                      (redwings['Ties'].str.contains('^—\\[m\\]')))]\n\nredwings['OT'] = redwings['OT'].str.replace('\\[k\\]','', regex=True)\nredwings['Ties'] = redwings['Ties'].apply(lambda x: 0 if x == '—' else x) # redwings.loc[redwings['Ties'] == '—', 'Ties'] = 0\nredwings['OT']   = redwings['OT'].apply(lambda x: 0 if x == '—' else x)   # redwings.loc[redwings['OT'] == '—', 'OT'] = 0\n\nredwings[['Year', 'Wins', 'Losses', 'Ties', 'OT']] = redwings[['Year', 'Wins', 'Losses', 'Ties', 'OT']].astype(int)\nredwings['Ties'] = redwings['Ties'] + redwings['OT']\nredwings = redwings[['Year', 'Wins', 'Losses', 'Ties']]\nredwings\n\n\n\n\n\n\n\n\nYear\nWins\nLosses\nTies\n\n\n\n\n1\n1926\n12\n28\n4\n\n\n2\n1927\n19\n19\n6\n\n\n3\n1928\n19\n16\n9\n\n\n4\n1929\n14\n24\n6\n\n\n6\n1930\n16\n21\n7\n\n\n...\n...\n...\n...\n...\n\n\n97\n2020\n19\n27\n10\n\n\n98\n2021\n32\n40\n10\n\n\n99\n2022\n35\n37\n10\n\n\n100\n2023\n41\n32\n9\n\n\n101\n2024\n39\n35\n8\n\n\n\n\n97 rows × 4 columns\n\n\n\n\n\n\n\n# pd.set_option('display.max_rows', None)\n# pd.reset_option('display.max_rows')\n\nwiki_tiger = pd.read_html(datasets['tiger'])[1]\n\ntiger = pd.DataFrame()\ntiger['Year'] = wiki_tiger['Season']\ntiger[['Wins','Losses']] = wiki_tiger[['Wins','Losses']]\n\ntiger = tiger[~(tiger['Year'].str.contains('Total'))]\ntiger[['Year', 'Wins', 'Losses']] = tiger[['Year', 'Wins', 'Losses']].astype(int)\n\ntiger\n\n\n\n\n\n\n\n\nYear\nWins\nLosses\n\n\n\n\n0\n1901\n74\n61\n\n\n1\n1902\n52\n83\n\n\n2\n1903\n65\n71\n\n\n3\n1904\n62\n90\n\n\n4\n1905\n79\n74\n\n\n...\n...\n...\n...\n\n\n120\n2020\n23\n35\n\n\n121\n2021\n77\n85\n\n\n122\n2022\n66\n96\n\n\n123\n2023\n78\n84\n\n\n124\n2024\n86\n76\n\n\n\n\n125 rows × 3 columns\n\n\n\n\n\n\n\n# pd.set_option('display.max_rows', None)\n# pd.reset_option('display.max_rows')\n\npd_version = Version(pd.__version__)\n\nwiki_lions = pd.read_html(datasets['lions'])[1]\n\nlions = pd.DataFrame()\nlions['Year'] = wiki_lions['Season'][['Season']]\nif pd_version &lt;=  Version(\"1.5.2\"):\n    lions[['Wins','Losses', 'Ties']] = wiki_lions['Regular season'][['.mw-parser-output .tooltip-dotted{border-bottom:1px dotted;cursor:help}W', 'L', 'T']]\nelse:\n    lions[['Wins','Losses', 'Ties']] = wiki_lions['Regular season'][['W', 'L', 'T']]\nlions = lions[~(lions['Year'].str.startswith('Totals'))]\nlions[['Year', 'Wins', 'Losses', 'Ties']] = lions[['Year', 'Wins', 'Losses', 'Ties']].astype(int)\n\nlions\n\n\n\n\n\n\n\n\nYear\nWins\nLosses\nTies\n\n\n\n\n0\n1928\n9\n3\n2\n\n\n1\n1929\n12\n2\n1\n\n\n2\n1930\n5\n6\n3\n\n\n3\n1931\n11\n3\n0\n\n\n4\n1932\n6\n2\n4\n\n\n...\n...\n...\n...\n...\n\n\n92\n2020\n5\n11\n0\n\n\n93\n2021\n3\n13\n1\n\n\n94\n2022\n9\n8\n0\n\n\n95\n2023\n12\n5\n0\n\n\n96\n2024\n15\n2\n0\n\n\n\n\n97 rows × 4 columns\n\n\n\n\n\n\n\nwindow=10 ## change window\n\npistons_agg = get_window_and_winratio(pistons, window=window, team='Pistons')\nredwings_agg = get_window_and_winratio(redwings, window=window, team='RedWings')\ntiger_agg = get_window_and_winratio(tiger, window=window, team='Tiger')\nlions_agg = get_window_and_winratio(lions, window=window, team='Lions')\n\ndf = pd.merge(pistons_agg, redwings_agg, on='Window').merge(tiger_agg, on='Window').merge(lions_agg, on='Window')\n\ndf.set_index('Window', inplace=True)\ndf\n\n\n\n\n\n\n\n\nWins_Pistons\nLosses_Pistons\nWinRatio_Pistons\nWins_RedWings\nLosses_RedWings\nTies_RedWings\nWinRatio_RedWings\nWins_Tiger\nLosses_Tiger\nWinRatio_Tiger\nWins_Lions\nLosses_Lions\nTies_Lions\nWinRatio_Lions\n\n\nWindow\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1940\n228\n137\n0.624658\n265\n190\n91\n0.568681\n834\n705\n0.541910\n35\n71\n4\n0.336364\n\n\n1950\n342\n368\n0.481690\n351\n218\n131\n0.595000\n738\n802\n0.479221\n68\n48\n4\n0.583333\n\n\n1960\n314\n492\n0.389578\n308\n292\n116\n0.511173\n882\n729\n0.547486\n66\n61\n11\n0.518116\n\n\n1970\n367\n453\n0.447561\n267\n410\n115\n0.409722\n789\n820\n0.490367\n66\n75\n3\n0.468750\n\n\n1980\n466\n354\n0.568293\n273\n410\n117\n0.414375\n839\n727\n0.535760\n61\n90\n1\n0.404605\n\n\n1990\n394\n394\n0.500000\n438\n248\n100\n0.620865\n702\n852\n0.451737\n79\n81\n0\n0.493750\n\n\n2000\n482\n338\n0.587805\n395\n163\n98\n0.676829\n729\n891\n0.450000\n42\n118\n0\n0.262500\n\n\n2010\n326\n462\n0.413706\n354\n316\n105\n0.524516\n782\n835\n0.483612\n72\n87\n1\n0.453125\n\n\n2020\n118\n282\n0.295000\n166\n171\n47\n0.493490\n330\n376\n0.467422\n44\n39\n1\n0.529762\n\n\n\n\n\n\n\n\n## check tiger aggregation with wiki data\n\ntigercheck = get_window_and_winratio(tiger, window=10, team='').rename(columns={'Window':'Decade'}).set_index('Decade')\ntigercheck.columns = [col.strip('_') for col in tigercheck.columns]\n\nwikidec = pd.read_html(datasets['tiger'])[2]\nwikidec['Decade'] = wikidec['Decade'].str.replace('s', '')\nwikidec = wikidec[:-1]\nwikidec = wikidec[['Decade', 'Wins','Losses']]\nwikidec[['Decade', 'Wins', 'Losses']] = wikidec[['Decade', 'Wins', 'Losses']].astype(int)\nwikidec.set_index('Decade', inplace=True)\n\nwikidec[['Wins','Losses']] == tigercheck[['Wins','Losses']] \npd.merge(tigercheck, wikidec, on='Decade')\ntiger[tiger['Window'] == 2020]\n\n# note: test passed for all windows except for 2020,\n#       because wikidec does not count wins/losses for 2024\n\n\n\n\n\n\n\n\nYear\nWins\nLosses\nWindow\n\n\n\n\n120\n2020\n23\n35\n2020\n\n\n121\n2021\n77\n85\n2020\n\n\n122\n2022\n66\n96\n2020\n\n\n123\n2023\n78\n84\n2020\n\n\n124\n2024\n86\n76\n2020\n\n\n\n\n\n\n\n\n\n\n\nfrom matplotlib.ticker import FixedLocator, FuncFormatter\nfrom scipy.interpolate import make_interp_spline\n\n# Teams and their win ratio columns \nteams = {'Pistons': 'WinRatio_Pistons', \n         'RedWings': 'WinRatio_RedWings', \n         'Tiger': 'WinRatio_Tiger', \n         'Lions': 'WinRatio_Lions'}\n\n# Generate new x values for smooth curves\nx_new = np.linspace(df.index.min(), df.index.max(), 300)\n\n# Create big4 smooth curves and plot\n# nb: it's better to smooth lines when you have several data points \nk=1 ## k&gt;=2 to smooth lines\nfig, ax = plt.subplots(figsize=(14, 8))\nfor team, col in teams.items():\n    spl = make_interp_spline(df.index, df[col], k=k)\n    y_smooth = spl(x_new)\n    plt.plot(x_new, y_smooth, label=team, linewidth=2)\n\nplt.yticks(size=11)\nplt.xticks(size=11)\nplt.xlabel('Season', size = 12)\nplt.ylabel('Average Win Percentage', size = 12)\nplt.title('Detroit Sports Team Win Perentage in a '+str(window)+'-year window', size = 14)\n\nyticks = np.arange(0.2, 0.8, 0.1) \nax.yaxis.set_major_locator(FixedLocator(yticks)) \nax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: f'{int(round(y * 100))}%')) \nax.set_ylim(0.20, 0.73)\n\nax.yaxis.grid(which='major', color='gray', linestyle='--', linewidth=0.5)\nax.spines[['right', 'top']].set_visible(False)\nplt.legend(loc='best', fontsize=13)\n\n# if k&gt;=2:\n#     plt.savefig('detroit-avg-win-smoothed.png')\n# else:\n#     plt.savefig('detroit-avg-win.png')\n    \nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe plot answers the question of how the winning percentages of the four major Detroit sports teams (Pistons, Red Wings, Tigers and Lions) have changed over the last 80 years. Wikipedia was scraped for data on wins, losses and ties by season for each team. For a fair comparison across sports, we computed the winning percentage as the number of wins divided by the total number of games played (i.e. wins plus ties plus losses). We assumed that a tie is 1/2 of a win, so the winning percentage was computed as \\(\\frac{(wins + 0.5×ties)}{(wins+ties+losses)}\\). For Pistons and Tigers (basketball and baseball) ties do not occur so the winning percentage was simply computed as \\(\\frac{(wins)}{(wins+losses)}\\). A 10 year moving average was plotted to identify trends in the team’s win percentages.\nThe Pistons saw high performance in 1940, 1980, and 2000, followed by periods of decline. The Red Wings experienced a notable rise from 1980 to 2000, with a subsequent downward trend. The Tigers’ win percentage remains relatively stable around 50% with minor fluctuations with lowest performance between 1990 and 2000. The Lions show an up and down trend, starting low, peaking in 1950, 1990 and 2010 and then declining again."
  },
  {
    "objectID": "detroit-wiki.html#goal",
    "href": "detroit-wiki.html#goal",
    "title": "Visualize Detroit’s sports teams win percentages",
    "section": "",
    "text": "How have the win percentages for Detroit’s major sports teams (Pistons, Red Wings, Tigers, and Lions) changed over the past 80 years using a 10-year sliding window?\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom packaging.version import Version\n\n\n\n\ndatasets={\n    'pistons':  'https://en.wikipedia.org/wiki/List_of_Detroit_Pistons_seasons',\n    'redwings': 'https://en.wikipedia.org/wiki/List_of_Detroit_Red_Wings_seasons',\n    'tiger':    'https://en.wikipedia.org/wiki/List_of_Detroit_Tigers_seasons',\n    'lions':    'https://en.wikipedia.org/wiki/List_of_Detroit_Lions_seasons'\n}\n\n\n\n\n\ndef get_window_and_winratio(df, window=10, team='Pistons'):\n    df['Window'] = (df['Year'] // window) * window\n    if team == 'RedWings' or team == 'Lions':\n        df = df.groupby('Window').agg({'Wins': 'sum', 'Losses': 'sum', 'Ties': 'sum'}).reset_index()\n        df['WinRatio'] = (df['Wins'] + 0.5 * df['Ties']) / (df['Wins'] + df['Losses'] + df['Ties'])\n    else:\n        df = df.groupby('Window').agg({'Wins': 'sum', 'Losses': 'sum'}).reset_index()\n        df['WinRatio'] = df['Wins'] / (df['Wins'] + df['Losses'])\n    df.columns = [col + '_' + team if col != 'Window' else col for col in df.columns]\n    return df\n\n\n\n\n\n# pd.set_option('display.max_rows', None)\n# pd.reset_option('display.max_rows')\n\nwiki_pistons = pd.read_html(datasets['pistons'])[1][1:]\n\npistons = pd.DataFrame()\npistons['Year'] = wiki_pistons['Team'].str[:4]\npistons[['Wins','Losses']] = wiki_pistons[['Wins','Losses']]\npistons = pistons[~(pistons['Year'].str.contains('Fort|Detr'))]\npistons[['Year', 'Wins', 'Losses']] = pistons[['Year', 'Wins', 'Losses']].astype(int)\npistons\n\n\n\n\n\n\n\n\nYear\nWins\nLosses\n\n\n\n\n1\n1941\n15\n9\n\n\n2\n1942\n17\n6\n\n\n3\n1943\n18\n4\n\n\n4\n1944\n25\n5\n\n\n5\n1945\n26\n8\n\n\n...\n...\n...\n...\n\n\n82\n2020\n20\n52\n\n\n83\n2021\n23\n59\n\n\n84\n2022\n17\n65\n\n\n85\n2023\n14\n68\n\n\n86\n2024\n44\n38\n\n\n\n\n84 rows × 3 columns\n\n\n\n\n\n\n\n# pd.set_option('display.max_rows', None)\n# pd.reset_option('display.max_rows')\n\nwiki_redwings = pd.read_html(datasets['redwings'])[2][1:]\n\nredwings = pd.DataFrame()\nredwings['Year'] = wiki_redwings['NHL season']['NHL season'].str[:4]\nredwings[['GP', 'Wins','Losses', 'Ties', 'OT']] = wiki_redwings['Regular season[3][6][7][8]'][['GP','W','L','T','OT']]\nredwings = redwings[~((redwings['Year'].str.contains('^Detr|^Tota')) |\n                      (redwings['Wins'].str.contains('^—')) |\n                      (redwings['Losses'].str.contains('^—')) |\n                      (redwings['Ties'].str.contains('^—\\[m\\]')))]\n\nredwings['OT'] = redwings['OT'].str.replace('\\[k\\]','', regex=True)\nredwings['Ties'] = redwings['Ties'].apply(lambda x: 0 if x == '—' else x) # redwings.loc[redwings['Ties'] == '—', 'Ties'] = 0\nredwings['OT']   = redwings['OT'].apply(lambda x: 0 if x == '—' else x)   # redwings.loc[redwings['OT'] == '—', 'OT'] = 0\n\nredwings[['Year', 'Wins', 'Losses', 'Ties', 'OT']] = redwings[['Year', 'Wins', 'Losses', 'Ties', 'OT']].astype(int)\nredwings['Ties'] = redwings['Ties'] + redwings['OT']\nredwings = redwings[['Year', 'Wins', 'Losses', 'Ties']]\nredwings\n\n\n\n\n\n\n\n\nYear\nWins\nLosses\nTies\n\n\n\n\n1\n1926\n12\n28\n4\n\n\n2\n1927\n19\n19\n6\n\n\n3\n1928\n19\n16\n9\n\n\n4\n1929\n14\n24\n6\n\n\n6\n1930\n16\n21\n7\n\n\n...\n...\n...\n...\n...\n\n\n97\n2020\n19\n27\n10\n\n\n98\n2021\n32\n40\n10\n\n\n99\n2022\n35\n37\n10\n\n\n100\n2023\n41\n32\n9\n\n\n101\n2024\n39\n35\n8\n\n\n\n\n97 rows × 4 columns\n\n\n\n\n\n\n\n# pd.set_option('display.max_rows', None)\n# pd.reset_option('display.max_rows')\n\nwiki_tiger = pd.read_html(datasets['tiger'])[1]\n\ntiger = pd.DataFrame()\ntiger['Year'] = wiki_tiger['Season']\ntiger[['Wins','Losses']] = wiki_tiger[['Wins','Losses']]\n\ntiger = tiger[~(tiger['Year'].str.contains('Total'))]\ntiger[['Year', 'Wins', 'Losses']] = tiger[['Year', 'Wins', 'Losses']].astype(int)\n\ntiger\n\n\n\n\n\n\n\n\nYear\nWins\nLosses\n\n\n\n\n0\n1901\n74\n61\n\n\n1\n1902\n52\n83\n\n\n2\n1903\n65\n71\n\n\n3\n1904\n62\n90\n\n\n4\n1905\n79\n74\n\n\n...\n...\n...\n...\n\n\n120\n2020\n23\n35\n\n\n121\n2021\n77\n85\n\n\n122\n2022\n66\n96\n\n\n123\n2023\n78\n84\n\n\n124\n2024\n86\n76\n\n\n\n\n125 rows × 3 columns\n\n\n\n\n\n\n\n# pd.set_option('display.max_rows', None)\n# pd.reset_option('display.max_rows')\n\npd_version = Version(pd.__version__)\n\nwiki_lions = pd.read_html(datasets['lions'])[1]\n\nlions = pd.DataFrame()\nlions['Year'] = wiki_lions['Season'][['Season']]\nif pd_version &lt;=  Version(\"1.5.2\"):\n    lions[['Wins','Losses', 'Ties']] = wiki_lions['Regular season'][['.mw-parser-output .tooltip-dotted{border-bottom:1px dotted;cursor:help}W', 'L', 'T']]\nelse:\n    lions[['Wins','Losses', 'Ties']] = wiki_lions['Regular season'][['W', 'L', 'T']]\nlions = lions[~(lions['Year'].str.startswith('Totals'))]\nlions[['Year', 'Wins', 'Losses', 'Ties']] = lions[['Year', 'Wins', 'Losses', 'Ties']].astype(int)\n\nlions\n\n\n\n\n\n\n\n\nYear\nWins\nLosses\nTies\n\n\n\n\n0\n1928\n9\n3\n2\n\n\n1\n1929\n12\n2\n1\n\n\n2\n1930\n5\n6\n3\n\n\n3\n1931\n11\n3\n0\n\n\n4\n1932\n6\n2\n4\n\n\n...\n...\n...\n...\n...\n\n\n92\n2020\n5\n11\n0\n\n\n93\n2021\n3\n13\n1\n\n\n94\n2022\n9\n8\n0\n\n\n95\n2023\n12\n5\n0\n\n\n96\n2024\n15\n2\n0\n\n\n\n\n97 rows × 4 columns\n\n\n\n\n\n\n\nwindow=10 ## change window\n\npistons_agg = get_window_and_winratio(pistons, window=window, team='Pistons')\nredwings_agg = get_window_and_winratio(redwings, window=window, team='RedWings')\ntiger_agg = get_window_and_winratio(tiger, window=window, team='Tiger')\nlions_agg = get_window_and_winratio(lions, window=window, team='Lions')\n\ndf = pd.merge(pistons_agg, redwings_agg, on='Window').merge(tiger_agg, on='Window').merge(lions_agg, on='Window')\n\ndf.set_index('Window', inplace=True)\ndf\n\n\n\n\n\n\n\n\nWins_Pistons\nLosses_Pistons\nWinRatio_Pistons\nWins_RedWings\nLosses_RedWings\nTies_RedWings\nWinRatio_RedWings\nWins_Tiger\nLosses_Tiger\nWinRatio_Tiger\nWins_Lions\nLosses_Lions\nTies_Lions\nWinRatio_Lions\n\n\nWindow\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1940\n228\n137\n0.624658\n265\n190\n91\n0.568681\n834\n705\n0.541910\n35\n71\n4\n0.336364\n\n\n1950\n342\n368\n0.481690\n351\n218\n131\n0.595000\n738\n802\n0.479221\n68\n48\n4\n0.583333\n\n\n1960\n314\n492\n0.389578\n308\n292\n116\n0.511173\n882\n729\n0.547486\n66\n61\n11\n0.518116\n\n\n1970\n367\n453\n0.447561\n267\n410\n115\n0.409722\n789\n820\n0.490367\n66\n75\n3\n0.468750\n\n\n1980\n466\n354\n0.568293\n273\n410\n117\n0.414375\n839\n727\n0.535760\n61\n90\n1\n0.404605\n\n\n1990\n394\n394\n0.500000\n438\n248\n100\n0.620865\n702\n852\n0.451737\n79\n81\n0\n0.493750\n\n\n2000\n482\n338\n0.587805\n395\n163\n98\n0.676829\n729\n891\n0.450000\n42\n118\n0\n0.262500\n\n\n2010\n326\n462\n0.413706\n354\n316\n105\n0.524516\n782\n835\n0.483612\n72\n87\n1\n0.453125\n\n\n2020\n118\n282\n0.295000\n166\n171\n47\n0.493490\n330\n376\n0.467422\n44\n39\n1\n0.529762\n\n\n\n\n\n\n\n\n## check tiger aggregation with wiki data\n\ntigercheck = get_window_and_winratio(tiger, window=10, team='').rename(columns={'Window':'Decade'}).set_index('Decade')\ntigercheck.columns = [col.strip('_') for col in tigercheck.columns]\n\nwikidec = pd.read_html(datasets['tiger'])[2]\nwikidec['Decade'] = wikidec['Decade'].str.replace('s', '')\nwikidec = wikidec[:-1]\nwikidec = wikidec[['Decade', 'Wins','Losses']]\nwikidec[['Decade', 'Wins', 'Losses']] = wikidec[['Decade', 'Wins', 'Losses']].astype(int)\nwikidec.set_index('Decade', inplace=True)\n\nwikidec[['Wins','Losses']] == tigercheck[['Wins','Losses']] \npd.merge(tigercheck, wikidec, on='Decade')\ntiger[tiger['Window'] == 2020]\n\n# note: test passed for all windows except for 2020,\n#       because wikidec does not count wins/losses for 2024\n\n\n\n\n\n\n\n\nYear\nWins\nLosses\nWindow\n\n\n\n\n120\n2020\n23\n35\n2020\n\n\n121\n2021\n77\n85\n2020\n\n\n122\n2022\n66\n96\n2020\n\n\n123\n2023\n78\n84\n2020\n\n\n124\n2024\n86\n76\n2020\n\n\n\n\n\n\n\n\n\n\n\nfrom matplotlib.ticker import FixedLocator, FuncFormatter\nfrom scipy.interpolate import make_interp_spline\n\n# Teams and their win ratio columns \nteams = {'Pistons': 'WinRatio_Pistons', \n         'RedWings': 'WinRatio_RedWings', \n         'Tiger': 'WinRatio_Tiger', \n         'Lions': 'WinRatio_Lions'}\n\n# Generate new x values for smooth curves\nx_new = np.linspace(df.index.min(), df.index.max(), 300)\n\n# Create big4 smooth curves and plot\n# nb: it's better to smooth lines when you have several data points \nk=1 ## k&gt;=2 to smooth lines\nfig, ax = plt.subplots(figsize=(14, 8))\nfor team, col in teams.items():\n    spl = make_interp_spline(df.index, df[col], k=k)\n    y_smooth = spl(x_new)\n    plt.plot(x_new, y_smooth, label=team, linewidth=2)\n\nplt.yticks(size=11)\nplt.xticks(size=11)\nplt.xlabel('Season', size = 12)\nplt.ylabel('Average Win Percentage', size = 12)\nplt.title('Detroit Sports Team Win Perentage in a '+str(window)+'-year window', size = 14)\n\nyticks = np.arange(0.2, 0.8, 0.1) \nax.yaxis.set_major_locator(FixedLocator(yticks)) \nax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: f'{int(round(y * 100))}%')) \nax.set_ylim(0.20, 0.73)\n\nax.yaxis.grid(which='major', color='gray', linestyle='--', linewidth=0.5)\nax.spines[['right', 'top']].set_visible(False)\nplt.legend(loc='best', fontsize=13)\n\n# if k&gt;=2:\n#     plt.savefig('detroit-avg-win-smoothed.png')\n# else:\n#     plt.savefig('detroit-avg-win.png')\n    \nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe plot answers the question of how the winning percentages of the four major Detroit sports teams (Pistons, Red Wings, Tigers and Lions) have changed over the last 80 years. Wikipedia was scraped for data on wins, losses and ties by season for each team. For a fair comparison across sports, we computed the winning percentage as the number of wins divided by the total number of games played (i.e. wins plus ties plus losses). We assumed that a tie is 1/2 of a win, so the winning percentage was computed as \\(\\frac{(wins + 0.5×ties)}{(wins+ties+losses)}\\). For Pistons and Tigers (basketball and baseball) ties do not occur so the winning percentage was simply computed as \\(\\frac{(wins)}{(wins+losses)}\\). A 10 year moving average was plotted to identify trends in the team’s win percentages.\nThe Pistons saw high performance in 1940, 1980, and 2000, followed by periods of decline. The Red Wings experienced a notable rise from 1980 to 2000, with a subsequent downward trend. The Tigers’ win percentage remains relatively stable around 50% with minor fluctuations with lowest performance between 1990 and 2000. The Lions show an up and down trend, starting low, peaking in 1950, 1990 and 2010 and then declining again."
  },
  {
    "objectID": "ml-video.html",
    "href": "ml-video.html",
    "title": "Predicting viewer engagement with educational videos",
    "section": "",
    "text": "Note\n\n\n\nNote: data from the Coursera course Applied Machine Learning in Python"
  },
  {
    "objectID": "ml-video.html#about-the-prediction-problem",
    "href": "ml-video.html#about-the-prediction-problem",
    "title": "Predicting viewer engagement with educational videos",
    "section": "About the prediction problem",
    "text": "About the prediction problem\nWith the accelerating popularity of online educational experiences, the role of online lectures and other educational video continues to increase in scope and importance. Open access educational repositories such as videolectures.net, as well as Massive Open Online Courses (MOOCs) on platforms like Coursera, have made access to many thousands of lectures and tutorials an accessible option for millions of people around the world. Yet this impressive volume of content has also led to a challenge in how to find, filter, and match these videos with learners.\nOne critical property of a video is engagement: how interesting or “engaging” it is for viewers, so that they decide to keep watching. Engagement is critical for learning, whether the instruction is coming from a video or any other source. There are many ways to define engagement with video, but one common approach is to estimate it by measuring how much of the video a user watches. If the video is not interesting and does not engage a viewer, they will typically abandon it quickly, e.g. only watch 5 or 10% of the total.\nA first step towards providing the best-matching educational content is to understand which features of educational material make it engaging for learners in general. This is where predictive modeling can be applied, via supervised machine learning. Here the task is to predict how engaging an educational video is likely to be for viewers, based on a set of features extracted from the video’s transcript, audio track, hosting site, and other sources."
  },
  {
    "objectID": "ml-video.html#about-the-dataset",
    "href": "ml-video.html#about-the-dataset",
    "title": "Predicting viewer engagement with educational videos",
    "section": "About the dataset",
    "text": "About the dataset\nWe extracted training and test datasets of educational video features from the VLE Dataset put together by researcher Sahan Bulathwela at University College London.\nTwo data files are provided: train.csv and test.csv. Each row in these two files corresponds to a single educational video, and includes information about diverse properties of the video content as described further below. The target variable is engagement which was defined as True if the median percentage of the video watched across all viewers was at least 30%, and False otherwise.\nFile descriptions\n\ntrain.csv - the training set\ntest.csv - the test set\n\nData fields\ntrain.csv & test.csv:\n\ntitle_word_count - the number of words in the title of the video.\ndocument_entropy - a score indicating how varied the topics are covered in the video, based on the transcript. Videos with smaller entropy scores will tend to be more cohesive and more focused on a single topic.\nfreshness - The number of days elapsed between 01/01/1970 and the lecture published date. Videos that are more recent will have higher freshness values.\neasiness - A text difficulty measure applied to the transcript. A lower score indicates more complex language used by the presenter.\nfraction_stopword_presence - A stopword is a very common word like ‘the’ or ‘and’. This feature computes the fraction of all words that are stopwords in the video lecture transcript.\nspeaker_speed - The average speaking rate in words per minute of the presenter in the video.\nsilent_period_rate - The fraction of time in the lecture video that is silence (no speaking).\n\ntrain.csv only:\n\nengagement - Target label for training. True if learners watched a substantial portion of the video (see description), or False otherwise.\n\nMore details on the original VLE dataset and others related to video engagement here.\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\nnp.random.seed(0)"
  },
  {
    "objectID": "ml-video.html#load-dataset",
    "href": "ml-video.html#load-dataset",
    "title": "Predicting viewer engagement with educational videos",
    "section": "Load dataset",
    "text": "Load dataset\n\ndf_train = pd.read_csv('data/train.csv')\ndf_test = pd.read_csv('data/test.csv')\ndf_train['engagement'] = df_train['engagement'].astype(int)"
  },
  {
    "objectID": "ml-video.html#exploratoy-data-analysis",
    "href": "ml-video.html#exploratoy-data-analysis",
    "title": "Predicting viewer engagement with educational videos",
    "section": "Exploratoy Data Analysis",
    "text": "Exploratoy Data Analysis\n\nFeature Distribution\n\nimport seaborn as sns\nfig, subaxes = plt.subplots(3, 3, figsize=(10, 10))\ni = 1\nfor row in subaxes:\n    for this_axis in row:\n        sns.histplot(df_train.iloc[:, i], ax=this_axis)\n        this_axis.set_title('{}'.format(df_train.columns[i]))\n        i += 1\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFeature Correlation\nFrom the heatmap we can observe that there’s not large correlations between the variables, except for easiness and normalization_rate.\n\ndf_corr = df_train.iloc[:,1:].corr()\nplt.figure(figsize=(8,8))\nsns.heatmap(df_corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 7},\n           xticklabels= df_corr.columns, \n           yticklabels= df_corr.columns,\n           cmap=sns.diverging_palette(120, 10, as_cmap=True))\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFeature Selection\nHere the most important features are document_entropy, freshness and easiness.\n\nfrom sklearn.feature_selection import SelectKBest, f_classif\nimport seaborn as sns\n\nX = df_train.iloc[:,1:-1]\ny = df_train.iloc[:,-1] ## engagement\n\nthis_k = 8\nselector = SelectKBest(f_classif, k='all')\nselector.fit(X, y)\n\n# get the score for each feature\nscores = selector.scores_\n\nfeature_scores = pd.DataFrame({'Feature': X.columns, 'Score': scores})\ntotal = feature_scores['Score'].sum()\nfeature_scores['Score'] = feature_scores['Score']/total\nfeature_scores.sort_values('Score', ascending=False, inplace=True)\n\nplt.figure(figsize=(6,3))\nsns.barplot(x='Score', y='Feature', data=feature_scores)\nplt.xlabel('Score')\nplt.ylabel('Features')\nplt.title('Features importance (Normalized)')\n# plt.xticks(rotation=20, ha='right')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nRandom Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\ntop_features = ['document_entropy', 'freshness', 'easiness']\nX_train, y_train = df_train[top_features], df_train['engagement'].astype(int)\nX_test = df_test[top_features]\n    \nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': ['sqrt', 'log2']\n}\n\nrfc = RandomForestClassifier(random_state=0)\n\ngrid_search = GridSearchCV(rfc, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)\n\n{'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n0.8750181867018478\n\n\n\n\nGradient Boosting\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': ['sqrt', 'log2']\n}\n\nrfc = GradientBoostingClassifier(random_state=0)\n\ngrid_search = GridSearchCV(rfc, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)\n\n{'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 50}\n0.8646361089042813\n\n\n\n\nGaussian Naive Bayes\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {'var_smoothing': [1e-10, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}\n\ngnb = GaussianNB()\n\ngrid_search = GridSearchCV(gnb, param_grid=param_grid, cv=5, n_jobs=-1, scoring='roc_auc')\ngrid_search.fit(X_train, y_train)\n\nprint(grid_search.best_score_)\nprint(grid_search.best_params_)\n\n0.8271618585290608\n{'var_smoothing': 1e-10}\n\n\n\n\nExplore model performance\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nclfs = ['GaussianNB', 'GradientBoostingClassifier', 'RandomForestClassifier']\n\nfig, subaxes = plt.subplots(1, 3, figsize=(12, 4))\nfor clf, this_axis in zip(clfs, subaxes):\n    nbclf = eval(clf)().fit(X_train, y_train)\n    y_probabilities = nbclf.predict_proba(X_test)\n    fpr_lr, tpr_lr, _ = roc_curve(y_test, y_probabilities[:,1])\n    roc_auc_lr = auc(fpr_lr, tpr_lr)\n    this_axis.plot(fpr_lr, tpr_lr, lw=3, label='{}'.format(clf) + ' ROC curve (area = {:0.2f})'.format(roc_auc_lr))\n    this_axis.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\n    this_axis.legend(loc='lower right', fontsize=7)\n    this_axis.set_xlabel('False Positive Rate', fontsize=8)\n    this_axis.set_ylabel('True Positive Rate', fontsize=8)\n    this_axis.set_title('ROC curve {}'.format(clf), fontsize=8)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfor clf in clfs:\n    nbclf = eval(clf)().fit(X_train, y_train)\n    y_probabilities = nbclf.predict_proba(X_test)\n    fpr_lr, tpr_lr, _ = roc_curve(y_test, y_probabilities[:,1])\n    roc_auc_lr = auc(fpr_lr, tpr_lr)\n    plt.plot(fpr_lr, tpr_lr, lw=3, label='{}'.format(clf) + ' ROC curve (area = {:0.2f})'.format(roc_auc_lr))\n    plt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\n    plt.legend(loc='lower right', fontsize=7)\n    plt.xlabel('False Positive Rate', fontsize=8)\n    plt.ylabel('True Positive Rate', fontsize=8)"
  },
  {
    "objectID": "ml-video.html#peformance-evaluation-on-the-best-models",
    "href": "ml-video.html#peformance-evaluation-on-the-best-models",
    "title": "Predicting viewer engagement with educational videos",
    "section": "Peformance evaluation on the best models",
    "text": "Peformance evaluation on the best models\nSince labels for the 2309 test set videos are not provided, I evalaute performance of models on the validation set.\n\ntop_features = ['document_entropy', 'freshness', 'easiness']\n\nX = df_train[top_features]\ny = df_train['engagement'].astype(int)\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n\n## results from grid search\n#  {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\nclf_rf = RandomForestClassifier(max_depth=10, max_features='sqrt', min_samples_leaf=1, \n                               min_samples_split=5, n_estimators=100, n_jobs=-1, \n                               random_state=0)\n\n## results from grid search\n# {'max_depth': 30, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\nclf_gb = GradientBoostingClassifier(max_depth=30, max_features='sqrt', min_samples_leaf=2, \n                                   min_samples_split=5, n_estimators=200, random_state=0)\n\nclf_rf.fit(X_train, y_train)\nclf_gb.fit(X_train, y_train)\n\ny_pred = clf_rf.predict_proba(X_val)[:, 1]\nroc_auc = roc_auc_score(y_val, y_pred)\nprint(f\"Random Forest AUROC: {roc_auc:.3f}\")\n\nclf_gb.fit(X_train, y_train)\ny_pred = clf_gb.predict_proba(X_val)[:, 1]\nroc_auc = roc_auc_score(y_val, y_pred)\nprint(f\"Gradient Boost AUROC: {roc_auc:.3f}\")\n\nRandom Forest AUROC: 0.854\nGradient Boost AUROC: 0.837\n\n\nCross validatated performance …\n\nmetrics = ['roc_auc', 'average_precision', 'balanced_accuracy']\nperfname = ['AUROC', 'AUPRC', 'Balanced Accuracy']\n\nclfs = [clf_rf, clf_gb]\nclfsname = ['Random Forest', 'Gradient Boost']\n\nfor clf, clfname in zip(clfs, clfsname):\n  for perf, name in zip(metrics, perfname):\n    scores = cross_val_score(clf, X, y, cv=5, scoring=perf)\n    print(f\"{clfname} - Averaged {name}: {scores.mean():.3f}\")\n\nRandom Forest - Averaged AUROC: 0.875\nRandom Forest - Averaged AUPRC: 0.610\nRandom Forest - Averaged Balanced Accuracy: 0.699\nGradient Boost - Averaged AUROC: 0.849\nGradient Boost - Averaged AUPRC: 0.562\nGradient Boost - Averaged Balanced Accuracy: 0.712"
  },
  {
    "objectID": "ml-video.html#prediction-on-the-2309-test-set-videos",
    "href": "ml-video.html#prediction-on-the-2309-test-set-videos",
    "title": "Predicting viewer engagement with educational videos",
    "section": "Prediction on the 2309 test set videos",
    "text": "Prediction on the 2309 test set videos\n\ntop_features = ['document_entropy', 'freshness', 'easiness']\n\nX_train, y_train = df_train[top_features], df_train.iloc[:,-1].astype(int)\nX_test = df_test[top_features]\n\n## results from grid search\n#  {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\nclf = RandomForestClassifier(max_depth=10, max_features='sqrt', min_samples_leaf=1, \n                              min_samples_split=5, n_estimators=100, n_jobs=-1, \n                              random_state=0)\n\n## results from grid search\n# {'max_depth': 30, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n# clf = GradientBoostingClassifier(max_depth=30, max_features='sqrt', min_samples_leaf=2, \n#                                  min_samples_split=5, n_estimators=200, random_state=0)\n\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict_proba(X_test)\n\nindexes = df_test['id'].values\nprobabilities = y_pred[:,1]\n\npred = pd.Series(probabilities, index=indexes)\npred\n\n9240     0.010444\n9241     0.031429\n9242     0.066196\n9243     0.751450\n9244     0.012729\n           ...   \n11544    0.018296\n11545    0.023312\n11546    0.015856\n11547    0.863286\n11548    0.053050\nLength: 2309, dtype: float64"
  }
]